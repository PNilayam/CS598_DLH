{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "fb43f3484ec9eb2eb818f2c4b60702eb23b3e73ba2f9349f357001e8ed925189"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 29\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"/Users/prashanti.nilayam/Desktop/temp/\"\n",
    "prev_value_map = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value_map = {\"Capillary refill rate\": 0.0,\n",
    "    \"Diastolic blood pressure\": 59.0,\n",
    "    \"Fraction inspired oxygen\": 0.21,\n",
    "    \"Glascow coma scale eye opening\": 4,\n",
    "    \"Glascow coma scale motor response\": 6,\n",
    "    \"Glascow coma scale total\": 15,\n",
    "    \"Glascow coma scale verbal response\": 5,\n",
    "    \"Glucose\": 128.0,\n",
    "    \"Heart Rate\": 86,\n",
    "    \"Height\": 170.0,\n",
    "    \"Mean blood pressure\": 77.0,\n",
    "    \"Oxygen saturation\": 98.0,\n",
    "    \"Respiratory rate\": 19,\n",
    "    \"Systolic blood pressure\": 118.0,\n",
    "    \"Temperature\": 36.6,\n",
    "    \"Weight\": 81.0,\n",
    "    \"pH\": 7.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_map = {\n",
    "    \"Glascow coma scale eye opening\":{\n",
    "        \"1 No Response\" : 1,\n",
    "        \"No Response\" : 1,\n",
    "        \"2 To pain\" : 2,\n",
    "        \"To Pain\" : 2,\n",
    "        \"3 To speech\" : 3,\n",
    "        \"To Speech\" : 3,\n",
    "        \"4 Spontaneously\" : 4,\n",
    "        \"Spontaneously\" : 4,\n",
    "        \"None\" : 5\n",
    "    },\n",
    "    \"Glascow coma scale motor response\":{\n",
    "        \"1 No Response\": 1,\n",
    "        \"2 Abnorm extensn\" : 2,\n",
    "        \"Abnormal extension\": 2,\n",
    "        \"3 Abnorm flexion\": 3,\n",
    "        \"Abnormal Flexion\": 3,\n",
    "        \"4 Flex-withdraws\" : 4,\n",
    "        \"Flex-withdraws\": 4,\n",
    "        \"5 Localizes Pain\": 5,\n",
    "        \"Localizes Pain\": 5,\n",
    "        \"6 Obeys Commands\": 6,\n",
    "        \"Obeys Commands\": 6,\n",
    "        \"No response\" : 7,\n",
    "    },\n",
    "    \"Glascow coma scale verbal response\":{\n",
    "        \"1 No Response\" :1,\n",
    "        \"No Response\":1,\n",
    "        \"2 Incomp sounds\": 2,\n",
    "        \"Incomprehensible sounds\":2,\n",
    "        \"3 Inapprop words\":3,\n",
    "        \"Inappropriate Words\":3,\n",
    "        \"4 Confused\":4,\n",
    "        \"Confused\":4,\n",
    "        \"5 Oriented\":5,\n",
    "        \"Oriented\":5,\n",
    "        \"No Response-ETT\":6,\n",
    "        \"1.0 ET/Trach\":7\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(elem):\n",
    "    return np.concatenate([np.array(i) for i in elem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(episode_df):\n",
    "    episode_df[\"Glascow coma scale eye opening\"] = episode_df[\"Glascow coma scale eye opening\"].apply(lambda x: replacement_map[\"Glascow coma scale eye opening\"][x] if x in replacement_map[\"Glascow coma scale eye opening\"] else x)\n",
    "    episode_df[\"Glascow coma scale motor response\"] = episode_df[\"Glascow coma scale motor response\"].apply(lambda x: replacement_map[\"Glascow coma scale motor response\"][x] if x in replacement_map[\"Glascow coma scale motor response\"] else x)\n",
    "    episode_df[\"Glascow coma scale verbal response\"] = episode_df[\"Glascow coma scale verbal response\"].apply(lambda x: replacement_map[\"Glascow coma scale verbal response\"][x] if x in replacement_map[\"Glascow coma scale verbal response\"] else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_capillary_refill_rate(person_id, value, colname):\n",
    "    if value is not None and not np.isnan(value):\n",
    "        prev_value_map[person_id][colname] = value\n",
    "        return value\n",
    "    if person_id in prev_value_map and colname in prev_value_map[person_id] and prev_value_map[person_id][colname] is not None:\n",
    "        prev = prev_value_map[person_id][colname]\n",
    "    else:\n",
    "        prev = default_value_map[colname]\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(pateint_id, episode_df):\n",
    "     prev_value_map[pateint_id] = {}\n",
    "     episode_df[\"Capillary refill rate\"] = episode_df[\"Capillary refill rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Capillary refill rate\"))\n",
    "     episode_df[\"Diastolic blood pressure\"] = episode_df[\"Diastolic blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Diastolic blood pressure\"))\n",
    "     episode_df[\"Fraction inspired oxygen\"] = episode_df[\"Fraction inspired oxygen\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Fraction inspired oxygen\"))\n",
    "     episode_df[\"Glascow coma scale eye opening\"] = episode_df[\"Glascow coma scale eye opening\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale eye opening\"))\n",
    "     episode_df[\"Glascow coma scale motor response\"] = episode_df[\"Glascow coma scale motor response\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale motor response\"))\n",
    "     episode_df[\"Glascow coma scale total\"] = episode_df[\"Glascow coma scale total\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale total\"))\n",
    "     episode_df[\"Glascow coma scale verbal response\"] = episode_df[\"Glascow coma scale verbal response\"].apply(lambda x: process_capillary_refill_rate(pateint_id,  x, \"Glascow coma scale verbal response\"))\n",
    "     episode_df[\"Glucose\"] = episode_df[\"Glucose\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glucose\"))\n",
    "     episode_df[\"Heart Rate\"] = episode_df[\"Heart Rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Heart Rate\"))\n",
    "     episode_df[\"Mean blood pressure\"] = episode_df[\"Mean blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Mean blood pressure\"))\n",
    "     episode_df[\"Height\"] = episode_df[\"Height\"].apply(lambda x: process_capillary_refill_rate(pateint_id,x, \"Height\"))\n",
    "     episode_df[\"Oxygen saturation\"] = episode_df[\"Oxygen saturation\"].apply(lambda x: process_capillary_refill_rate(pateint_id,  x, \"Oxygen saturation\"))\n",
    "     episode_df[\"Respiratory rate\"] = episode_df[\"Respiratory rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Respiratory rate\"))\n",
    "     episode_df[\"Systolic blood pressure\"] = episode_df[\"Systolic blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Systolic blood pressure\"))\n",
    "     episode_df[\"Temperature\"] = episode_df[\"Temperature\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Temperature\"))\n",
    "     episode_df[\"Weight\"] = episode_df[\"Weight\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Weight\"))\n",
    "     episode_df[\"pH\"] = episode_df[\"pH\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"pH\"))\n",
    "     del prev_value_map[pateint_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_indices(data_len):\n",
    "    i = 0\n",
    "    indices = []\n",
    "    while i <= data_len-4:\n",
    "        indices.append([i, i+1, i+2, i+3])\n",
    "        i +=1\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "get_window_indices(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    x_path = DATA_PATH +'/'+path+'/'\n",
    "    X = torch.empty(0,17,4)\n",
    "    Y = torch.empty(0,)\n",
    "    y_df = pd.read_csv(DATA_PATH + path +'_listfile.csv') \n",
    "    data_files = os.listdir(x_path)\n",
    "    print(data_files)\n",
    "    for data_file in data_files:\n",
    "        print(data_file)\n",
    "        if data_file.endswith(\".csv\"):\n",
    "            episode_df = pd.read_csv(x_path + data_file)\n",
    "            cleanup(episode_df)\n",
    "            fill_missing_values(data_file, episode_df)\n",
    "            episode_df[\"H_IDX\"] = episode_df.Hours.apply(np.floor).astype('int32')\n",
    "            episode_df = episode_df.groupby(by = \"H_IDX\").mean()\n",
    "            episode_df = episode_df[episode_df.Hours>=5].reset_index(drop = True)\n",
    "            temp_y = y_df[y_df.stay == data_file].sort_values(by = \"period_length\").reset_index(drop = True)\n",
    "            temp_y = temp_y[[\"period_length\", \"y_true\"]].set_index(\"period_length\")\n",
    "            episode_df = episode_df.join(temp_y, how = \"inner\").drop('Hours', axis=1).reset_index(drop = True)\n",
    "            if(len(episode_df) >0):\n",
    "                indices = get_window_indices(len(episode_df))\n",
    "                windows = []\n",
    "                y_values = []\n",
    "                for idx in indices:\n",
    "                    window = episode_df.loc[idx]\n",
    "                    y_values.append(window.loc[idx[-1]].y_true)\n",
    "                    windows.append(window.drop(\"y_true\", axis=1).transpose().values.astype(np.float32))\n",
    "                t_windows = torch.tensor(windows)\n",
    "                t_y_values = torch.tensor(y_values)\n",
    "                X = torch.cat((X, t_windows), 0)\n",
    "                Y = torch.cat((Y, t_y_values), 0)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['.DS_Store', '10021_episode1_timeseries.csv', '10010_episode1_timeseries.csv', '10003_episode1_timeseries.csv', '10014_episode1_timeseries.csv', '10007_episode1_timeseries.csv', '1000_episode1_timeseries.csv', '10017_episode1_timeseries.csv', '10022_episode1_timeseries.csv', '10013_episode1_timeseries.csv']\n",
      ".DS_Store\n",
      "10021_episode1_timeseries.csv\n",
      "10010_episode1_timeseries.csv\n",
      "10003_episode1_timeseries.csv\n",
      "10014_episode1_timeseries.csv\n",
      "10007_episode1_timeseries.csv\n",
      "1000_episode1_timeseries.csv\n",
      "10017_episode1_timeseries.csv\n",
      "10022_episode1_timeseries.csv\n",
      "10013_episode1_timeseries.csv\n",
      "['10006_episode1_timeseries.csv', '.DS_Store', '10004_episode2_timeseries.csv', '10004_episode1_timeseries.csv']\n",
      "10006_episode1_timeseries.csv\n",
      ".DS_Store\n",
      "10004_episode2_timeseries.csv\n",
      "10004_episode1_timeseries.csv\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EpisodeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, obs, los):\n",
    "        self.x = obs\n",
    "        self.y = los\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "        \n",
    "X_train, Y_train = preprocess('train')\n",
    "train_dataset = EpisodeDataset(X_train, Y_train)\n",
    "X_val, Y_val = preprocess('val')\n",
    "val_dataset = EpisodeDataset(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True)                              \n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=32, shuffle=False)                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [5.0000e+01, 5.7000e+01, 5.7000e+01, 5.1000e+01],\n        [2.1000e-01, 2.1000e-01, 2.1000e-01, 2.1000e-01],\n        [4.0000e+00, 4.0000e+00, 4.0000e+00, 4.0000e+00],\n        [6.0000e+00, 6.0000e+00, 6.0000e+00, 6.0000e+00],\n        [1.5000e+01, 1.5000e+01, 1.5000e+01, 1.5000e+01],\n        [5.0000e+00, 5.0000e+00, 5.0000e+00, 5.0000e+00],\n        [9.3000e+01, 2.3200e+02, 2.3200e+02, 2.3200e+02],\n        [7.6000e+01, 7.4000e+01, 7.0000e+01, 7.1000e+01],\n        [1.7000e+02, 1.7000e+02, 1.7000e+02, 1.7000e+02],\n        [7.2333e+01, 8.0667e+01, 8.0667e+01, 7.4333e+01],\n        [1.0000e+02, 1.0000e+02, 1.0000e+02, 9.8000e+01],\n        [2.2000e+01, 1.5000e+01, 2.2000e+01, 2.0000e+01],\n        [1.1700e+02, 1.2800e+02, 1.2800e+02, 1.2100e+02],\n        [3.7556e+01, 3.6444e+01, 3.6444e+01, 3.7389e+01],\n        [8.1000e+01, 8.1000e+01, 8.1000e+01, 8.1000e+01],\n        [7.4000e+00, 7.4000e+00, 7.4000e+00, 7.4000e+00]]) tensor(31.1800, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for data in val_dataset:\n",
    "    print(data[0], data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EpisiodeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EpisiodeCNN, self).__init__()\n",
    "        #input shape 1 * 17 * 4\n",
    "        #output shape 17 * 17 * 4\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=17, kernel_size=3, padding=1, stride = 1)\n",
    "        #input shape 17 * 17 * 4\n",
    "        #output shape 68 * 17 * 4\n",
    "        self.conv2 = nn.Conv2d(in_channels=17, out_channels=34, kernel_size=3, padding=1, stride = 1)\n",
    "        #input shape 34 * 17 * 4\n",
    "        #output shape 34 * 8 * 2\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        #input shape 34 * 8 * 2\n",
    "        #output shape 68 * 8 * 2\n",
    "        self.conv3 = nn.Conv2d(in_channels=34, out_channels=68, kernel_size=3, padding=1, stride = 1)\n",
    "        self.fc1 = nn.Linear(68*8*2, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #input is of shape (batch_size=32, 3, 224, 224) if you did the dataloader right\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = x.view(-1, 68 * 8 * 2)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EpisiodeCNN()\n",
    "learning_rate = 0.00001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_y_true = torch.DoubleTensor()\n",
    "    all_y_pred = torch.DoubleTensor()\n",
    "    for x, y in val_loader:\n",
    "        y_hat = model(x)\n",
    "        all_y_true = torch.cat((all_y_true, y.to('cpu')), dim=0)\n",
    "        all_y_pred = torch.cat((all_y_pred,  y_hat.to('cpu')), dim=0)\n",
    "    mse= mean_squared_error(all_y_true.detach().numpy(), all_y_pred.detach().numpy())\n",
    "    print(f\"mse: {mse:.3f}\")\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".493482\n",
      "Epoch: 15 \tTraining Loss: 1121.108690\n",
      "Epoch: 15 \tTraining Loss: 944.082880\n",
      "Epoch: 15 \tTraining Loss: 460.884569\n",
      "Epoch: 15 \tTraining Loss: 462.819253\n",
      "mse: 75320.166\n",
      "Epoch: 16 \tTraining Loss: 2185.282283\n",
      "Epoch: 16 \tTraining Loss: 760.809532\n",
      "Epoch: 16 \tTraining Loss: 1065.913697\n",
      "Epoch: 16 \tTraining Loss: 482.804190\n",
      "Epoch: 16 \tTraining Loss: 560.709989\n",
      "Epoch: 16 \tTraining Loss: 550.388923\n",
      "Epoch: 16 \tTraining Loss: 380.368642\n",
      "Epoch: 16 \tTraining Loss: 401.159290\n",
      "Epoch: 16 \tTraining Loss: 1086.274821\n",
      "Epoch: 16 \tTraining Loss: 571.696206\n",
      "Epoch: 16 \tTraining Loss: 894.718031\n",
      "Epoch: 16 \tTraining Loss: 564.990372\n",
      "Epoch: 16 \tTraining Loss: 654.087668\n",
      "Epoch: 16 \tTraining Loss: 1231.724466\n",
      "Epoch: 16 \tTraining Loss: 556.699874\n",
      "Epoch: 16 \tTraining Loss: 1124.444456\n",
      "Epoch: 16 \tTraining Loss: 406.143762\n",
      "Epoch: 16 \tTraining Loss: 1223.882414\n",
      "Epoch: 16 \tTraining Loss: 304.743700\n",
      "Epoch: 16 \tTraining Loss: 550.747053\n",
      "Epoch: 16 \tTraining Loss: 519.058375\n",
      "Epoch: 16 \tTraining Loss: 854.716468\n",
      "Epoch: 16 \tTraining Loss: 924.500196\n",
      "Epoch: 16 \tTraining Loss: 436.208522\n",
      "Epoch: 16 \tTraining Loss: 1229.225633\n",
      "Epoch: 16 \tTraining Loss: 1544.954804\n",
      "Epoch: 16 \tTraining Loss: 962.804312\n",
      "Epoch: 16 \tTraining Loss: 1281.724138\n",
      "Epoch: 16 \tTraining Loss: 568.468465\n",
      "Epoch: 16 \tTraining Loss: 433.872332\n",
      "Epoch: 16 \tTraining Loss: 1134.364705\n",
      "Epoch: 16 \tTraining Loss: 1338.439954\n",
      "mse: 75262.368\n",
      "Epoch: 17 \tTraining Loss: 271.942045\n",
      "Epoch: 17 \tTraining Loss: 1094.965453\n",
      "Epoch: 17 \tTraining Loss: 497.110101\n",
      "Epoch: 17 \tTraining Loss: 756.860697\n",
      "Epoch: 17 \tTraining Loss: 757.989621\n",
      "Epoch: 17 \tTraining Loss: 1112.862261\n",
      "Epoch: 17 \tTraining Loss: 986.595076\n",
      "Epoch: 17 \tTraining Loss: 357.497231\n",
      "Epoch: 17 \tTraining Loss: 1332.053177\n",
      "Epoch: 17 \tTraining Loss: 411.052039\n",
      "Epoch: 17 \tTraining Loss: 525.738311\n",
      "Epoch: 17 \tTraining Loss: 392.652458\n",
      "Epoch: 17 \tTraining Loss: 398.029532\n",
      "Epoch: 17 \tTraining Loss: 596.648503\n",
      "Epoch: 17 \tTraining Loss: 472.595705\n",
      "Epoch: 17 \tTraining Loss: 1339.831483\n",
      "Epoch: 17 \tTraining Loss: 1057.972731\n",
      "Epoch: 17 \tTraining Loss: 365.571534\n",
      "Epoch: 17 \tTraining Loss: 834.749273\n",
      "Epoch: 17 \tTraining Loss: 666.762145\n",
      "Epoch: 17 \tTraining Loss: 628.700375\n",
      "Epoch: 17 \tTraining Loss: 962.802931\n",
      "Epoch: 17 \tTraining Loss: 1571.241793\n",
      "Epoch: 17 \tTraining Loss: 1222.651934\n",
      "Epoch: 17 \tTraining Loss: 504.874815\n",
      "Epoch: 17 \tTraining Loss: 301.694457\n",
      "Epoch: 17 \tTraining Loss: 1061.906079\n",
      "Epoch: 17 \tTraining Loss: 840.878354\n",
      "Epoch: 17 \tTraining Loss: 407.650632\n",
      "Epoch: 17 \tTraining Loss: 1101.610100\n",
      "Epoch: 17 \tTraining Loss: 1896.972947\n",
      "Epoch: 17 \tTraining Loss: 2504.198300\n",
      "mse: 79704.754\n",
      "Epoch: 18 \tTraining Loss: 441.329350\n",
      "Epoch: 18 \tTraining Loss: 975.860349\n",
      "Epoch: 18 \tTraining Loss: 1005.678564\n",
      "Epoch: 18 \tTraining Loss: 1550.117412\n",
      "Epoch: 18 \tTraining Loss: 570.933084\n",
      "Epoch: 18 \tTraining Loss: 1139.028270\n",
      "Epoch: 18 \tTraining Loss: 455.211605\n",
      "Epoch: 18 \tTraining Loss: 1434.013754\n",
      "Epoch: 18 \tTraining Loss: 1225.109314\n",
      "Epoch: 18 \tTraining Loss: 519.044363\n",
      "Epoch: 18 \tTraining Loss: 377.770406\n",
      "Epoch: 18 \tTraining Loss: 658.520121\n",
      "Epoch: 18 \tTraining Loss: 406.839651\n",
      "Epoch: 18 \tTraining Loss: 1158.496232\n",
      "Epoch: 18 \tTraining Loss: 989.804567\n",
      "Epoch: 18 \tTraining Loss: 568.336397\n",
      "Epoch: 18 \tTraining Loss: 989.876376\n",
      "Epoch: 18 \tTraining Loss: 1314.493501\n",
      "Epoch: 18 \tTraining Loss: 874.611562\n",
      "Epoch: 18 \tTraining Loss: 1124.813146\n",
      "Epoch: 18 \tTraining Loss: 1206.578369\n",
      "Epoch: 18 \tTraining Loss: 1124.452626\n",
      "Epoch: 18 \tTraining Loss: 483.275980\n",
      "Epoch: 18 \tTraining Loss: 1134.101547\n",
      "Epoch: 18 \tTraining Loss: 462.366239\n",
      "Epoch: 18 \tTraining Loss: 501.107506\n",
      "Epoch: 18 \tTraining Loss: 951.133399\n",
      "Epoch: 18 \tTraining Loss: 320.409108\n",
      "Epoch: 18 \tTraining Loss: 332.428815\n",
      "Epoch: 18 \tTraining Loss: 1136.437669\n",
      "Epoch: 18 \tTraining Loss: 599.409166\n",
      "Epoch: 18 \tTraining Loss: 428.291180\n",
      "mse: 70617.858\n",
      "Epoch: 19 \tTraining Loss: 920.947512\n",
      "Epoch: 19 \tTraining Loss: 912.902192\n",
      "Epoch: 19 \tTraining Loss: 529.191189\n",
      "Epoch: 19 \tTraining Loss: 615.854148\n",
      "Epoch: 19 \tTraining Loss: 462.815874\n",
      "Epoch: 19 \tTraining Loss: 410.698912\n",
      "Epoch: 19 \tTraining Loss: 908.127898\n",
      "Epoch: 19 \tTraining Loss: 737.540728\n",
      "Epoch: 19 \tTraining Loss: 333.539791\n",
      "Epoch: 19 \tTraining Loss: 357.789146\n",
      "Epoch: 19 \tTraining Loss: 974.637556\n",
      "Epoch: 19 \tTraining Loss: 1146.461899\n",
      "Epoch: 19 \tTraining Loss: 962.284925\n",
      "Epoch: 19 \tTraining Loss: 647.070855\n",
      "Epoch: 19 \tTraining Loss: 298.579209\n",
      "Epoch: 19 \tTraining Loss: 850.524304\n",
      "Epoch: 19 \tTraining Loss: 631.498764\n",
      "Epoch: 19 \tTraining Loss: 1719.392860\n",
      "Epoch: 19 \tTraining Loss: 1019.850212\n",
      "Epoch: 19 \tTraining Loss: 845.485247\n",
      "Epoch: 19 \tTraining Loss: 809.532361\n",
      "Epoch: 19 \tTraining Loss: 2126.550964\n",
      "Epoch: 19 \tTraining Loss: 613.328335\n",
      "Epoch: 19 \tTraining Loss: 912.923881\n",
      "Epoch: 19 \tTraining Loss: 1259.629248\n",
      "Epoch: 19 \tTraining Loss: 975.935463\n",
      "Epoch: 19 \tTraining Loss: 1634.243759\n",
      "Epoch: 19 \tTraining Loss: 655.423128\n",
      "Epoch: 19 \tTraining Loss: 513.510272\n",
      "Epoch: 19 \tTraining Loss: 392.735990\n",
      "Epoch: 19 \tTraining Loss: 687.941441\n",
      "Epoch: 19 \tTraining Loss: 394.274879\n",
      "mse: 77997.537\n",
      "Epoch: 20 \tTraining Loss: 787.148845\n",
      "Epoch: 20 \tTraining Loss: 938.777600\n",
      "Epoch: 20 \tTraining Loss: 1032.009322\n",
      "Epoch: 20 \tTraining Loss: 844.714086\n",
      "Epoch: 20 \tTraining Loss: 1680.486519\n",
      "Epoch: 20 \tTraining Loss: 511.760132\n",
      "Epoch: 20 \tTraining Loss: 954.159223\n",
      "Epoch: 20 \tTraining Loss: 563.640846\n",
      "Epoch: 20 \tTraining Loss: 387.223270\n",
      "Epoch: 20 \tTraining Loss: 511.578379\n",
      "Epoch: 20 \tTraining Loss: 361.291712\n",
      "Epoch: 20 \tTraining Loss: 434.773812\n",
      "Epoch: 20 \tTraining Loss: 879.745134\n",
      "Epoch: 20 \tTraining Loss: 1294.950300\n",
      "Epoch: 20 \tTraining Loss: 1023.782066\n",
      "Epoch: 20 \tTraining Loss: 966.076302\n",
      "Epoch: 20 \tTraining Loss: 1879.715106\n",
      "Epoch: 20 \tTraining Loss: 729.126085\n",
      "Epoch: 20 \tTraining Loss: 626.193598\n",
      "Epoch: 20 \tTraining Loss: 538.155240\n",
      "Epoch: 20 \tTraining Loss: 547.380249\n",
      "Epoch: 20 \tTraining Loss: 1878.252617\n",
      "Epoch: 20 \tTraining Loss: 1211.808954\n",
      "Epoch: 20 \tTraining Loss: 634.303987\n",
      "Epoch: 20 \tTraining Loss: 1095.822264\n",
      "Epoch: 20 \tTraining Loss: 839.153527\n",
      "Epoch: 20 \tTraining Loss: 455.111042\n",
      "Epoch: 20 \tTraining Loss: 478.435926\n",
      "Epoch: 20 \tTraining Loss: 1026.352008\n",
      "Epoch: 20 \tTraining Loss: 386.436149\n",
      "Epoch: 20 \tTraining Loss: 383.907843\n",
      "Epoch: 20 \tTraining Loss: 369.108284\n",
      "mse: 70963.263\n",
      "Epoch: 21 \tTraining Loss: 501.584868\n",
      "Epoch: 21 \tTraining Loss: 1048.396058\n",
      "Epoch: 21 \tTraining Loss: 1007.423132\n",
      "Epoch: 21 \tTraining Loss: 893.871514\n",
      "Epoch: 21 \tTraining Loss: 816.090767\n",
      "Epoch: 21 \tTraining Loss: 1153.290563\n",
      "Epoch: 21 \tTraining Loss: 495.493530\n",
      "Epoch: 21 \tTraining Loss: 1245.099260\n",
      "Epoch: 21 \tTraining Loss: 656.771073\n",
      "Epoch: 21 \tTraining Loss: 563.336871\n",
      "Epoch: 21 \tTraining Loss: 1224.604209\n",
      "Epoch: 21 \tTraining Loss: 441.159473\n",
      "Epoch: 21 \tTraining Loss: 815.988403\n",
      "Epoch: 21 \tTraining Loss: 594.149499\n",
      "Epoch: 21 \tTraining Loss: 643.655257\n",
      "Epoch: 21 \tTraining Loss: 238.994542\n",
      "Epoch: 21 \tTraining Loss: 1793.217956\n",
      "Epoch: 21 \tTraining Loss: 694.444518\n",
      "Epoch: 21 \tTraining Loss: 1009.725048\n",
      "Epoch: 21 \tTraining Loss: 1157.749361\n",
      "Epoch: 21 \tTraining Loss: 2052.752852\n",
      "Epoch: 21 \tTraining Loss: 595.817285\n",
      "Epoch: 21 \tTraining Loss: 598.654315\n",
      "Epoch: 21 \tTraining Loss: 618.457112\n",
      "Epoch: 21 \tTraining Loss: 1054.762973\n",
      "Epoch: 21 \tTraining Loss: 991.675296\n",
      "Epoch: 21 \tTraining Loss: 1467.523113\n",
      "Epoch: 21 \tTraining Loss: 301.986201\n",
      "Epoch: 21 \tTraining Loss: 476.596266\n",
      "Epoch: 21 \tTraining Loss: 406.646781\n",
      "Epoch: 21 \tTraining Loss: 294.463416\n",
      "Epoch: 21 \tTraining Loss: 456.724966\n",
      "mse: 71279.097\n",
      "Epoch: 22 \tTraining Loss: 603.146791\n",
      "Epoch: 22 \tTraining Loss: 1784.935852\n",
      "Epoch: 22 \tTraining Loss: 1050.583231\n",
      "Epoch: 22 \tTraining Loss: 1321.623901\n",
      "Epoch: 22 \tTraining Loss: 629.683747\n",
      "Epoch: 22 \tTraining Loss: 1386.007292\n",
      "Epoch: 22 \tTraining Loss: 447.715975\n",
      "Epoch: 22 \tTraining Loss: 917.863763\n",
      "Epoch: 22 \tTraining Loss: 388.612681\n",
      "Epoch: 22 \tTraining Loss: 943.323264\n",
      "Epoch: 22 \tTraining Loss: 496.267137\n",
      "Epoch: 22 \tTraining Loss: 995.582963\n",
      "Epoch: 22 \tTraining Loss: 1124.896784\n",
      "Epoch: 22 \tTraining Loss: 383.737374\n",
      "Epoch: 22 \tTraining Loss: 524.673128\n",
      "Epoch: 22 \tTraining Loss: 665.140276\n",
      "Epoch: 22 \tTraining Loss: 1475.073526\n",
      "Epoch: 22 \tTraining Loss: 468.450502\n",
      "Epoch: 22 \tTraining Loss: 477.154431\n",
      "Epoch: 22 \tTraining Loss: 1006.600159\n",
      "Epoch: 22 \tTraining Loss: 1196.575117\n",
      "Epoch: 22 \tTraining Loss: 1153.067682\n",
      "Epoch: 22 \tTraining Loss: 454.083013\n",
      "Epoch: 22 \tTraining Loss: 1282.670906\n",
      "Epoch: 22 \tTraining Loss: 314.750536\n",
      "Epoch: 22 \tTraining Loss: 508.113715\n",
      "Epoch: 22 \tTraining Loss: 436.877035\n",
      "Epoch: 22 \tTraining Loss: 547.903539\n",
      "Epoch: 22 \tTraining Loss: 720.534149\n",
      "Epoch: 22 \tTraining Loss: 963.242507\n",
      "Epoch: 22 \tTraining Loss: 367.556889\n",
      "Epoch: 22 \tTraining Loss: 1755.092998\n",
      "mse: 76319.045\n",
      "Epoch: 23 \tTraining Loss: 395.773632\n",
      "Epoch: 23 \tTraining Loss: 577.602281\n",
      "Epoch: 23 \tTraining Loss: 450.549892\n",
      "Epoch: 23 \tTraining Loss: 1466.586727\n",
      "Epoch: 23 \tTraining Loss: 628.157936\n",
      "Epoch: 23 \tTraining Loss: 542.946371\n",
      "Epoch: 23 \tTraining Loss: 702.199920\n",
      "Epoch: 23 \tTraining Loss: 1259.852636\n",
      "Epoch: 23 \tTraining Loss: 1069.320984\n",
      "Epoch: 23 \tTraining Loss: 728.802217\n",
      "Epoch: 23 \tTraining Loss: 1219.710788\n",
      "Epoch: 23 \tTraining Loss: 343.159745\n",
      "Epoch: 23 \tTraining Loss: 1084.205341\n",
      "Epoch: 23 \tTraining Loss: 397.222178\n",
      "Epoch: 23 \tTraining Loss: 557.127745\n",
      "Epoch: 23 \tTraining Loss: 346.395035\n",
      "Epoch: 23 \tTraining Loss: 405.105501\n",
      "Epoch: 23 \tTraining Loss: 1054.631648\n",
      "Epoch: 23 \tTraining Loss: 935.098881\n",
      "Epoch: 23 \tTraining Loss: 412.682637\n",
      "Epoch: 23 \tTraining Loss: 1738.485042\n",
      "Epoch: 23 \tTraining Loss: 871.437287\n",
      "Epoch: 23 \tTraining Loss: 946.457447\n",
      "Epoch: 23 \tTraining Loss: 952.810621\n",
      "Epoch: 23 \tTraining Loss: 448.180036\n",
      "Epoch: 23 \tTraining Loss: 1005.863964\n",
      "Epoch: 23 \tTraining Loss: 1449.056676\n",
      "Epoch: 23 \tTraining Loss: 1030.887217\n",
      "Epoch: 23 \tTraining Loss: 490.258633\n",
      "Epoch: 23 \tTraining Loss: 1076.001059\n",
      "Epoch: 23 \tTraining Loss: 589.355098\n",
      "Epoch: 23 \tTraining Loss: 1461.118510\n",
      "mse: 77000.143\n",
      "Epoch: 24 \tTraining Loss: 1515.896173\n",
      "Epoch: 24 \tTraining Loss: 924.139573\n",
      "Epoch: 24 \tTraining Loss: 1153.153168\n",
      "Epoch: 24 \tTraining Loss: 558.010156\n",
      "Epoch: 24 \tTraining Loss: 670.133865\n",
      "Epoch: 24 \tTraining Loss: 844.181954\n",
      "Epoch: 24 \tTraining Loss: 708.885231\n",
      "Epoch: 24 \tTraining Loss: 734.340904\n",
      "Epoch: 24 \tTraining Loss: 427.346465\n",
      "Epoch: 24 \tTraining Loss: 553.184951\n",
      "Epoch: 24 \tTraining Loss: 476.680396\n",
      "Epoch: 24 \tTraining Loss: 1514.308667\n",
      "Epoch: 24 \tTraining Loss: 1021.609809\n",
      "Epoch: 24 \tTraining Loss: 985.059416\n",
      "Epoch: 24 \tTraining Loss: 1829.255537\n",
      "Epoch: 24 \tTraining Loss: 342.976764\n",
      "Epoch: 24 \tTraining Loss: 363.669694\n",
      "Epoch: 24 \tTraining Loss: 574.743364\n",
      "Epoch: 24 \tTraining Loss: 417.105606\n",
      "Epoch: 24 \tTraining Loss: 335.586303\n",
      "Epoch: 24 \tTraining Loss: 514.715503\n",
      "Epoch: 24 \tTraining Loss: 947.157441\n",
      "Epoch: 24 \tTraining Loss: 1514.175485\n",
      "Epoch: 24 \tTraining Loss: 920.179984\n",
      "Epoch: 24 \tTraining Loss: 622.118779\n",
      "Epoch: 24 \tTraining Loss: 753.262993\n",
      "Epoch: 24 \tTraining Loss: 962.404038\n",
      "Epoch: 24 \tTraining Loss: 990.596049\n",
      "Epoch: 24 \tTraining Loss: 1003.526215\n",
      "Epoch: 24 \tTraining Loss: 542.364680\n",
      "Epoch: 24 \tTraining Loss: 584.667129\n",
      "Epoch: 24 \tTraining Loss: 1273.959869\n",
      "mse: 79910.711\n",
      "Epoch: 25 \tTraining Loss: 399.151392\n",
      "Epoch: 25 \tTraining Loss: 1544.150711\n",
      "Epoch: 25 \tTraining Loss: 677.711110\n",
      "Epoch: 25 \tTraining Loss: 350.800501\n",
      "Epoch: 25 \tTraining Loss: 1529.198114\n",
      "Epoch: 25 \tTraining Loss: 881.947404\n",
      "Epoch: 25 \tTraining Loss: 399.762605\n",
      "Epoch: 25 \tTraining Loss: 1137.135713\n",
      "Epoch: 25 \tTraining Loss: 1280.135589\n",
      "Epoch: 25 \tTraining Loss: 1388.279634\n",
      "Epoch: 25 \tTraining Loss: 582.379548\n",
      "Epoch: 25 \tTraining Loss: 554.568305\n",
      "Epoch: 25 \tTraining Loss: 1065.787357\n",
      "Epoch: 25 \tTraining Loss: 467.746689\n",
      "Epoch: 25 \tTraining Loss: 583.500153\n",
      "Epoch: 25 \tTraining Loss: 713.834882\n",
      "Epoch: 25 \tTraining Loss: 950.574241\n",
      "Epoch: 25 \tTraining Loss: 412.286874\n",
      "Epoch: 25 \tTraining Loss: 1521.184213\n",
      "Epoch: 25 \tTraining Loss: 1698.012760\n",
      "Epoch: 25 \tTraining Loss: 583.452997\n",
      "Epoch: 25 \tTraining Loss: 833.786179\n",
      "Epoch: 25 \tTraining Loss: 366.941300\n",
      "Epoch: 25 \tTraining Loss: 1136.534753\n",
      "Epoch: 25 \tTraining Loss: 433.224273\n",
      "Epoch: 25 \tTraining Loss: 1010.670205\n",
      "Epoch: 25 \tTraining Loss: 504.033885\n",
      "Epoch: 25 \tTraining Loss: 800.080660\n",
      "Epoch: 25 \tTraining Loss: 898.161669\n",
      "Epoch: 25 \tTraining Loss: 363.409457\n",
      "Epoch: 25 \tTraining Loss: 433.728762\n",
      "Epoch: 25 \tTraining Loss: 1005.419484\n",
      "mse: 71198.032\n",
      "Epoch: 26 \tTraining Loss: 626.462073\n",
      "Epoch: 26 \tTraining Loss: 944.864774\n",
      "Epoch: 26 \tTraining Loss: 379.542280\n",
      "Epoch: 26 \tTraining Loss: 724.500409\n",
      "Epoch: 26 \tTraining Loss: 626.448453\n",
      "Epoch: 26 \tTraining Loss: 1600.209161\n",
      "Epoch: 26 \tTraining Loss: 820.090581\n",
      "Epoch: 26 \tTraining Loss: 1135.732926\n",
      "Epoch: 26 \tTraining Loss: 425.006907\n",
      "Epoch: 26 \tTraining Loss: 739.019536\n",
      "Epoch: 26 \tTraining Loss: 391.934641\n",
      "Epoch: 26 \tTraining Loss: 1388.119345\n",
      "Epoch: 26 \tTraining Loss: 1212.340509\n",
      "Epoch: 26 \tTraining Loss: 1034.069539\n",
      "Epoch: 26 \tTraining Loss: 439.739096\n",
      "Epoch: 26 \tTraining Loss: 680.604593\n",
      "Epoch: 26 \tTraining Loss: 1573.277855\n",
      "Epoch: 26 \tTraining Loss: 421.583829\n",
      "Epoch: 26 \tTraining Loss: 1161.383137\n",
      "Epoch: 26 \tTraining Loss: 405.673986\n",
      "Epoch: 26 \tTraining Loss: 381.550408\n",
      "Epoch: 26 \tTraining Loss: 1043.179273\n",
      "Epoch: 26 \tTraining Loss: 1086.239250\n",
      "Epoch: 26 \tTraining Loss: 597.533480\n",
      "Epoch: 26 \tTraining Loss: 373.184256\n",
      "Epoch: 26 \tTraining Loss: 879.050105\n",
      "Epoch: 26 \tTraining Loss: 397.611100\n",
      "Epoch: 26 \tTraining Loss: 620.563223\n",
      "Epoch: 26 \tTraining Loss: 1591.928148\n",
      "Epoch: 26 \tTraining Loss: 1595.179105\n",
      "Epoch: 26 \tTraining Loss: 483.856201\n",
      "Epoch: 26 \tTraining Loss: 433.507649\n",
      "mse: 75746.130\n",
      "Epoch: 27 \tTraining Loss: 998.760447\n",
      "Epoch: 27 \tTraining Loss: 568.478766\n",
      "Epoch: 27 \tTraining Loss: 511.330512\n",
      "Epoch: 27 \tTraining Loss: 1395.583148\n",
      "Epoch: 27 \tTraining Loss: 910.266220\n",
      "Epoch: 27 \tTraining Loss: 1442.413882\n",
      "Epoch: 27 \tTraining Loss: 573.209698\n",
      "Epoch: 27 \tTraining Loss: 746.633334\n",
      "Epoch: 27 \tTraining Loss: 795.338060\n",
      "Epoch: 27 \tTraining Loss: 990.027850\n",
      "Epoch: 27 \tTraining Loss: 940.323264\n",
      "Epoch: 27 \tTraining Loss: 605.110497\n",
      "Epoch: 27 \tTraining Loss: 704.071046\n",
      "Epoch: 27 \tTraining Loss: 500.265505\n",
      "Epoch: 27 \tTraining Loss: 515.961100\n",
      "Epoch: 27 \tTraining Loss: 587.538872\n",
      "Epoch: 27 \tTraining Loss: 1246.118338\n",
      "Epoch: 27 \tTraining Loss: 1115.512995\n",
      "Epoch: 27 \tTraining Loss: 319.241732\n",
      "Epoch: 27 \tTraining Loss: 715.227343\n",
      "Epoch: 27 \tTraining Loss: 634.659691\n",
      "Epoch: 27 \tTraining Loss: 392.483280\n",
      "Epoch: 27 \tTraining Loss: 584.193806\n",
      "Epoch: 27 \tTraining Loss: 2330.387081\n",
      "Epoch: 27 \tTraining Loss: 589.503926\n",
      "Epoch: 27 \tTraining Loss: 450.420262\n",
      "Epoch: 27 \tTraining Loss: 515.303029\n",
      "Epoch: 27 \tTraining Loss: 611.997732\n",
      "Epoch: 27 \tTraining Loss: 899.812879\n",
      "Epoch: 27 \tTraining Loss: 923.923803\n",
      "Epoch: 27 \tTraining Loss: 1605.151355\n",
      "Epoch: 27 \tTraining Loss: 657.131081\n",
      "mse: 72567.675\n",
      "Epoch: 28 \tTraining Loss: 1011.446705\n",
      "Epoch: 28 \tTraining Loss: 428.530778\n",
      "Epoch: 28 \tTraining Loss: 1078.434284\n",
      "Epoch: 28 \tTraining Loss: 961.646872\n",
      "Epoch: 28 \tTraining Loss: 582.529193\n",
      "Epoch: 28 \tTraining Loss: 1086.570465\n",
      "Epoch: 28 \tTraining Loss: 1405.093118\n",
      "Epoch: 28 \tTraining Loss: 592.411143\n",
      "Epoch: 28 \tTraining Loss: 1375.200890\n",
      "Epoch: 28 \tTraining Loss: 995.247866\n",
      "Epoch: 28 \tTraining Loss: 1293.996951\n",
      "Epoch: 28 \tTraining Loss: 1220.535190\n",
      "Epoch: 28 \tTraining Loss: 486.518258\n",
      "Epoch: 28 \tTraining Loss: 398.061619\n",
      "Epoch: 28 \tTraining Loss: 376.516039\n",
      "Epoch: 28 \tTraining Loss: 940.033572\n",
      "Epoch: 28 \tTraining Loss: 359.216227\n",
      "Epoch: 28 \tTraining Loss: 1007.242725\n",
      "Epoch: 28 \tTraining Loss: 409.983979\n",
      "Epoch: 28 \tTraining Loss: 514.901131\n",
      "Epoch: 28 \tTraining Loss: 410.297259\n",
      "Epoch: 28 \tTraining Loss: 514.848064\n",
      "Epoch: 28 \tTraining Loss: 508.678987\n",
      "Epoch: 28 \tTraining Loss: 1448.176344\n",
      "Epoch: 28 \tTraining Loss: 1644.766803\n",
      "Epoch: 28 \tTraining Loss: 400.495286\n",
      "Epoch: 28 \tTraining Loss: 336.608958\n",
      "Epoch: 28 \tTraining Loss: 923.259947\n",
      "Epoch: 28 \tTraining Loss: 1177.714543\n",
      "Epoch: 28 \tTraining Loss: 317.366948\n",
      "Epoch: 28 \tTraining Loss: 1369.305975\n",
      "Epoch: 28 \tTraining Loss: 643.229154\n",
      "mse: 71924.430\n",
      "Epoch: 29 \tTraining Loss: 967.830088\n",
      "Epoch: 29 \tTraining Loss: 506.466688\n",
      "Epoch: 29 \tTraining Loss: 509.323919\n",
      "Epoch: 29 \tTraining Loss: 421.655594\n",
      "Epoch: 29 \tTraining Loss: 1684.652713\n",
      "Epoch: 29 \tTraining Loss: 1974.261328\n",
      "Epoch: 29 \tTraining Loss: 512.410695\n",
      "Epoch: 29 \tTraining Loss: 1099.465806\n",
      "Epoch: 29 \tTraining Loss: 1687.901763\n",
      "Epoch: 29 \tTraining Loss: 447.753075\n",
      "Epoch: 29 \tTraining Loss: 978.072452\n",
      "Epoch: 29 \tTraining Loss: 584.776854\n",
      "Epoch: 29 \tTraining Loss: 579.411825\n",
      "Epoch: 29 \tTraining Loss: 496.429463\n",
      "Epoch: 29 \tTraining Loss: 1082.150766\n",
      "Epoch: 29 \tTraining Loss: 939.142613\n",
      "Epoch: 29 \tTraining Loss: 694.799207\n",
      "Epoch: 29 \tTraining Loss: 352.445358\n",
      "Epoch: 29 \tTraining Loss: 525.596534\n",
      "Epoch: 29 \tTraining Loss: 407.806741\n",
      "Epoch: 29 \tTraining Loss: 1162.840379\n",
      "Epoch: 29 \tTraining Loss: 1301.058649\n",
      "Epoch: 29 \tTraining Loss: 1364.661991\n",
      "Epoch: 29 \tTraining Loss: 459.558125\n",
      "Epoch: 29 \tTraining Loss: 267.260715\n",
      "Epoch: 29 \tTraining Loss: 824.589236\n",
      "Epoch: 29 \tTraining Loss: 491.727825\n",
      "Epoch: 29 \tTraining Loss: 452.798629\n",
      "Epoch: 29 \tTraining Loss: 518.131859\n",
      "Epoch: 29 \tTraining Loss: 307.488432\n",
      "Epoch: 29 \tTraining Loss: 1269.103077\n",
      "Epoch: 29 \tTraining Loss: 1831.074164\n",
      "mse: 74048.489\n",
      "Epoch: 30 \tTraining Loss: 948.817235\n",
      "Epoch: 30 \tTraining Loss: 1103.919062\n",
      "Epoch: 30 \tTraining Loss: 929.110729\n",
      "Epoch: 30 \tTraining Loss: 434.442953\n",
      "Epoch: 30 \tTraining Loss: 1454.822795\n",
      "Epoch: 30 \tTraining Loss: 717.960084\n",
      "Epoch: 30 \tTraining Loss: 467.497826\n",
      "Epoch: 30 \tTraining Loss: 444.533335\n",
      "Epoch: 30 \tTraining Loss: 496.671974\n",
      "Epoch: 30 \tTraining Loss: 924.907445\n",
      "Epoch: 30 \tTraining Loss: 787.340823\n",
      "Epoch: 30 \tTraining Loss: 1257.780397\n",
      "Epoch: 30 \tTraining Loss: 362.159718\n",
      "Epoch: 30 \tTraining Loss: 975.361251\n",
      "Epoch: 30 \tTraining Loss: 1152.714695\n",
      "Epoch: 30 \tTraining Loss: 465.955636\n",
      "Epoch: 30 \tTraining Loss: 1012.912927\n",
      "Epoch: 30 \tTraining Loss: 1277.498449\n",
      "Epoch: 30 \tTraining Loss: 506.729531\n",
      "Epoch: 30 \tTraining Loss: 2035.664332\n",
      "Epoch: 30 \tTraining Loss: 962.415461\n",
      "Epoch: 30 \tTraining Loss: 727.917839\n",
      "Epoch: 30 \tTraining Loss: 334.340767\n",
      "Epoch: 30 \tTraining Loss: 1061.789786\n",
      "Epoch: 30 \tTraining Loss: 711.991209\n",
      "Epoch: 30 \tTraining Loss: 1173.864186\n",
      "Epoch: 30 \tTraining Loss: 678.695944\n",
      "Epoch: 30 \tTraining Loss: 427.248919\n",
      "Epoch: 30 \tTraining Loss: 410.848201\n",
      "Epoch: 30 \tTraining Loss: 462.964633\n",
      "Epoch: 30 \tTraining Loss: 985.409677\n",
      "Epoch: 30 \tTraining Loss: 310.513323\n",
      "mse: 74820.340\n",
      "Epoch: 31 \tTraining Loss: 1061.537144\n",
      "Epoch: 31 \tTraining Loss: 927.701716\n",
      "Epoch: 31 \tTraining Loss: 1153.907249\n",
      "Epoch: 31 \tTraining Loss: 894.478712\n",
      "Epoch: 31 \tTraining Loss: 952.987620\n",
      "Epoch: 31 \tTraining Loss: 397.461295\n",
      "Epoch: 31 \tTraining Loss: 458.853685\n",
      "Epoch: 31 \tTraining Loss: 1124.770782\n",
      "Epoch: 31 \tTraining Loss: 773.209067\n",
      "Epoch: 31 \tTraining Loss: 601.214456\n",
      "Epoch: 31 \tTraining Loss: 669.807893\n",
      "Epoch: 31 \tTraining Loss: 353.738431\n",
      "Epoch: 31 \tTraining Loss: 1137.936293\n",
      "Epoch: 31 \tTraining Loss: 988.821039\n",
      "Epoch: 31 \tTraining Loss: 836.441467\n",
      "Epoch: 31 \tTraining Loss: 376.054434\n",
      "Epoch: 31 \tTraining Loss: 1219.869978\n",
      "Epoch: 31 \tTraining Loss: 1251.342188\n",
      "Epoch: 31 \tTraining Loss: 1141.432999\n",
      "Epoch: 31 \tTraining Loss: 698.041238\n",
      "Epoch: 31 \tTraining Loss: 1338.283806\n",
      "Epoch: 31 \tTraining Loss: 355.601929\n",
      "Epoch: 31 \tTraining Loss: 1219.497238\n",
      "Epoch: 31 \tTraining Loss: 544.030994\n",
      "Epoch: 31 \tTraining Loss: 1047.080887\n",
      "Epoch: 31 \tTraining Loss: 994.360588\n",
      "Epoch: 31 \tTraining Loss: 513.641439\n",
      "Epoch: 31 \tTraining Loss: 1137.147308\n",
      "Epoch: 31 \tTraining Loss: 619.353966\n",
      "Epoch: 31 \tTraining Loss: 352.479464\n",
      "Epoch: 31 \tTraining Loss: 467.016636\n",
      "Epoch: 31 \tTraining Loss: 446.619514\n",
      "mse: 74179.772\n",
      "Epoch: 32 \tTraining Loss: 1525.440091\n",
      "Epoch: 32 \tTraining Loss: 1213.023192\n",
      "Epoch: 32 \tTraining Loss: 376.763702\n",
      "Epoch: 32 \tTraining Loss: 352.680232\n",
      "Epoch: 32 \tTraining Loss: 912.494833\n",
      "Epoch: 32 \tTraining Loss: 391.276004\n",
      "Epoch: 32 \tTraining Loss: 905.439828\n",
      "Epoch: 32 \tTraining Loss: 475.496282\n",
      "Epoch: 32 \tTraining Loss: 626.373845\n",
      "Epoch: 32 \tTraining Loss: 516.387618\n",
      "Epoch: 32 \tTraining Loss: 616.363212\n",
      "Epoch: 32 \tTraining Loss: 295.977084\n",
      "Epoch: 32 \tTraining Loss: 366.490097\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-1d10e5ec2162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# number of epochs to train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-1d10e5ec2162>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, n_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            y_hat = y_hat.view(y_hat.shape[0]).double()\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        eval_model(model, val_loader)\n",
    "\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 250\n",
    "train(model, train_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}