{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "fb43f3484ec9eb2eb818f2c4b60702eb23b3e73ba2f9349f357001e8ed925189"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 29\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"/Users/prashanti.nilayam/Desktop/temp/\"\n",
    "prev_value_map = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_df = pd.read_csv(DATA_PATH + 'train_listfile.csv') \n",
    "train_files = train_y_df[\"stay\"].unique().tolist()\n",
    "val_y_df = pd.read_csv(DATA_PATH + 'train_listfile.csv') \n",
    "val_files = val_y_df[\"stay\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "29168"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "len(train_y_df[\"stay\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value_map = {\"Capillary refill rate\": 0.0,\n",
    "    \"Diastolic blood pressure\": 59.0,\n",
    "    \"Fraction inspired oxygen\": 0.21,\n",
    "    \"Glascow coma scale eye opening\": 4,\n",
    "    \"Glascow coma scale motor response\": 6,\n",
    "    \"Glascow coma scale total\": 15,\n",
    "    \"Glascow coma scale verbal response\": 5,\n",
    "    \"Glucose\": 128.0,\n",
    "    \"Heart Rate\": 86,\n",
    "    \"Height\": 170.0,\n",
    "    \"Mean blood pressure\": 77.0,\n",
    "    \"Oxygen saturation\": 98.0,\n",
    "    \"Respiratory rate\": 19,\n",
    "    \"Systolic blood pressure\": 118.0,\n",
    "    \"Temperature\": 36.6,\n",
    "    \"Weight\": 81.0,\n",
    "    \"pH\": 7.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_map = {\n",
    "    \"Glascow coma scale eye opening\":{\n",
    "        \"1 No Response\" : 1,\n",
    "        \"No Response\" : 1,\n",
    "        \"2 To pain\" : 2,\n",
    "        \"To Pain\" : 2,\n",
    "        \"3 To speech\" : 3,\n",
    "        \"To Speech\" : 3,\n",
    "        \"4 Spontaneously\" : 4,\n",
    "        \"Spontaneously\" : 4,\n",
    "        \"None\" : 5\n",
    "    },\n",
    "    \"Glascow coma scale motor response\":{\n",
    "        \"1 No Response\": 1,\n",
    "        \"2 Abnorm extensn\" : 2,\n",
    "        \"Abnormal extension\": 2,\n",
    "        \"3 Abnorm flexion\": 3,\n",
    "        \"Abnormal Flexion\": 3,\n",
    "        \"4 Flex-withdraws\" : 4,\n",
    "        \"Flex-withdraws\": 4,\n",
    "        \"5 Localizes Pain\": 5,\n",
    "        \"Localizes Pain\": 5,\n",
    "        \"6 Obeys Commands\": 6,\n",
    "        \"Obeys Commands\": 6,\n",
    "        \"No response\" : 7,\n",
    "    },\n",
    "    \"Glascow coma scale verbal response\":{\n",
    "        \"1 No Response\" :1,\n",
    "        \"No Response\":1,\n",
    "        \"2 Incomp sounds\": 2,\n",
    "        \"Incomprehensible sounds\":2,\n",
    "        \"3 Inapprop words\":3,\n",
    "        \"Inappropriate Words\":3,\n",
    "        \"4 Confused\":4,\n",
    "        \"Confused\":4,\n",
    "        \"5 Oriented\":5,\n",
    "        \"Oriented\":5,\n",
    "        \"No Response-ETT\":6,\n",
    "        \"1.0 ET/Trach\":7\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(elem):\n",
    "    return np.concatenate([np.array(i) for i in elem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(episode_df):\n",
    "    episode_df[\"Glascow coma scale eye opening\"] = episode_df[\"Glascow coma scale eye opening\"].apply(lambda x: replacement_map[\"Glascow coma scale eye opening\"][x] if x in replacement_map[\"Glascow coma scale eye opening\"] else x)\n",
    "    episode_df[\"Glascow coma scale motor response\"] = episode_df[\"Glascow coma scale motor response\"].apply(lambda x: replacement_map[\"Glascow coma scale motor response\"][x] if x in replacement_map[\"Glascow coma scale motor response\"] else x)\n",
    "    episode_df[\"Glascow coma scale verbal response\"] = episode_df[\"Glascow coma scale verbal response\"].apply(lambda x: replacement_map[\"Glascow coma scale verbal response\"][x] if x in replacement_map[\"Glascow coma scale verbal response\"] else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_capillary_refill_rate(person_id, value, colname):\n",
    "    if value is not None and not np.isnan(value):\n",
    "        prev_value_map[person_id][colname] = value\n",
    "        return value\n",
    "    if person_id in prev_value_map and colname in prev_value_map[person_id] and prev_value_map[person_id][colname] is not None:\n",
    "        prev = prev_value_map[person_id][colname]\n",
    "    else:\n",
    "        prev = default_value_map[colname]\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(pateint_id, episode_df):\n",
    "     prev_value_map[pateint_id] = {}\n",
    "     episode_df[\"Capillary refill rate\"] = episode_df[\"Capillary refill rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Capillary refill rate\"))\n",
    "     episode_df[\"Diastolic blood pressure\"] = episode_df[\"Diastolic blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Diastolic blood pressure\"))\n",
    "     episode_df[\"Fraction inspired oxygen\"] = episode_df[\"Fraction inspired oxygen\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Fraction inspired oxygen\"))\n",
    "     episode_df[\"Glascow coma scale eye opening\"] = episode_df[\"Glascow coma scale eye opening\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale eye opening\"))\n",
    "     episode_df[\"Glascow coma scale motor response\"] = episode_df[\"Glascow coma scale motor response\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale motor response\"))\n",
    "     episode_df[\"Glascow coma scale total\"] = episode_df[\"Glascow coma scale total\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale total\"))\n",
    "     episode_df[\"Glascow coma scale verbal response\"] = episode_df[\"Glascow coma scale verbal response\"].apply(lambda x: process_capillary_refill_rate(pateint_id,  x, \"Glascow coma scale verbal response\"))\n",
    "     episode_df[\"Glucose\"] = episode_df[\"Glucose\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glucose\"))\n",
    "     episode_df[\"Heart Rate\"] = episode_df[\"Heart Rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Heart Rate\"))\n",
    "     episode_df[\"Mean blood pressure\"] = episode_df[\"Mean blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Mean blood pressure\"))\n",
    "     episode_df[\"Height\"] = episode_df[\"Height\"].apply(lambda x: process_capillary_refill_rate(pateint_id,x, \"Height\"))\n",
    "     episode_df[\"Oxygen saturation\"] = episode_df[\"Oxygen saturation\"].apply(lambda x: process_capillary_refill_rate(pateint_id,  x, \"Oxygen saturation\"))\n",
    "     episode_df[\"Respiratory rate\"] = episode_df[\"Respiratory rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Respiratory rate\"))\n",
    "     episode_df[\"Systolic blood pressure\"] = episode_df[\"Systolic blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Systolic blood pressure\"))\n",
    "     episode_df[\"Temperature\"] = episode_df[\"Temperature\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Temperature\"))\n",
    "     episode_df[\"Weight\"] = episode_df[\"Weight\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Weight\"))\n",
    "     episode_df[\"pH\"] = episode_df[\"pH\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"pH\"))\n",
    "     del prev_value_map[pateint_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_indices(data_len):\n",
    "    i = 0\n",
    "    indices = []\n",
    "    while i <= data_len-4:\n",
    "        indices.append([i, i+1, i+2, i+3])\n",
    "        i +=1\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "get_window_indices(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    x_path = DATA_PATH +'/'+path+'/'\n",
    "    X = torch.empty(0,17,4)\n",
    "    Y = torch.empty(0,)\n",
    "    y_df = pd.read_csv(DATA_PATH + path +'_listfile.csv') \n",
    "    data_files = os.listdir(x_path)\n",
    "    print(data_files)\n",
    "    for data_file in data_files:\n",
    "        print(data_file)\n",
    "        if data_file.endswith(\".csv\"):\n",
    "            episode_df = pd.read_csv(x_path + data_file)\n",
    "            cleanup(episode_df)\n",
    "            fill_missing_values(data_file, episode_df)\n",
    "            episode_df[\"H_IDX\"] = episode_df.Hours.apply(np.floor).astype('int32')\n",
    "            episode_df = episode_df.groupby(by = \"H_IDX\").mean()\n",
    "            episode_df = episode_df[episode_df.Hours>=5].reset_index(drop = True)\n",
    "            temp_y = y_df[y_df.stay == data_file].sort_values(by = \"period_length\").reset_index(drop = True)\n",
    "            temp_y = temp_y[[\"period_length\", \"y_true\"]].set_index(\"period_length\")\n",
    "            episode_df = episode_df.join(temp_y, how = \"inner\").drop('Hours', axis=1).reset_index(drop = True)\n",
    "            if(len(episode_df) >0):\n",
    "                indices = get_window_indices(len(episode_df))\n",
    "                windows = []\n",
    "                y_values = []\n",
    "                for idx in indices:\n",
    "                    window = episode_df.loc[idx]\n",
    "                    y_values.append(window.loc[idx[-1]].y_true)\n",
    "                    windows.append(window.drop(\"y_true\", axis=1).transpose().values.astype(np.float32))\n",
    "                t_windows = torch.tensor(windows)\n",
    "                t_y_values = torch.tensor(y_values)\n",
    "                X = torch.cat((X, t_windows), 0)\n",
    "                Y = torch.cat((Y, t_y_values), 0)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['.DS_Store', '10021_episode1_timeseries.csv', '10010_episode1_timeseries.csv', '10003_episode1_timeseries.csv', '10014_episode1_timeseries.csv', '10007_episode1_timeseries.csv', '1000_episode1_timeseries.csv', '10017_episode1_timeseries.csv', '10022_episode1_timeseries.csv', '10013_episode1_timeseries.csv']\n",
      ".DS_Store\n",
      "10021_episode1_timeseries.csv\n",
      "10010_episode1_timeseries.csv\n",
      "10003_episode1_timeseries.csv\n",
      "10014_episode1_timeseries.csv\n",
      "10007_episode1_timeseries.csv\n",
      "1000_episode1_timeseries.csv\n",
      "10017_episode1_timeseries.csv\n",
      "10022_episode1_timeseries.csv\n",
      "10013_episode1_timeseries.csv\n",
      "['10006_episode1_timeseries.csv', '.DS_Store', '10004_episode2_timeseries.csv', '10004_episode1_timeseries.csv']\n",
      "10006_episode1_timeseries.csv\n",
      ".DS_Store\n",
      "10004_episode2_timeseries.csv\n",
      "10004_episode1_timeseries.csv\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EpisodeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, obs, los):\n",
    "        self.x = obs\n",
    "        self.y = los\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "        \n",
    "X_train, Y_train = preprocess('train')\n",
    "train_dataset = EpisodeDataset(X_train, Y_train)\n",
    "X_val, Y_val = preprocess('val')\n",
    "val_dataset = EpisodeDataset(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True)                              \n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=32, shuffle=False)                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [5.0000e+01, 5.7000e+01, 5.7000e+01, 5.1000e+01],\n        [2.1000e-01, 2.1000e-01, 2.1000e-01, 2.1000e-01],\n        [4.0000e+00, 4.0000e+00, 4.0000e+00, 4.0000e+00],\n        [6.0000e+00, 6.0000e+00, 6.0000e+00, 6.0000e+00],\n        [1.5000e+01, 1.5000e+01, 1.5000e+01, 1.5000e+01],\n        [5.0000e+00, 5.0000e+00, 5.0000e+00, 5.0000e+00],\n        [9.3000e+01, 2.3200e+02, 2.3200e+02, 2.3200e+02],\n        [7.6000e+01, 7.4000e+01, 7.0000e+01, 7.1000e+01],\n        [1.7000e+02, 1.7000e+02, 1.7000e+02, 1.7000e+02],\n        [7.2333e+01, 8.0667e+01, 8.0667e+01, 7.4333e+01],\n        [1.0000e+02, 1.0000e+02, 1.0000e+02, 9.8000e+01],\n        [2.2000e+01, 1.5000e+01, 2.2000e+01, 2.0000e+01],\n        [1.1700e+02, 1.2800e+02, 1.2800e+02, 1.2100e+02],\n        [3.7556e+01, 3.6444e+01, 3.6444e+01, 3.7389e+01],\n        [8.1000e+01, 8.1000e+01, 8.1000e+01, 8.1000e+01],\n        [7.4000e+00, 7.4000e+00, 7.4000e+00, 7.4000e+00]]) tensor(31.1800, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for data in val_dataset:\n",
    "    print(data[0], data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EpisiodeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EpisiodeCNN, self).__init__()\n",
    "        #input shape 1 * 17 * 4\n",
    "        #output shape 17 * 17 * 4\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=17, kernel_size=3, padding=1, stride = 1)\n",
    "        #input shape 17 * 17 * 4\n",
    "        #output shape 68 * 17 * 4\n",
    "        self.conv2 = nn.Conv2d(in_channels=17, out_channels=34, kernel_size=3, padding=1, stride = 1)\n",
    "        #input shape 34 * 17 * 4\n",
    "        #output shape 34 * 8 * 2\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        #input shape 34 * 8 * 2\n",
    "        #output shape 68 * 8 * 2\n",
    "        self.conv3 = nn.Conv2d(in_channels=34, out_channels=68, kernel_size=3, padding=1, stride = 1)\n",
    "        self.fc1 = nn.Linear(68*8*2, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #input is of shape (batch_size=32, 3, 224, 224) if you did the dataloader right\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = x.view(-1, 68 * 8 * 2)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EpisiodeCNN()\n",
    "learning_rate = 0.00001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_y_true = torch.DoubleTensor()\n",
    "    all_y_pred = torch.DoubleTensor()\n",
    "    for x, y in val_loader:\n",
    "        y_hat = model(x)\n",
    "        all_y_true = torch.cat((all_y_true, y.to('cpu')), dim=0)\n",
    "        all_y_pred = torch.cat((all_y_pred,  y_hat.to('cpu')), dim=0)\n",
    "    mse= mean_squared_error(all_y_true.detach().numpy(), all_y_pred.detach().numpy())\n",
    "    print(f\"mse: {mse:.3f}\")\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "h: 9 \tTraining Loss: 4310.898022\n",
      "Epoch: 9 \tTraining Loss: 6857.695369\n",
      "Epoch: 9 \tTraining Loss: 6042.648013\n",
      "Epoch: 9 \tTraining Loss: 5710.431808\n",
      "Epoch: 9 \tTraining Loss: 6019.340248\n",
      "Epoch: 9 \tTraining Loss: 4779.232258\n",
      "Epoch: 9 \tTraining Loss: 7678.051793\n",
      "Epoch: 9 \tTraining Loss: 4252.709808\n",
      "mse: 6717.440\n",
      "Epoch: 10 \tTraining Loss: 5621.502651\n",
      "Epoch: 10 \tTraining Loss: 3446.792672\n",
      "Epoch: 10 \tTraining Loss: 5241.648207\n",
      "Epoch: 10 \tTraining Loss: 7379.397649\n",
      "Epoch: 10 \tTraining Loss: 7109.256326\n",
      "Epoch: 10 \tTraining Loss: 4614.747427\n",
      "Epoch: 10 \tTraining Loss: 6436.230716\n",
      "Epoch: 10 \tTraining Loss: 4613.525433\n",
      "Epoch: 10 \tTraining Loss: 3147.882323\n",
      "Epoch: 10 \tTraining Loss: 6322.174237\n",
      "Epoch: 10 \tTraining Loss: 5995.472556\n",
      "Epoch: 10 \tTraining Loss: 4574.608463\n",
      "Epoch: 10 \tTraining Loss: 5197.615795\n",
      "Epoch: 10 \tTraining Loss: 4761.694044\n",
      "Epoch: 10 \tTraining Loss: 4312.591152\n",
      "Epoch: 10 \tTraining Loss: 5110.882983\n",
      "Epoch: 10 \tTraining Loss: 4853.644943\n",
      "Epoch: 10 \tTraining Loss: 6569.663020\n",
      "Epoch: 10 \tTraining Loss: 4679.657305\n",
      "Epoch: 10 \tTraining Loss: 5688.761098\n",
      "Epoch: 10 \tTraining Loss: 6387.383524\n",
      "Epoch: 10 \tTraining Loss: 4548.729707\n",
      "Epoch: 10 \tTraining Loss: 6031.400105\n",
      "Epoch: 10 \tTraining Loss: 3063.370045\n",
      "Epoch: 10 \tTraining Loss: 5016.766705\n",
      "Epoch: 10 \tTraining Loss: 3177.577277\n",
      "Epoch: 10 \tTraining Loss: 3856.792867\n",
      "Epoch: 10 \tTraining Loss: 5360.262368\n",
      "Epoch: 10 \tTraining Loss: 4853.453082\n",
      "Epoch: 10 \tTraining Loss: 3863.384610\n",
      "Epoch: 10 \tTraining Loss: 5656.560857\n",
      "Epoch: 10 \tTraining Loss: 7314.766578\n",
      "mse: 5106.655\n",
      "Epoch: 11 \tTraining Loss: 4351.365124\n",
      "Epoch: 11 \tTraining Loss: 4060.981174\n",
      "Epoch: 11 \tTraining Loss: 4569.570204\n",
      "Epoch: 11 \tTraining Loss: 4872.338254\n",
      "Epoch: 11 \tTraining Loss: 4393.322216\n",
      "Epoch: 11 \tTraining Loss: 4654.550690\n",
      "Epoch: 11 \tTraining Loss: 3540.538519\n",
      "Epoch: 11 \tTraining Loss: 3966.141062\n",
      "Epoch: 11 \tTraining Loss: 3731.855052\n",
      "Epoch: 11 \tTraining Loss: 3264.760250\n",
      "Epoch: 11 \tTraining Loss: 2897.746643\n",
      "Epoch: 11 \tTraining Loss: 3073.785547\n",
      "Epoch: 11 \tTraining Loss: 4576.425741\n",
      "Epoch: 11 \tTraining Loss: 4031.591931\n",
      "Epoch: 11 \tTraining Loss: 4243.343150\n",
      "Epoch: 11 \tTraining Loss: 4659.028828\n",
      "Epoch: 11 \tTraining Loss: 7037.094776\n",
      "Epoch: 11 \tTraining Loss: 4154.607269\n",
      "Epoch: 11 \tTraining Loss: 3841.796225\n",
      "Epoch: 11 \tTraining Loss: 3292.152801\n",
      "Epoch: 11 \tTraining Loss: 4497.405606\n",
      "Epoch: 11 \tTraining Loss: 4075.470153\n",
      "Epoch: 11 \tTraining Loss: 5687.801977\n",
      "Epoch: 11 \tTraining Loss: 4956.394751\n",
      "Epoch: 11 \tTraining Loss: 3601.944083\n",
      "Epoch: 11 \tTraining Loss: 3928.382118\n",
      "Epoch: 11 \tTraining Loss: 4786.637021\n",
      "Epoch: 11 \tTraining Loss: 5174.704442\n",
      "Epoch: 11 \tTraining Loss: 4850.258550\n",
      "Epoch: 11 \tTraining Loss: 3993.031010\n",
      "Epoch: 11 \tTraining Loss: 4096.536249\n",
      "Epoch: 11 \tTraining Loss: 4419.114566\n",
      "mse: 9517.379\n",
      "Epoch: 12 \tTraining Loss: 3838.822081\n",
      "Epoch: 12 \tTraining Loss: 4653.056792\n",
      "Epoch: 12 \tTraining Loss: 3609.332199\n",
      "Epoch: 12 \tTraining Loss: 2255.644384\n",
      "Epoch: 12 \tTraining Loss: 4423.901112\n",
      "Epoch: 12 \tTraining Loss: 4343.406722\n",
      "Epoch: 12 \tTraining Loss: 3574.524348\n",
      "Epoch: 12 \tTraining Loss: 4678.309822\n",
      "Epoch: 12 \tTraining Loss: 4048.425645\n",
      "Epoch: 12 \tTraining Loss: 3351.318802\n",
      "Epoch: 12 \tTraining Loss: 4203.662806\n",
      "Epoch: 12 \tTraining Loss: 3517.138065\n",
      "Epoch: 12 \tTraining Loss: 3991.040921\n",
      "Epoch: 12 \tTraining Loss: 3638.796491\n",
      "Epoch: 12 \tTraining Loss: 3894.063634\n",
      "Epoch: 12 \tTraining Loss: 3248.935558\n",
      "Epoch: 12 \tTraining Loss: 2522.669076\n",
      "Epoch: 12 \tTraining Loss: 3730.528184\n",
      "Epoch: 12 \tTraining Loss: 3831.725907\n",
      "Epoch: 12 \tTraining Loss: 3280.607643\n",
      "Epoch: 12 \tTraining Loss: 2808.058974\n",
      "Epoch: 12 \tTraining Loss: 2262.501390\n",
      "Epoch: 12 \tTraining Loss: 2273.682177\n",
      "Epoch: 12 \tTraining Loss: 3600.549334\n",
      "Epoch: 12 \tTraining Loss: 2530.831801\n",
      "Epoch: 12 \tTraining Loss: 3257.577736\n",
      "Epoch: 12 \tTraining Loss: 3726.988239\n",
      "Epoch: 12 \tTraining Loss: 1832.358419\n",
      "Epoch: 12 \tTraining Loss: 2767.815113\n",
      "Epoch: 12 \tTraining Loss: 3284.315574\n",
      "Epoch: 12 \tTraining Loss: 3793.461006\n",
      "Epoch: 12 \tTraining Loss: 3155.965407\n",
      "mse: 24930.029\n",
      "Epoch: 13 \tTraining Loss: 2653.009860\n",
      "Epoch: 13 \tTraining Loss: 3043.976985\n",
      "Epoch: 13 \tTraining Loss: 2703.023258\n",
      "Epoch: 13 \tTraining Loss: 3624.403927\n",
      "Epoch: 13 \tTraining Loss: 2878.131000\n",
      "Epoch: 13 \tTraining Loss: 3177.628457\n",
      "Epoch: 13 \tTraining Loss: 2086.500433\n",
      "Epoch: 13 \tTraining Loss: 3327.742411\n",
      "Epoch: 13 \tTraining Loss: 2728.884747\n",
      "Epoch: 13 \tTraining Loss: 3367.501315\n",
      "Epoch: 13 \tTraining Loss: 2670.355984\n",
      "Epoch: 13 \tTraining Loss: 3095.904389\n",
      "Epoch: 13 \tTraining Loss: 2042.790051\n",
      "Epoch: 13 \tTraining Loss: 2929.361398\n",
      "Epoch: 13 \tTraining Loss: 2586.245639\n",
      "Epoch: 13 \tTraining Loss: 3014.044108\n",
      "Epoch: 13 \tTraining Loss: 2073.005952\n",
      "Epoch: 13 \tTraining Loss: 2064.934065\n",
      "Epoch: 13 \tTraining Loss: 2702.490501\n",
      "Epoch: 13 \tTraining Loss: 2628.819883\n",
      "Epoch: 13 \tTraining Loss: 2746.228485\n",
      "Epoch: 13 \tTraining Loss: 2048.693364\n",
      "Epoch: 13 \tTraining Loss: 3098.828555\n",
      "Epoch: 13 \tTraining Loss: 3242.089160\n",
      "Epoch: 13 \tTraining Loss: 3243.474255\n",
      "Epoch: 13 \tTraining Loss: 2966.435112\n",
      "Epoch: 13 \tTraining Loss: 2721.981997\n",
      "Epoch: 13 \tTraining Loss: 3184.530247\n",
      "Epoch: 13 \tTraining Loss: 2357.607902\n",
      "Epoch: 13 \tTraining Loss: 2486.531262\n",
      "Epoch: 13 \tTraining Loss: 2933.961242\n",
      "Epoch: 13 \tTraining Loss: 2929.536532\n",
      "mse: 50506.370\n",
      "Epoch: 14 \tTraining Loss: 2862.295508\n",
      "Epoch: 14 \tTraining Loss: 3412.699519\n",
      "Epoch: 14 \tTraining Loss: 2728.359637\n",
      "Epoch: 14 \tTraining Loss: 2543.327399\n",
      "Epoch: 14 \tTraining Loss: 1850.306805\n",
      "Epoch: 14 \tTraining Loss: 2214.056935\n",
      "Epoch: 14 \tTraining Loss: 3091.009775\n",
      "Epoch: 14 \tTraining Loss: 1714.634541\n",
      "Epoch: 14 \tTraining Loss: 2724.014675\n",
      "Epoch: 14 \tTraining Loss: 2433.794613\n",
      "Epoch: 14 \tTraining Loss: 1954.415610\n",
      "Epoch: 14 \tTraining Loss: 1643.055705\n",
      "Epoch: 14 \tTraining Loss: 3081.749855\n",
      "Epoch: 14 \tTraining Loss: 3120.428756\n",
      "Epoch: 14 \tTraining Loss: 2104.009014\n",
      "Epoch: 14 \tTraining Loss: 2151.128739\n",
      "Epoch: 14 \tTraining Loss: 2358.421849\n",
      "Epoch: 14 \tTraining Loss: 2392.166458\n",
      "Epoch: 14 \tTraining Loss: 3321.456837\n",
      "Epoch: 14 \tTraining Loss: 2285.950798\n",
      "Epoch: 14 \tTraining Loss: 2635.959998\n",
      "Epoch: 14 \tTraining Loss: 2265.678232\n",
      "Epoch: 14 \tTraining Loss: 1987.230113\n",
      "Epoch: 14 \tTraining Loss: 2626.835807\n",
      "Epoch: 14 \tTraining Loss: 2936.957087\n",
      "Epoch: 14 \tTraining Loss: 3343.530945\n",
      "Epoch: 14 \tTraining Loss: 3090.879440\n",
      "Epoch: 14 \tTraining Loss: 2410.740053\n",
      "Epoch: 14 \tTraining Loss: 2367.705893\n",
      "Epoch: 14 \tTraining Loss: 1981.444712\n",
      "Epoch: 14 \tTraining Loss: 2203.208315\n",
      "Epoch: 14 \tTraining Loss: 2514.125005\n",
      "mse: 75635.962\n",
      "Epoch: 15 \tTraining Loss: 2460.476656\n",
      "Epoch: 15 \tTraining Loss: 2283.951685\n",
      "Epoch: 15 \tTraining Loss: 1889.414898\n",
      "Epoch: 15 \tTraining Loss: 2291.539867\n",
      "Epoch: 15 \tTraining Loss: 3217.774948\n",
      "Epoch: 15 \tTraining Loss: 2464.486727\n",
      "Epoch: 15 \tTraining Loss: 2879.076178\n",
      "Epoch: 15 \tTraining Loss: 2273.138115\n",
      "Epoch: 15 \tTraining Loss: 2287.057881\n",
      "Epoch: 15 \tTraining Loss: 2442.584309\n",
      "Epoch: 15 \tTraining Loss: 2194.538530\n",
      "Epoch: 15 \tTraining Loss: 1926.512293\n",
      "Epoch: 15 \tTraining Loss: 2493.327230\n",
      "Epoch: 15 \tTraining Loss: 2298.769152\n",
      "Epoch: 15 \tTraining Loss: 3175.272130\n",
      "Epoch: 15 \tTraining Loss: 2026.578583\n",
      "Epoch: 15 \tTraining Loss: 2490.781826\n",
      "Epoch: 15 \tTraining Loss: 2535.835931\n",
      "Epoch: 15 \tTraining Loss: 1656.044983\n",
      "Epoch: 15 \tTraining Loss: 2747.146848\n",
      "Epoch: 15 \tTraining Loss: 2698.755321\n",
      "Epoch: 15 \tTraining Loss: 2237.521569\n",
      "Epoch: 15 \tTraining Loss: 1676.021315\n",
      "Epoch: 15 \tTraining Loss: 2539.197024\n",
      "Epoch: 15 \tTraining Loss: 2381.806599\n",
      "Epoch: 15 \tTraining Loss: 2382.010984\n",
      "Epoch: 15 \tTraining Loss: 2462.116699\n",
      "Epoch: 15 \tTraining Loss: 2200.578844\n",
      "Epoch: 15 \tTraining Loss: 2165.246873\n",
      "Epoch: 15 \tTraining Loss: 2590.384535\n",
      "Epoch: 15 \tTraining Loss: 2823.135942\n",
      "Epoch: 15 \tTraining Loss: 1958.433494\n",
      "mse: 86479.087\n",
      "Epoch: 16 \tTraining Loss: 2366.103519\n",
      "Epoch: 16 \tTraining Loss: 2367.597512\n",
      "Epoch: 16 \tTraining Loss: 2982.918446\n",
      "Epoch: 16 \tTraining Loss: 2287.186810\n",
      "Epoch: 16 \tTraining Loss: 2407.931354\n",
      "Epoch: 16 \tTraining Loss: 1984.855926\n",
      "Epoch: 16 \tTraining Loss: 1790.673808\n",
      "Epoch: 16 \tTraining Loss: 2802.575191\n",
      "Epoch: 16 \tTraining Loss: 2535.499247\n",
      "Epoch: 16 \tTraining Loss: 2331.672291\n",
      "Epoch: 16 \tTraining Loss: 2522.734378\n",
      "Epoch: 16 \tTraining Loss: 2603.091507\n",
      "Epoch: 16 \tTraining Loss: 2471.470928\n",
      "Epoch: 16 \tTraining Loss: 2337.410057\n",
      "Epoch: 16 \tTraining Loss: 1629.240722\n",
      "Epoch: 16 \tTraining Loss: 2506.424600\n",
      "Epoch: 16 \tTraining Loss: 3351.510041\n",
      "Epoch: 16 \tTraining Loss: 2872.815612\n",
      "Epoch: 16 \tTraining Loss: 2432.634763\n",
      "Epoch: 16 \tTraining Loss: 2255.529080\n",
      "Epoch: 16 \tTraining Loss: 1870.127846\n",
      "Epoch: 16 \tTraining Loss: 2139.073061\n",
      "Epoch: 16 \tTraining Loss: 2802.061658\n",
      "Epoch: 16 \tTraining Loss: 2434.319393\n",
      "Epoch: 16 \tTraining Loss: 2647.389144\n",
      "Epoch: 16 \tTraining Loss: 2132.938156\n",
      "Epoch: 16 \tTraining Loss: 2383.094561\n",
      "Epoch: 16 \tTraining Loss: 2860.404734\n",
      "Epoch: 16 \tTraining Loss: 2027.240071\n",
      "Epoch: 16 \tTraining Loss: 2572.276638\n",
      "Epoch: 16 \tTraining Loss: 1995.735972\n",
      "Epoch: 16 \tTraining Loss: 2073.490574\n",
      "mse: 89884.031\n",
      "Epoch: 17 \tTraining Loss: 2508.575592\n",
      "Epoch: 17 \tTraining Loss: 1816.205184\n",
      "Epoch: 17 \tTraining Loss: 2014.199198\n",
      "Epoch: 17 \tTraining Loss: 2009.853117\n",
      "Epoch: 17 \tTraining Loss: 2498.417095\n",
      "Epoch: 17 \tTraining Loss: 2251.979485\n",
      "Epoch: 17 \tTraining Loss: 2648.837298\n",
      "Epoch: 17 \tTraining Loss: 2801.172320\n",
      "Epoch: 17 \tTraining Loss: 3210.368837\n",
      "Epoch: 17 \tTraining Loss: 2232.388102\n",
      "Epoch: 17 \tTraining Loss: 2524.174896\n",
      "Epoch: 17 \tTraining Loss: 2335.247491\n",
      "Epoch: 17 \tTraining Loss: 2472.469358\n",
      "Epoch: 17 \tTraining Loss: 2877.724478\n",
      "Epoch: 17 \tTraining Loss: 1812.586028\n",
      "Epoch: 17 \tTraining Loss: 2793.181938\n",
      "Epoch: 17 \tTraining Loss: 2810.643578\n",
      "Epoch: 17 \tTraining Loss: 2020.352750\n",
      "Epoch: 17 \tTraining Loss: 2865.090702\n",
      "Epoch: 17 \tTraining Loss: 2380.703692\n",
      "Epoch: 17 \tTraining Loss: 2628.663110\n",
      "Epoch: 17 \tTraining Loss: 2414.109821\n",
      "Epoch: 17 \tTraining Loss: 2256.308751\n",
      "Epoch: 17 \tTraining Loss: 2131.027785\n",
      "Epoch: 17 \tTraining Loss: 2499.419974\n",
      "Epoch: 17 \tTraining Loss: 2482.827419\n",
      "Epoch: 17 \tTraining Loss: 2644.081619\n",
      "Epoch: 17 \tTraining Loss: 3122.735325\n",
      "Epoch: 17 \tTraining Loss: 2091.106475\n",
      "Epoch: 17 \tTraining Loss: 2715.016029\n",
      "Epoch: 17 \tTraining Loss: 1821.654854\n",
      "Epoch: 17 \tTraining Loss: 2357.077453\n",
      "mse: 87982.231\n",
      "Epoch: 18 \tTraining Loss: 2230.524958\n",
      "Epoch: 18 \tTraining Loss: 3151.587929\n",
      "Epoch: 18 \tTraining Loss: 3128.047534\n",
      "Epoch: 18 \tTraining Loss: 2769.278001\n",
      "Epoch: 18 \tTraining Loss: 2093.503491\n",
      "Epoch: 18 \tTraining Loss: 2122.356652\n",
      "Epoch: 18 \tTraining Loss: 2492.070262\n",
      "Epoch: 18 \tTraining Loss: 2514.254218\n",
      "Epoch: 18 \tTraining Loss: 2690.739893\n",
      "Epoch: 18 \tTraining Loss: 2298.762300\n",
      "Epoch: 18 \tTraining Loss: 1978.854137\n",
      "Epoch: 18 \tTraining Loss: 1818.440893\n",
      "Epoch: 18 \tTraining Loss: 2313.313795\n",
      "Epoch: 18 \tTraining Loss: 2590.259316\n",
      "Epoch: 18 \tTraining Loss: 2916.858390\n",
      "Epoch: 18 \tTraining Loss: 1935.423101\n",
      "Epoch: 18 \tTraining Loss: 3155.352451\n",
      "Epoch: 18 \tTraining Loss: 2465.181308\n",
      "Epoch: 18 \tTraining Loss: 2225.737514\n",
      "Epoch: 18 \tTraining Loss: 1990.691030\n",
      "Epoch: 18 \tTraining Loss: 2387.123910\n",
      "Epoch: 18 \tTraining Loss: 2522.097428\n",
      "Epoch: 18 \tTraining Loss: 2145.882451\n",
      "Epoch: 18 \tTraining Loss: 2436.284697\n",
      "Epoch: 18 \tTraining Loss: 2250.668010\n",
      "Epoch: 18 \tTraining Loss: 2411.546916\n",
      "Epoch: 18 \tTraining Loss: 2323.560540\n",
      "Epoch: 18 \tTraining Loss: 2640.894811\n",
      "Epoch: 18 \tTraining Loss: 2432.753362\n",
      "Epoch: 18 \tTraining Loss: 3307.734369\n",
      "Epoch: 18 \tTraining Loss: 1998.876775\n",
      "Epoch: 18 \tTraining Loss: 1894.007303\n",
      "mse: 89652.172\n",
      "Epoch: 19 \tTraining Loss: 2714.819925\n",
      "Epoch: 19 \tTraining Loss: 2573.761660\n",
      "Epoch: 19 \tTraining Loss: 2543.410791\n",
      "Epoch: 19 \tTraining Loss: 2689.172914\n",
      "Epoch: 19 \tTraining Loss: 2848.552702\n",
      "Epoch: 19 \tTraining Loss: 2758.951196\n",
      "Epoch: 19 \tTraining Loss: 1913.707306\n",
      "Epoch: 19 \tTraining Loss: 2350.255173\n",
      "Epoch: 19 \tTraining Loss: 1768.150871\n",
      "Epoch: 19 \tTraining Loss: 2387.398768\n",
      "Epoch: 19 \tTraining Loss: 2652.032418\n",
      "Epoch: 19 \tTraining Loss: 2592.242309\n",
      "Epoch: 19 \tTraining Loss: 3354.798724\n",
      "Epoch: 19 \tTraining Loss: 2924.215088\n",
      "Epoch: 19 \tTraining Loss: 2440.542464\n",
      "Epoch: 19 \tTraining Loss: 2582.414712\n",
      "Epoch: 19 \tTraining Loss: 2209.204143\n",
      "Epoch: 19 \tTraining Loss: 2651.766947\n",
      "Epoch: 19 \tTraining Loss: 2717.764969\n",
      "Epoch: 19 \tTraining Loss: 2145.045156\n",
      "Epoch: 19 \tTraining Loss: 2367.651541\n",
      "Epoch: 19 \tTraining Loss: 2808.857964\n",
      "Epoch: 19 \tTraining Loss: 2079.329063\n",
      "Epoch: 19 \tTraining Loss: 1769.753916\n",
      "Epoch: 19 \tTraining Loss: 2400.377919\n",
      "Epoch: 19 \tTraining Loss: 2123.601672\n",
      "Epoch: 19 \tTraining Loss: 2571.600905\n",
      "Epoch: 19 \tTraining Loss: 2119.819952\n",
      "Epoch: 19 \tTraining Loss: 2261.806519\n",
      "Epoch: 19 \tTraining Loss: 2162.688641\n",
      "Epoch: 19 \tTraining Loss: 1597.827019\n",
      "Epoch: 19 \tTraining Loss: 3126.908775\n",
      "mse: 89083.224\n",
      "Epoch: 20 \tTraining Loss: 2251.238943\n",
      "Epoch: 20 \tTraining Loss: 2884.612564\n",
      "Epoch: 20 \tTraining Loss: 2788.489137\n",
      "Epoch: 20 \tTraining Loss: 2074.144592\n",
      "Epoch: 20 \tTraining Loss: 3181.905221\n",
      "Epoch: 20 \tTraining Loss: 2686.633622\n",
      "Epoch: 20 \tTraining Loss: 2682.830047\n",
      "Epoch: 20 \tTraining Loss: 2756.953157\n",
      "Epoch: 20 \tTraining Loss: 2098.920478\n",
      "Epoch: 20 \tTraining Loss: 2162.966930\n",
      "Epoch: 20 \tTraining Loss: 2543.004759\n",
      "Epoch: 20 \tTraining Loss: 2153.167280\n",
      "Epoch: 20 \tTraining Loss: 2371.604819\n",
      "Epoch: 20 \tTraining Loss: 2197.779996\n",
      "Epoch: 20 \tTraining Loss: 1905.128262\n",
      "Epoch: 20 \tTraining Loss: 2528.313362\n",
      "Epoch: 20 \tTraining Loss: 2595.504196\n",
      "Epoch: 20 \tTraining Loss: 2650.911926\n",
      "Epoch: 20 \tTraining Loss: 2866.721800\n",
      "Epoch: 20 \tTraining Loss: 1628.532224\n",
      "Epoch: 20 \tTraining Loss: 2485.850963\n",
      "Epoch: 20 \tTraining Loss: 1966.084445\n",
      "Epoch: 20 \tTraining Loss: 2388.186986\n",
      "Epoch: 20 \tTraining Loss: 2224.224214\n",
      "Epoch: 20 \tTraining Loss: 2869.682482\n",
      "Epoch: 20 \tTraining Loss: 2627.457925\n",
      "Epoch: 20 \tTraining Loss: 2175.855116\n",
      "Epoch: 20 \tTraining Loss: 2351.840389\n",
      "Epoch: 20 \tTraining Loss: 2898.161779\n",
      "Epoch: 20 \tTraining Loss: 2187.792869\n",
      "Epoch: 20 \tTraining Loss: 2347.307608\n",
      "Epoch: 20 \tTraining Loss: 2556.003206\n",
      "mse: 88668.943\n",
      "Epoch: 21 \tTraining Loss: 2242.351011\n",
      "Epoch: 21 \tTraining Loss: 2218.426965\n",
      "Epoch: 21 \tTraining Loss: 2154.422753\n",
      "Epoch: 21 \tTraining Loss: 2868.339642\n",
      "Epoch: 21 \tTraining Loss: 2346.388032\n",
      "Epoch: 21 \tTraining Loss: 2303.422408\n",
      "Epoch: 21 \tTraining Loss: 2325.217441\n",
      "Epoch: 21 \tTraining Loss: 1956.846750\n",
      "Epoch: 21 \tTraining Loss: 1928.067468\n",
      "Epoch: 21 \tTraining Loss: 2400.631958\n",
      "Epoch: 21 \tTraining Loss: 2755.689599\n",
      "Epoch: 21 \tTraining Loss: 2398.600609\n",
      "Epoch: 21 \tTraining Loss: 2226.132297\n",
      "Epoch: 21 \tTraining Loss: 3508.282765\n",
      "Epoch: 21 \tTraining Loss: 1950.625463\n",
      "Epoch: 21 \tTraining Loss: 2481.179866\n",
      "Epoch: 21 \tTraining Loss: 2253.682300\n",
      "Epoch: 21 \tTraining Loss: 2692.637868\n",
      "Epoch: 21 \tTraining Loss: 2765.712692\n",
      "Epoch: 21 \tTraining Loss: 1731.913031\n",
      "Epoch: 21 \tTraining Loss: 2030.592824\n",
      "Epoch: 21 \tTraining Loss: 2680.414852\n",
      "Epoch: 21 \tTraining Loss: 1938.583083\n",
      "Epoch: 21 \tTraining Loss: 2968.109213\n",
      "Epoch: 21 \tTraining Loss: 2722.269570\n",
      "Epoch: 21 \tTraining Loss: 3170.802772\n",
      "Epoch: 21 \tTraining Loss: 2226.168624\n",
      "Epoch: 21 \tTraining Loss: 2676.450285\n",
      "Epoch: 21 \tTraining Loss: 2271.363174\n",
      "Epoch: 21 \tTraining Loss: 2361.247244\n",
      "Epoch: 21 \tTraining Loss: 2185.324289\n",
      "Epoch: 21 \tTraining Loss: 2135.116592\n",
      "mse: 90123.677\n",
      "Epoch: 22 \tTraining Loss: 2128.972875\n",
      "Epoch: 22 \tTraining Loss: 2537.612004\n",
      "Epoch: 22 \tTraining Loss: 2928.385435\n",
      "Epoch: 22 \tTraining Loss: 2372.661320\n",
      "Epoch: 22 \tTraining Loss: 2673.125032\n",
      "Epoch: 22 \tTraining Loss: 2283.143250\n",
      "Epoch: 22 \tTraining Loss: 2507.999444\n",
      "Epoch: 22 \tTraining Loss: 2319.008754\n",
      "Epoch: 22 \tTraining Loss: 2692.849760\n",
      "Epoch: 22 \tTraining Loss: 1577.490589\n",
      "Epoch: 22 \tTraining Loss: 2406.994053\n",
      "Epoch: 22 \tTraining Loss: 2463.998892\n",
      "Epoch: 22 \tTraining Loss: 2636.802789\n",
      "Epoch: 22 \tTraining Loss: 2254.511151\n",
      "Epoch: 22 \tTraining Loss: 2194.768958\n",
      "Epoch: 22 \tTraining Loss: 2103.782145\n",
      "Epoch: 22 \tTraining Loss: 2184.400118\n",
      "Epoch: 22 \tTraining Loss: 2363.988999\n",
      "Epoch: 22 \tTraining Loss: 2081.578563\n",
      "Epoch: 22 \tTraining Loss: 2700.369782\n",
      "Epoch: 22 \tTraining Loss: 2772.098171\n",
      "Epoch: 22 \tTraining Loss: 2332.733501\n",
      "Epoch: 22 \tTraining Loss: 2507.864095\n",
      "Epoch: 22 \tTraining Loss: 2344.683501\n",
      "Epoch: 22 \tTraining Loss: 2723.374938\n",
      "Epoch: 22 \tTraining Loss: 3000.884738\n",
      "Epoch: 22 \tTraining Loss: 2107.886395\n",
      "Epoch: 22 \tTraining Loss: 2294.562872\n",
      "Epoch: 22 \tTraining Loss: 3037.149469\n",
      "Epoch: 22 \tTraining Loss: 2604.033696\n",
      "Epoch: 22 \tTraining Loss: 2022.120498\n",
      "Epoch: 22 \tTraining Loss: 1786.521664\n",
      "mse: 90959.537\n",
      "Epoch: 23 \tTraining Loss: 1904.552679\n",
      "Epoch: 23 \tTraining Loss: 2886.775052\n",
      "Epoch: 23 \tTraining Loss: 2837.332159\n",
      "Epoch: 23 \tTraining Loss: 2770.811543\n",
      "Epoch: 23 \tTraining Loss: 2547.438647\n",
      "Epoch: 23 \tTraining Loss: 1791.968532\n",
      "Epoch: 23 \tTraining Loss: 2422.692848\n",
      "Epoch: 23 \tTraining Loss: 2702.521602\n",
      "Epoch: 23 \tTraining Loss: 1906.804417\n",
      "Epoch: 23 \tTraining Loss: 2313.744573\n",
      "Epoch: 23 \tTraining Loss: 1805.783527\n",
      "Epoch: 23 \tTraining Loss: 1924.511896\n",
      "Epoch: 23 \tTraining Loss: 2813.959648\n",
      "Epoch: 23 \tTraining Loss: 2584.144305\n",
      "Epoch: 23 \tTraining Loss: 2392.169336\n",
      "Epoch: 23 \tTraining Loss: 2228.038206\n",
      "Epoch: 23 \tTraining Loss: 2740.925981\n",
      "Epoch: 23 \tTraining Loss: 2347.388132\n",
      "Epoch: 23 \tTraining Loss: 2635.340535\n",
      "Epoch: 23 \tTraining Loss: 2093.692094\n",
      "Epoch: 23 \tTraining Loss: 2552.013668\n",
      "Epoch: 23 \tTraining Loss: 2004.146927\n",
      "Epoch: 23 \tTraining Loss: 2289.208737\n",
      "Epoch: 23 \tTraining Loss: 2325.564791\n",
      "Epoch: 23 \tTraining Loss: 2439.811865\n",
      "Epoch: 23 \tTraining Loss: 2865.946595\n",
      "Epoch: 23 \tTraining Loss: 2521.424825\n",
      "Epoch: 23 \tTraining Loss: 1931.064824\n",
      "Epoch: 23 \tTraining Loss: 3104.494989\n",
      "Epoch: 23 \tTraining Loss: 2317.482072\n",
      "Epoch: 23 \tTraining Loss: 2916.466213\n",
      "Epoch: 23 \tTraining Loss: 2482.881165\n",
      "mse: 90939.551\n",
      "Epoch: 24 \tTraining Loss: 2073.683664\n",
      "Epoch: 24 \tTraining Loss: 2930.559581\n",
      "Epoch: 24 \tTraining Loss: 2853.376422\n",
      "Epoch: 24 \tTraining Loss: 2026.972796\n",
      "Epoch: 24 \tTraining Loss: 2539.933829\n",
      "Epoch: 24 \tTraining Loss: 3002.230292\n",
      "Epoch: 24 \tTraining Loss: 2653.888606\n",
      "Epoch: 24 \tTraining Loss: 2581.089365\n",
      "Epoch: 24 \tTraining Loss: 2749.944754\n",
      "Epoch: 24 \tTraining Loss: 2856.676672\n",
      "Epoch: 24 \tTraining Loss: 2032.226963\n",
      "Epoch: 24 \tTraining Loss: 2341.612164\n",
      "Epoch: 24 \tTraining Loss: 2949.179430\n",
      "Epoch: 24 \tTraining Loss: 3212.042627\n",
      "Epoch: 24 \tTraining Loss: 2614.264978\n",
      "Epoch: 24 \tTraining Loss: 2221.171955\n",
      "Epoch: 24 \tTraining Loss: 2675.741695\n",
      "Epoch: 24 \tTraining Loss: 2204.339570\n",
      "Epoch: 24 \tTraining Loss: 1942.901621\n",
      "Epoch: 24 \tTraining Loss: 2408.471257\n",
      "Epoch: 24 \tTraining Loss: 2317.395760\n",
      "Epoch: 24 \tTraining Loss: 2508.175803\n",
      "Epoch: 24 \tTraining Loss: 2211.390027\n",
      "Epoch: 24 \tTraining Loss: 2556.752952\n",
      "Epoch: 24 \tTraining Loss: 2113.603002\n",
      "Epoch: 24 \tTraining Loss: 2075.881334\n",
      "Epoch: 24 \tTraining Loss: 2730.122250\n",
      "Epoch: 24 \tTraining Loss: 1906.597174\n",
      "Epoch: 24 \tTraining Loss: 1969.409511\n",
      "Epoch: 24 \tTraining Loss: 2231.192303\n",
      "Epoch: 24 \tTraining Loss: 2147.707488\n",
      "Epoch: 24 \tTraining Loss: 2117.017115\n",
      "mse: 88245.930\n",
      "Epoch: 25 \tTraining Loss: 2458.972522\n",
      "Epoch: 25 \tTraining Loss: 1811.397625\n",
      "Epoch: 25 \tTraining Loss: 2727.468873\n",
      "Epoch: 25 \tTraining Loss: 2614.733724\n",
      "Epoch: 25 \tTraining Loss: 2299.614393\n",
      "Epoch: 25 \tTraining Loss: 2756.151298\n",
      "Epoch: 25 \tTraining Loss: 1986.123040\n",
      "Epoch: 25 \tTraining Loss: 2837.507027\n",
      "Epoch: 25 \tTraining Loss: 2056.914608\n",
      "Epoch: 25 \tTraining Loss: 2570.104331\n",
      "Epoch: 25 \tTraining Loss: 2137.836210\n",
      "Epoch: 25 \tTraining Loss: 2898.015542\n",
      "Epoch: 25 \tTraining Loss: 2375.812892\n",
      "Epoch: 25 \tTraining Loss: 2727.186576\n",
      "Epoch: 25 \tTraining Loss: 2145.303501\n",
      "Epoch: 25 \tTraining Loss: 2102.949894\n",
      "Epoch: 25 \tTraining Loss: 2011.354095\n",
      "Epoch: 25 \tTraining Loss: 2915.052835\n",
      "Epoch: 25 \tTraining Loss: 2293.364365\n",
      "Epoch: 25 \tTraining Loss: 1728.232647\n",
      "Epoch: 25 \tTraining Loss: 2385.036194\n",
      "Epoch: 25 \tTraining Loss: 2539.882749\n",
      "Epoch: 25 \tTraining Loss: 2304.815892\n",
      "Epoch: 25 \tTraining Loss: 2349.996755\n",
      "Epoch: 25 \tTraining Loss: 2572.295694\n",
      "Epoch: 25 \tTraining Loss: 2948.108598\n",
      "Epoch: 25 \tTraining Loss: 2671.971089\n",
      "Epoch: 25 \tTraining Loss: 2438.888267\n",
      "Epoch: 25 \tTraining Loss: 2385.982410\n",
      "Epoch: 25 \tTraining Loss: 1971.142009\n",
      "Epoch: 25 \tTraining Loss: 1804.522251\n",
      "Epoch: 25 \tTraining Loss: 3810.698144\n",
      "mse: 88042.650\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            y_hat = y_hat.view(y_hat.shape[0]).double()\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        eval_model(model, val_loader)\n",
    "\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 25\n",
    "train(model, train_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), DATA_PATH+\"/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mse: 91337.575\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "91337.57466707213"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "model_loaded = EpisiodeCNN()\n",
    "model_loaded.load_state_dict(torch.load(DATA_PATH+\"/model.pt\"))\n",
    "eval_model(model_loaded, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}