{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "fb43f3484ec9eb2eb818f2c4b60702eb23b3e73ba2f9349f357001e8ed925189"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 29\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"/Users/prashanti.nilayam/Desktop/temp/\"\n",
    "prev_value_map = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value_map = {\"Capillary refill rate\": 0.0,\n",
    "    \"Diastolic blood pressure\": 59.0,\n",
    "    \"Fraction inspired oxygen\": 0.21,\n",
    "    \"Glascow coma scale eye opening\": 4,\n",
    "    \"Glascow coma scale motor response\": 6,\n",
    "    \"Glascow coma scale total\": 15,\n",
    "    \"Glascow coma scale verbal response\": 5,\n",
    "    \"Glucose\": 128.0,\n",
    "    \"Heart Rate\": 86,\n",
    "    \"Height\": 170.0,\n",
    "    \"Mean blood pressure\": 77.0,\n",
    "    \"Oxygen saturation\": 98.0,\n",
    "    \"Respiratory rate\": 19,\n",
    "    \"Systolic blood pressure\": 118.0,\n",
    "    \"Temperature\": 36.6,\n",
    "    \"Weight\": 81.0,\n",
    "    \"pH\": 7.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_map = {\n",
    "    \"Glascow coma scale eye opening\":{\n",
    "        \"1 No Response\" : 1,\n",
    "        \"No Response\" : 1,\n",
    "        \"2 To pain\" : 2,\n",
    "        \"To Pain\" : 2,\n",
    "        \"3 To speech\" : 3,\n",
    "        \"To Speech\" : 3,\n",
    "        \"4 Spontaneously\" : 4,\n",
    "        \"Spontaneously\" : 4,\n",
    "        \"None\" : 5\n",
    "    },\n",
    "    \"Glascow coma scale motor response\":{\n",
    "        \"1 No Response\": 1,\n",
    "        \"2 Abnorm extensn\" : 2,\n",
    "        \"Abnormal extension\": 2,\n",
    "        \"3 Abnorm flexion\": 3,\n",
    "        \"Abnormal Flexion\": 3,\n",
    "        \"4 Flex-withdraws\" : 4,\n",
    "        \"Flex-withdraws\": 4,\n",
    "        \"5 Localizes Pain\": 5,\n",
    "        \"Localizes Pain\": 5,\n",
    "        \"6 Obeys Commands\": 6,\n",
    "        \"Obeys Commands\": 6,\n",
    "        \"No response\" : 7,\n",
    "    },\n",
    "    \"Glascow coma scale verbal response\":{\n",
    "        \"1 No Response\" :1,\n",
    "        \"No Response\":1,\n",
    "        \"2 Incomp sounds\": 2,\n",
    "        \"Incomprehensible sounds\":2,\n",
    "        \"3 Inapprop words\":3,\n",
    "        \"Inappropriate Words\":3,\n",
    "        \"4 Confused\":4,\n",
    "        \"Confused\":4,\n",
    "        \"5 Oriented\":5,\n",
    "        \"Oriented\":5,\n",
    "        \"No Response-ETT\":6,\n",
    "        \"1.0 ET/Trach\":7\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(elem):\n",
    "    return np.concatenate([np.array(i) for i in elem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(episode_df):\n",
    "    episode_df[\"Glascow coma scale eye opening\"] = episode_df[\"Glascow coma scale eye opening\"].apply(lambda x: replacement_map[\"Glascow coma scale eye opening\"][x] if x in replacement_map[\"Glascow coma scale eye opening\"] else x)\n",
    "    episode_df[\"Glascow coma scale motor response\"] = episode_df[\"Glascow coma scale motor response\"].apply(lambda x: replacement_map[\"Glascow coma scale motor response\"][x] if x in replacement_map[\"Glascow coma scale motor response\"] else x)\n",
    "    episode_df[\"Glascow coma scale verbal response\"] = episode_df[\"Glascow coma scale verbal response\"].apply(lambda x: replacement_map[\"Glascow coma scale verbal response\"][x] if x in replacement_map[\"Glascow coma scale verbal response\"] else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_capillary_refill_rate(person_id, value, colname):\n",
    "    if value is not None and not np.isnan(value):\n",
    "        prev_value_map[person_id][colname] = value\n",
    "        return value\n",
    "    if person_id in prev_value_map and colname in prev_value_map[person_id] and prev_value_map[person_id][colname] is not None:\n",
    "        prev = prev_value_map[person_id][colname]\n",
    "    else:\n",
    "        prev = default_value_map[colname]\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(pateint_id, episode_df):\n",
    "     prev_value_map[pateint_id] = {}\n",
    "     episode_df[\"Capillary refill rate\"] = episode_df[\"Capillary refill rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Capillary refill rate\"))\n",
    "     episode_df[\"Diastolic blood pressure\"] = episode_df[\"Diastolic blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Diastolic blood pressure\"))\n",
    "     episode_df[\"Fraction inspired oxygen\"] = episode_df[\"Fraction inspired oxygen\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Fraction inspired oxygen\"))\n",
    "     episode_df[\"Glascow coma scale eye opening\"] = episode_df[\"Glascow coma scale eye opening\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale eye opening\"))\n",
    "     episode_df[\"Glascow coma scale motor response\"] = episode_df[\"Glascow coma scale motor response\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale motor response\"))\n",
    "     episode_df[\"Glascow coma scale total\"] = episode_df[\"Glascow coma scale total\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale total\"))\n",
    "     episode_df[\"Glascow coma scale verbal response\"] = episode_df[\"Glascow coma scale verbal response\"].apply(lambda x: process_capillary_refill_rate(pateint_id,  x, \"Glascow coma scale verbal response\"))\n",
    "     episode_df[\"Glucose\"] = episode_df[\"Glucose\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glucose\"))\n",
    "     episode_df[\"Heart Rate\"] = episode_df[\"Heart Rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Heart Rate\"))\n",
    "     episode_df[\"Mean blood pressure\"] = episode_df[\"Mean blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Mean blood pressure\"))\n",
    "     episode_df[\"Height\"] = episode_df[\"Height\"].apply(lambda x: process_capillary_refill_rate(pateint_id,x, \"Height\"))\n",
    "     episode_df[\"Oxygen saturation\"] = episode_df[\"Oxygen saturation\"].apply(lambda x: process_capillary_refill_rate(pateint_id,  x, \"Oxygen saturation\"))\n",
    "     episode_df[\"Respiratory rate\"] = episode_df[\"Respiratory rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Respiratory rate\"))\n",
    "     episode_df[\"Systolic blood pressure\"] = episode_df[\"Systolic blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Systolic blood pressure\"))\n",
    "     episode_df[\"Temperature\"] = episode_df[\"Temperature\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Temperature\"))\n",
    "     episode_df[\"Weight\"] = episode_df[\"Weight\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Weight\"))\n",
    "     episode_df[\"pH\"] = episode_df[\"pH\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"pH\"))\n",
    "     del prev_value_map[pateint_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_indices(data_len):\n",
    "    i = 0\n",
    "    indices = []\n",
    "    while i <= data_len-4:\n",
    "        indices.append([i, i+1, i+2, i+3])\n",
    "        i +=1\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "get_window_indices(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    x_path = DATA_PATH +'/'+path+'/'\n",
    "    X = torch.empty(0,17,4)\n",
    "    Y = torch.empty(0,)\n",
    "    y_df = pd.read_csv(DATA_PATH + path +'_listfile.csv') \n",
    "    data_files = os.listdir(x_path)\n",
    "    for data_file in data_files:\n",
    "        episode_df = pd.read_csv(x_path + data_file)\n",
    "        cleanup(episode_df)\n",
    "        fill_missing_values(data_file, episode_df)\n",
    "        episode_df[\"H_IDX\"] = episode_df.Hours.apply(np.floor).astype('int32')\n",
    "        episode_df = episode_df.groupby(by = \"H_IDX\").mean()\n",
    "        episode_df = episode_df[episode_df.Hours>=5].reset_index(drop = True)\n",
    "        temp_y = y_df[y_df.stay == data_file].sort_values(by = \"period_length\").reset_index(drop = True)\n",
    "        temp_y = temp_y[[\"period_length\", \"y_true\"]].set_index(\"period_length\")\n",
    "        episode_df = episode_df.join(temp_y, how = \"inner\").drop('Hours', axis=1).reset_index(drop = True)\n",
    "        if(len(episode_df) ==0):\n",
    "            continue\n",
    "        indices = get_window_indices(len(episode_df))\n",
    "        windows = []\n",
    "        y_values = []\n",
    "        for idx in indices:\n",
    "            window = episode_df.loc[idx]\n",
    "            y_values.append(window.loc[idx[-1]].y_true)\n",
    "            windows.append(window.drop(\"y_true\", axis=1).transpose().values.astype(np.float32))\n",
    "        t_windows = torch.tensor(windows)\n",
    "        t_y_values = torch.tensor(y_values)\n",
    "        X = torch.cat((X, t_windows), 0)\n",
    "        Y = torch.cat((Y, t_y_values), 0)\n",
    "        return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EpisodeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, obs, los):\n",
    "        self.x = obs\n",
    "        self.y = los\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "        \n",
    "X_train, Y_train = preprocess('train')\n",
    "train_dataset = EpisodeDataset(X, Y)\n",
    "X_val, Y_val = preprocess('val')\n",
    "val_dataset = EpisodeDataset(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True)                              \n",
    "#val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=32, shuffle=False)                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [5.0000e+01, 5.7000e+01, 5.7000e+01, 5.1000e+01],\n        [2.1000e-01, 2.1000e-01, 2.1000e-01, 2.1000e-01],\n        [4.0000e+00, 4.0000e+00, 4.0000e+00, 4.0000e+00],\n        [6.0000e+00, 6.0000e+00, 6.0000e+00, 6.0000e+00],\n        [1.5000e+01, 1.5000e+01, 1.5000e+01, 1.5000e+01],\n        [5.0000e+00, 5.0000e+00, 5.0000e+00, 5.0000e+00],\n        [9.3000e+01, 2.3200e+02, 2.3200e+02, 2.3200e+02],\n        [7.6000e+01, 7.4000e+01, 7.0000e+01, 7.1000e+01],\n        [1.7000e+02, 1.7000e+02, 1.7000e+02, 1.7000e+02],\n        [7.2333e+01, 8.0667e+01, 8.0667e+01, 7.4333e+01],\n        [1.0000e+02, 1.0000e+02, 1.0000e+02, 9.8000e+01],\n        [2.2000e+01, 1.5000e+01, 2.2000e+01, 2.0000e+01],\n        [1.1700e+02, 1.2800e+02, 1.2800e+02, 1.2100e+02],\n        [3.7556e+01, 3.6444e+01, 3.6444e+01, 3.7389e+01],\n        [8.1000e+01, 8.1000e+01, 8.1000e+01, 8.1000e+01],\n        [7.4000e+00, 7.4000e+00, 7.4000e+00, 7.4000e+00]]) tensor(31.1800, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataset:\n",
    "    print(data[0], data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EpisiodeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EpisiodeCNN, self).__init__()\n",
    "        #input shape 1 * 17 * 4\n",
    "        #output shape 17 * 17 * 4\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=17, kernel_size=3, padding=1, stride = 1)\n",
    "        #input shape 17 * 17 * 4\n",
    "        #output shape 68 * 17 * 4\n",
    "        self.conv2 = nn.Conv2d(in_channels=17, out_channels=34, kernel_size=3, padding=1, stride = 1)\n",
    "        #input shape 34 * 17 * 4\n",
    "        #output shape 34 * 8 * 2\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        #input shape 34 * 8 * 2\n",
    "        #output shape 68 * 8 * 2\n",
    "        self.conv3 = nn.Conv2d(in_channels=34, out_channels=68, kernel_size=3, padding=1, stride = 1)\n",
    "        self.fc1 = nn.Linear(68*8*2, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #input is of shape (batch_size=32, 3, 224, 224) if you did the dataloader right\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = x.view(-1, 68 * 8 * 2)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EpisiodeCNN()\n",
    "learning_rate = 0.00001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_y_true = torch.DoubleTensor()\n",
    "    all_y_pred = torch.DoubleTensor()\n",
    "    for x, y in val_loader:\n",
    "        y_hat = model(x)\n",
    "        all_y_true = torch.cat((all_y_true, y.to('cpu')), dim=0)\n",
    "        all_y_pred = torch.cat((all_y_pred,  y_hat.to('cpu')), dim=0)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(all_y_true, all_y_pred > 0.5, average='binary')\n",
    "    roc_auc = roc_auc_score(all_y_true.detach().numpy().tolist(), all_y_pred.detach().numpy().tolist())\n",
    "    print(f\"auc: {roc_auc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.3f}\")\n",
    "    return precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 \tTraining Loss: 737.607990\n",
      "Epoch: 2 \tTraining Loss: 736.088061\n",
      "Epoch: 3 \tTraining Loss: 737.132043\n",
      "Epoch: 4 \tTraining Loss: 737.694085\n",
      "Epoch: 5 \tTraining Loss: 733.414790\n",
      "Epoch: 6 \tTraining Loss: 734.603961\n",
      "Epoch: 7 \tTraining Loss: 733.809379\n",
      "Epoch: 8 \tTraining Loss: 734.022794\n",
      "Epoch: 9 \tTraining Loss: 732.386895\n",
      "Epoch: 10 \tTraining Loss: 730.333172\n",
      "Epoch: 11 \tTraining Loss: 731.132872\n",
      "Epoch: 12 \tTraining Loss: 730.595919\n",
      "Epoch: 13 \tTraining Loss: 730.949078\n",
      "Epoch: 14 \tTraining Loss: 728.767329\n",
      "Epoch: 15 \tTraining Loss: 730.054300\n",
      "Epoch: 16 \tTraining Loss: 728.616436\n",
      "Epoch: 17 \tTraining Loss: 727.411756\n",
      "Epoch: 18 \tTraining Loss: 727.884421\n",
      "Epoch: 19 \tTraining Loss: 728.261433\n",
      "Epoch: 20 \tTraining Loss: 727.845316\n",
      "Epoch: 21 \tTraining Loss: 728.596957\n",
      "Epoch: 22 \tTraining Loss: 724.766469\n",
      "Epoch: 23 \tTraining Loss: 726.772509\n",
      "Epoch: 24 \tTraining Loss: 725.817605\n",
      "Epoch: 25 \tTraining Loss: 726.969748\n",
      "Epoch: 26 \tTraining Loss: 725.104453\n",
      "Epoch: 27 \tTraining Loss: 726.481620\n",
      "Epoch: 28 \tTraining Loss: 726.332866\n",
      "Epoch: 29 \tTraining Loss: 723.401118\n",
      "Epoch: 30 \tTraining Loss: 723.726198\n",
      "Epoch: 31 \tTraining Loss: 722.617452\n",
      "Epoch: 32 \tTraining Loss: 721.901639\n",
      "Epoch: 33 \tTraining Loss: 722.533179\n",
      "Epoch: 34 \tTraining Loss: 723.457845\n",
      "Epoch: 35 \tTraining Loss: 721.758487\n",
      "Epoch: 36 \tTraining Loss: 722.648170\n",
      "Epoch: 37 \tTraining Loss: 720.764661\n",
      "Epoch: 38 \tTraining Loss: 725.107321\n",
      "Epoch: 39 \tTraining Loss: 720.845573\n",
      "Epoch: 40 \tTraining Loss: 720.562805\n",
      "Epoch: 41 \tTraining Loss: 721.332973\n",
      "Epoch: 42 \tTraining Loss: 720.004329\n",
      "Epoch: 43 \tTraining Loss: 720.275809\n",
      "Epoch: 44 \tTraining Loss: 720.899180\n",
      "Epoch: 45 \tTraining Loss: 720.363527\n",
      "Epoch: 46 \tTraining Loss: 716.540080\n",
      "Epoch: 47 \tTraining Loss: 715.392232\n",
      "Epoch: 48 \tTraining Loss: 717.199710\n",
      "Epoch: 49 \tTraining Loss: 714.275857\n",
      "Epoch: 50 \tTraining Loss: 713.813285\n",
      "Epoch: 51 \tTraining Loss: 716.579535\n",
      "Epoch: 52 \tTraining Loss: 714.492194\n",
      "Epoch: 53 \tTraining Loss: 712.641957\n",
      "Epoch: 54 \tTraining Loss: 715.310358\n",
      "Epoch: 55 \tTraining Loss: 713.902709\n",
      "Epoch: 56 \tTraining Loss: 712.441628\n",
      "Epoch: 57 \tTraining Loss: 711.756429\n",
      "Epoch: 58 \tTraining Loss: 712.020724\n",
      "Epoch: 59 \tTraining Loss: 708.893406\n",
      "Epoch: 60 \tTraining Loss: 710.812432\n",
      "Epoch: 61 \tTraining Loss: 710.436178\n",
      "Epoch: 62 \tTraining Loss: 711.041671\n",
      "Epoch: 63 \tTraining Loss: 710.537731\n",
      "Epoch: 64 \tTraining Loss: 709.741098\n",
      "Epoch: 65 \tTraining Loss: 708.230625\n",
      "Epoch: 66 \tTraining Loss: 710.563600\n",
      "Epoch: 67 \tTraining Loss: 705.571039\n",
      "Epoch: 68 \tTraining Loss: 711.504838\n",
      "Epoch: 69 \tTraining Loss: 705.340648\n",
      "Epoch: 70 \tTraining Loss: 703.169236\n",
      "Epoch: 71 \tTraining Loss: 701.788343\n",
      "Epoch: 72 \tTraining Loss: 705.908185\n",
      "Epoch: 73 \tTraining Loss: 703.329091\n",
      "Epoch: 74 \tTraining Loss: 704.205040\n",
      "Epoch: 75 \tTraining Loss: 701.132202\n",
      "Epoch: 76 \tTraining Loss: 704.097132\n",
      "Epoch: 77 \tTraining Loss: 700.604503\n",
      "Epoch: 78 \tTraining Loss: 699.137659\n",
      "Epoch: 79 \tTraining Loss: 698.504062\n",
      "Epoch: 80 \tTraining Loss: 698.866466\n",
      "Epoch: 81 \tTraining Loss: 696.359622\n",
      "Epoch: 82 \tTraining Loss: 703.900741\n",
      "Epoch: 83 \tTraining Loss: 698.898797\n",
      "Epoch: 84 \tTraining Loss: 697.313601\n",
      "Epoch: 85 \tTraining Loss: 696.924389\n",
      "Epoch: 86 \tTraining Loss: 692.366282\n",
      "Epoch: 87 \tTraining Loss: 694.425810\n",
      "Epoch: 88 \tTraining Loss: 691.735373\n",
      "Epoch: 89 \tTraining Loss: 692.371636\n",
      "Epoch: 90 \tTraining Loss: 693.261327\n",
      "Epoch: 91 \tTraining Loss: 692.167522\n",
      "Epoch: 92 \tTraining Loss: 686.810009\n",
      "Epoch: 93 \tTraining Loss: 691.185904\n",
      "Epoch: 94 \tTraining Loss: 689.450388\n",
      "Epoch: 95 \tTraining Loss: 688.383263\n",
      "Epoch: 96 \tTraining Loss: 679.795423\n",
      "Epoch: 97 \tTraining Loss: 688.013618\n",
      "Epoch: 98 \tTraining Loss: 692.315425\n",
      "Epoch: 99 \tTraining Loss: 680.921651\n",
      "Epoch: 100 \tTraining Loss: 685.190054\n",
      "Epoch: 101 \tTraining Loss: 682.567174\n",
      "Epoch: 102 \tTraining Loss: 676.808475\n",
      "Epoch: 103 \tTraining Loss: 678.571945\n",
      "Epoch: 104 \tTraining Loss: 678.552811\n",
      "Epoch: 105 \tTraining Loss: 673.470305\n",
      "Epoch: 106 \tTraining Loss: 669.871484\n",
      "Epoch: 107 \tTraining Loss: 668.094193\n",
      "Epoch: 108 \tTraining Loss: 675.463249\n",
      "Epoch: 109 \tTraining Loss: 670.132014\n",
      "Epoch: 110 \tTraining Loss: 672.209385\n",
      "Epoch: 111 \tTraining Loss: 664.922467\n",
      "Epoch: 112 \tTraining Loss: 667.564826\n",
      "Epoch: 113 \tTraining Loss: 657.333516\n",
      "Epoch: 114 \tTraining Loss: 658.916993\n",
      "Epoch: 115 \tTraining Loss: 665.904659\n",
      "Epoch: 116 \tTraining Loss: 659.052051\n",
      "Epoch: 117 \tTraining Loss: 658.193383\n",
      "Epoch: 118 \tTraining Loss: 656.961558\n",
      "Epoch: 119 \tTraining Loss: 658.889201\n",
      "Epoch: 120 \tTraining Loss: 659.335964\n",
      "Epoch: 121 \tTraining Loss: 647.186502\n",
      "Epoch: 122 \tTraining Loss: 648.115423\n",
      "Epoch: 123 \tTraining Loss: 641.647994\n",
      "Epoch: 124 \tTraining Loss: 635.319568\n",
      "Epoch: 125 \tTraining Loss: 642.671968\n",
      "Epoch: 126 \tTraining Loss: 643.718967\n",
      "Epoch: 127 \tTraining Loss: 650.730580\n",
      "Epoch: 128 \tTraining Loss: 637.496311\n",
      "Epoch: 129 \tTraining Loss: 630.211857\n",
      "Epoch: 130 \tTraining Loss: 647.325057\n",
      "Epoch: 131 \tTraining Loss: 641.389195\n",
      "Epoch: 132 \tTraining Loss: 627.045586\n",
      "Epoch: 133 \tTraining Loss: 636.800655\n",
      "Epoch: 134 \tTraining Loss: 621.533889\n",
      "Epoch: 135 \tTraining Loss: 629.102326\n",
      "Epoch: 136 \tTraining Loss: 630.582483\n",
      "Epoch: 137 \tTraining Loss: 622.764289\n",
      "Epoch: 138 \tTraining Loss: 629.687477\n",
      "Epoch: 139 \tTraining Loss: 615.979444\n",
      "Epoch: 140 \tTraining Loss: 632.391456\n",
      "Epoch: 141 \tTraining Loss: 622.194998\n",
      "Epoch: 142 \tTraining Loss: 620.057097\n",
      "Epoch: 143 \tTraining Loss: 599.194484\n",
      "Epoch: 144 \tTraining Loss: 600.013452\n",
      "Epoch: 145 \tTraining Loss: 604.771719\n",
      "Epoch: 146 \tTraining Loss: 608.769609\n",
      "Epoch: 147 \tTraining Loss: 600.362464\n",
      "Epoch: 148 \tTraining Loss: 597.042921\n",
      "Epoch: 149 \tTraining Loss: 611.729916\n",
      "Epoch: 150 \tTraining Loss: 594.543996\n",
      "Epoch: 151 \tTraining Loss: 583.398110\n",
      "Epoch: 152 \tTraining Loss: 584.271486\n",
      "Epoch: 153 \tTraining Loss: 587.223134\n",
      "Epoch: 154 \tTraining Loss: 581.611178\n",
      "Epoch: 155 \tTraining Loss: 590.331269\n",
      "Epoch: 156 \tTraining Loss: 578.322591\n",
      "Epoch: 157 \tTraining Loss: 571.955919\n",
      "Epoch: 158 \tTraining Loss: 576.640922\n",
      "Epoch: 159 \tTraining Loss: 567.566003\n",
      "Epoch: 160 \tTraining Loss: 559.488888\n",
      "Epoch: 161 \tTraining Loss: 551.700456\n",
      "Epoch: 162 \tTraining Loss: 541.668816\n",
      "Epoch: 163 \tTraining Loss: 568.517583\n",
      "Epoch: 164 \tTraining Loss: 559.383004\n",
      "Epoch: 165 \tTraining Loss: 544.989742\n",
      "Epoch: 166 \tTraining Loss: 540.864163\n",
      "Epoch: 167 \tTraining Loss: 550.769221\n",
      "Epoch: 168 \tTraining Loss: 541.501093\n",
      "Epoch: 169 \tTraining Loss: 533.829444\n",
      "Epoch: 170 \tTraining Loss: 534.512671\n",
      "Epoch: 171 \tTraining Loss: 527.227959\n",
      "Epoch: 172 \tTraining Loss: 518.599987\n",
      "Epoch: 173 \tTraining Loss: 520.598530\n",
      "Epoch: 174 \tTraining Loss: 532.675428\n",
      "Epoch: 175 \tTraining Loss: 524.740532\n",
      "Epoch: 176 \tTraining Loss: 523.402794\n",
      "Epoch: 177 \tTraining Loss: 505.233334\n",
      "Epoch: 178 \tTraining Loss: 504.552503\n",
      "Epoch: 179 \tTraining Loss: 501.436306\n",
      "Epoch: 180 \tTraining Loss: 492.472098\n",
      "Epoch: 181 \tTraining Loss: 492.573566\n",
      "Epoch: 182 \tTraining Loss: 492.100580\n",
      "Epoch: 183 \tTraining Loss: 504.251498\n",
      "Epoch: 184 \tTraining Loss: 487.336240\n",
      "Epoch: 185 \tTraining Loss: 472.957321\n",
      "Epoch: 186 \tTraining Loss: 482.284093\n",
      "Epoch: 187 \tTraining Loss: 466.299585\n",
      "Epoch: 188 \tTraining Loss: 462.066857\n",
      "Epoch: 189 \tTraining Loss: 443.694357\n",
      "Epoch: 190 \tTraining Loss: 457.731379\n",
      "Epoch: 191 \tTraining Loss: 443.317384\n",
      "Epoch: 192 \tTraining Loss: 439.609626\n",
      "Epoch: 193 \tTraining Loss: 438.903873\n",
      "Epoch: 194 \tTraining Loss: 411.732116\n",
      "Epoch: 195 \tTraining Loss: 439.710937\n",
      "Epoch: 196 \tTraining Loss: 411.066745\n",
      "Epoch: 197 \tTraining Loss: 439.965242\n",
      "Epoch: 198 \tTraining Loss: 453.054355\n",
      "Epoch: 199 \tTraining Loss: 398.580558\n",
      "Epoch: 200 \tTraining Loss: 413.072292\n",
      "Epoch: 201 \tTraining Loss: 401.454776\n",
      "Epoch: 202 \tTraining Loss: 402.487919\n",
      "Epoch: 203 \tTraining Loss: 394.629414\n",
      "Epoch: 204 \tTraining Loss: 392.466972\n",
      "Epoch: 205 \tTraining Loss: 404.723849\n",
      "Epoch: 206 \tTraining Loss: 390.339033\n",
      "Epoch: 207 \tTraining Loss: 398.508943\n",
      "Epoch: 208 \tTraining Loss: 371.828791\n",
      "Epoch: 209 \tTraining Loss: 367.437174\n",
      "Epoch: 210 \tTraining Loss: 379.289537\n",
      "Epoch: 211 \tTraining Loss: 338.903719\n",
      "Epoch: 212 \tTraining Loss: 336.965282\n",
      "Epoch: 213 \tTraining Loss: 333.562058\n",
      "Epoch: 214 \tTraining Loss: 324.665324\n",
      "Epoch: 215 \tTraining Loss: 304.260670\n",
      "Epoch: 216 \tTraining Loss: 335.719883\n",
      "Epoch: 217 \tTraining Loss: 334.667067\n",
      "Epoch: 218 \tTraining Loss: 322.441929\n",
      "Epoch: 219 \tTraining Loss: 312.690592\n",
      "Epoch: 220 \tTraining Loss: 320.997234\n",
      "Epoch: 221 \tTraining Loss: 275.778846\n",
      "Epoch: 222 \tTraining Loss: 288.643614\n",
      "Epoch: 223 \tTraining Loss: 299.754836\n",
      "Epoch: 224 \tTraining Loss: 301.134624\n",
      "Epoch: 225 \tTraining Loss: 290.158070\n",
      "Epoch: 226 \tTraining Loss: 271.199125\n",
      "Epoch: 227 \tTraining Loss: 264.053822\n",
      "Epoch: 228 \tTraining Loss: 248.586008\n",
      "Epoch: 229 \tTraining Loss: 274.320740\n",
      "Epoch: 230 \tTraining Loss: 250.793133\n",
      "Epoch: 231 \tTraining Loss: 253.630822\n",
      "Epoch: 232 \tTraining Loss: 233.127629\n",
      "Epoch: 233 \tTraining Loss: 226.311397\n",
      "Epoch: 234 \tTraining Loss: 252.672616\n",
      "Epoch: 235 \tTraining Loss: 250.365549\n",
      "Epoch: 236 \tTraining Loss: 221.109107\n",
      "Epoch: 237 \tTraining Loss: 229.066134\n",
      "Epoch: 238 \tTraining Loss: 190.688565\n",
      "Epoch: 239 \tTraining Loss: 194.440466\n",
      "Epoch: 240 \tTraining Loss: 199.600771\n",
      "Epoch: 241 \tTraining Loss: 185.592889\n",
      "Epoch: 242 \tTraining Loss: 185.207725\n",
      "Epoch: 243 \tTraining Loss: 167.474796\n",
      "Epoch: 244 \tTraining Loss: 173.341557\n",
      "Epoch: 245 \tTraining Loss: 178.856835\n",
      "Epoch: 246 \tTraining Loss: 170.149891\n",
      "Epoch: 247 \tTraining Loss: 172.048327\n",
      "Epoch: 248 \tTraining Loss: 157.435418\n",
      "Epoch: 249 \tTraining Loss: 150.377948\n",
      "Epoch: 250 \tTraining Loss: 109.486282\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            y_hat = y_hat.view(y_hat.shape[0]).double()\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        #eval_model(model, val_loader)\n",
    "\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 250\n",
    "train(model, train_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}