{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fb43f3484ec9eb2eb818f2c4b60702eb23b3e73ba2f9349f357001e8ed925189"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 29\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"/Users/prashanti.nilayam/Desktop/temp/\"\n",
    "prev_value_map = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_df = pd.read_csv(DATA_PATH + 'train_listfile.csv') \n",
    "train_files = train_y_df[\"stay\"].unique().tolist()\n",
    "val_y_df = pd.read_csv(DATA_PATH + 'train_listfile.csv') \n",
    "val_files = val_y_df[\"stay\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "29168"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "len(train_y_df[\"stay\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['3613_episode1_timeseries.csv', '10205_episode1_timeseries.csv',\n",
       "       '3319_episode1_timeseries.csv', ...,\n",
       "       '11346_episode5_timeseries.csv', '60988_episode1_timeseries.csv',\n",
       "       '51485_episode1_timeseries.csv'], dtype='<U30')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "np.random.choice(train_y_df[\"stay\"].unique().tolist(), 5000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value_map = {\"Capillary refill rate\": 0.0,\n",
    "    \"Diastolic blood pressure\": 59.0,\n",
    "    \"Fraction inspired oxygen\": 0.21,\n",
    "    \"Glascow coma scale eye opening\": 4,\n",
    "    \"Glascow coma scale motor response\": 6,\n",
    "    \"Glascow coma scale total\": 15,\n",
    "    \"Glascow coma scale verbal response\": 5,\n",
    "    \"Glucose\": 128.0,\n",
    "    \"Heart Rate\": 86,\n",
    "    \"Height\": 170.0,\n",
    "    \"Mean blood pressure\": 77.0,\n",
    "    \"Oxygen saturation\": 98.0,\n",
    "    \"Respiratory rate\": 19,\n",
    "    \"Systolic blood pressure\": 118.0,\n",
    "    \"Temperature\": 36.6,\n",
    "    \"Weight\": 81.0,\n",
    "    \"pH\": 7.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_map = {\n",
    "    \"Glascow coma scale eye opening\":{\n",
    "        \"1 No Response\" : 1,\n",
    "        \"No Response\" : 1,\n",
    "        \"2 To pain\" : 2,\n",
    "        \"To Pain\" : 2,\n",
    "        \"3 To speech\" : 3,\n",
    "        \"To Speech\" : 3,\n",
    "        \"4 Spontaneously\" : 4,\n",
    "        \"Spontaneously\" : 4,\n",
    "        \"None\" : 5\n",
    "    },\n",
    "    \"Glascow coma scale motor response\":{\n",
    "        \"1 No Response\": 1,\n",
    "        \"2 Abnorm extensn\" : 2,\n",
    "        \"Abnormal extension\": 2,\n",
    "        \"3 Abnorm flexion\": 3,\n",
    "        \"Abnormal Flexion\": 3,\n",
    "        \"4 Flex-withdraws\" : 4,\n",
    "        \"Flex-withdraws\": 4,\n",
    "        \"5 Localizes Pain\": 5,\n",
    "        \"Localizes Pain\": 5,\n",
    "        \"6 Obeys Commands\": 6,\n",
    "        \"Obeys Commands\": 6,\n",
    "        \"No response\" : 7,\n",
    "    },\n",
    "    \"Glascow coma scale verbal response\":{\n",
    "        \"1 No Response\" :1,\n",
    "        \"No Response\":1,\n",
    "        \"2 Incomp sounds\": 2,\n",
    "        \"Incomprehensible sounds\":2,\n",
    "        \"3 Inapprop words\":3,\n",
    "        \"Inappropriate Words\":3,\n",
    "        \"4 Confused\":4,\n",
    "        \"Confused\":4,\n",
    "        \"5 Oriented\":5,\n",
    "        \"Oriented\":5,\n",
    "        \"No Response-ETT\":6,\n",
    "        \"1.0 ET/Trach\":7\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(elem):\n",
    "    return np.concatenate([np.array(i) for i in elem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(episode_df):\n",
    "    episode_df[\"Glascow coma scale eye opening\"] = episode_df[\"Glascow coma scale eye opening\"].apply(lambda x: replacement_map[\"Glascow coma scale eye opening\"][x] if x in replacement_map[\"Glascow coma scale eye opening\"] else x)\n",
    "    episode_df[\"Glascow coma scale motor response\"] = episode_df[\"Glascow coma scale motor response\"].apply(lambda x: replacement_map[\"Glascow coma scale motor response\"][x] if x in replacement_map[\"Glascow coma scale motor response\"] else x)\n",
    "    episode_df[\"Glascow coma scale verbal response\"] = episode_df[\"Glascow coma scale verbal response\"].apply(lambda x: replacement_map[\"Glascow coma scale verbal response\"][x] if x in replacement_map[\"Glascow coma scale verbal response\"] else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_capillary_refill_rate(person_id, value, colname):\n",
    "    if value is not None and not np.isnan(value):\n",
    "        prev_value_map[person_id][colname] = value\n",
    "        return value\n",
    "    if person_id in prev_value_map and colname in prev_value_map[person_id] and prev_value_map[person_id][colname] is not None:\n",
    "        prev = prev_value_map[person_id][colname]\n",
    "    else:\n",
    "        prev = default_value_map[colname]\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(pateint_id, episode_df):\n",
    "     prev_value_map[pateint_id] = {}\n",
    "     episode_df[\"Capillary refill rate\"] = episode_df[\"Capillary refill rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Capillary refill rate\"))\n",
    "     episode_df[\"Diastolic blood pressure\"] = episode_df[\"Diastolic blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Diastolic blood pressure\"))\n",
    "     episode_df[\"Fraction inspired oxygen\"] = episode_df[\"Fraction inspired oxygen\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Fraction inspired oxygen\"))\n",
    "     episode_df[\"Glascow coma scale eye opening\"] = episode_df[\"Glascow coma scale eye opening\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale eye opening\"))\n",
    "     episode_df[\"Glascow coma scale motor response\"] = episode_df[\"Glascow coma scale motor response\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale motor response\"))\n",
    "     episode_df[\"Glascow coma scale total\"] = episode_df[\"Glascow coma scale total\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale total\"))\n",
    "     episode_df[\"Glascow coma scale verbal response\"] = episode_df[\"Glascow coma scale verbal response\"].apply(lambda x: process_capillary_refill_rate(pateint_id,  x, \"Glascow coma scale verbal response\"))\n",
    "     episode_df[\"Glucose\"] = episode_df[\"Glucose\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glucose\"))\n",
    "     episode_df[\"Heart Rate\"] = episode_df[\"Heart Rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Heart Rate\"))\n",
    "     episode_df[\"Mean blood pressure\"] = episode_df[\"Mean blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Mean blood pressure\"))\n",
    "     episode_df[\"Height\"] = episode_df[\"Height\"].apply(lambda x: process_capillary_refill_rate(pateint_id,x, \"Height\"))\n",
    "     episode_df[\"Oxygen saturation\"] = episode_df[\"Oxygen saturation\"].apply(lambda x: process_capillary_refill_rate(pateint_id,  x, \"Oxygen saturation\"))\n",
    "     episode_df[\"Respiratory rate\"] = episode_df[\"Respiratory rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Respiratory rate\"))\n",
    "     episode_df[\"Systolic blood pressure\"] = episode_df[\"Systolic blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Systolic blood pressure\"))\n",
    "     episode_df[\"Temperature\"] = episode_df[\"Temperature\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Temperature\"))\n",
    "     episode_df[\"Weight\"] = episode_df[\"Weight\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Weight\"))\n",
    "     episode_df[\"pH\"] = episode_df[\"pH\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"pH\"))\n",
    "     del prev_value_map[pateint_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_indices(data_len):\n",
    "    i = 0\n",
    "    indices = []\n",
    "    while i <= data_len-4:\n",
    "        indices.append([i, i+1, i+2, i+3])\n",
    "        i +=1\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "get_window_indices(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    x_path = DATA_PATH +'/'+path+'/'\n",
    "    X = torch.empty(0,17,4)\n",
    "    Y = torch.empty(0,)\n",
    "    y_df = pd.read_csv(DATA_PATH + path +'_listfile.csv') \n",
    "    data_files = os.listdir(x_path)\n",
    "    print(data_files)\n",
    "    for data_file in data_files:\n",
    "        print(data_file)\n",
    "        if data_file.endswith(\".csv\"):\n",
    "            episode_df = pd.read_csv(x_path + data_file)\n",
    "            cleanup(episode_df)\n",
    "            fill_missing_values(data_file, episode_df)\n",
    "            episode_df[\"H_IDX\"] = episode_df.Hours.apply(np.floor).astype('int32')\n",
    "            episode_df = episode_df.groupby(by = \"H_IDX\").mean()\n",
    "            episode_df = episode_df[episode_df.Hours>=5].reset_index(drop = True)\n",
    "            temp_y = y_df[y_df.stay == data_file].sort_values(by = \"period_length\").reset_index(drop = True)\n",
    "            temp_y = temp_y[[\"period_length\", \"y_true\"]].set_index(\"period_length\")\n",
    "            episode_df = episode_df.join(temp_y, how = \"inner\").drop('Hours', axis=1).reset_index(drop = True)\n",
    "            if(len(episode_df) >0):\n",
    "                indices = get_window_indices(len(episode_df))\n",
    "                windows = []\n",
    "                y_values = []\n",
    "                for idx in indices:\n",
    "                    window = episode_df.loc[idx]\n",
    "                    y_values.append(window.loc[idx[-1]].y_true)\n",
    "                    windows.append(window.drop(\"y_true\", axis=1).transpose().values.astype(np.float32))\n",
    "                t_windows = torch.tensor(windows)\n",
    "                t_y_values = torch.tensor(y_values)\n",
    "                X = torch.cat((X, t_windows), 0)\n",
    "                Y = torch.cat((Y, t_y_values), 0)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['.DS_Store', '10021_episode1_timeseries.csv', '10010_episode1_timeseries.csv', '10003_episode1_timeseries.csv', '10014_episode1_timeseries.csv', '10007_episode1_timeseries.csv', '1000_episode1_timeseries.csv', '10017_episode1_timeseries.csv', '10022_episode1_timeseries.csv', '10013_episode1_timeseries.csv']\n",
      ".DS_Store\n",
      "10021_episode1_timeseries.csv\n",
      "10010_episode1_timeseries.csv\n",
      "10003_episode1_timeseries.csv\n",
      "10014_episode1_timeseries.csv\n",
      "10007_episode1_timeseries.csv\n",
      "1000_episode1_timeseries.csv\n",
      "10017_episode1_timeseries.csv\n",
      "10022_episode1_timeseries.csv\n",
      "10013_episode1_timeseries.csv\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EpisodeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, obs, los):\n",
    "        self.x = obs\n",
    "        self.y = los\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "        \n",
    "X_train, Y_train = preprocess('train')\n",
    "train_dataset = EpisodeDataset(X_train, Y_train)\n",
    "#X_val, Y_val = preprocess('val')\n",
    "#val_dataset = EpisodeDataset(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True)                              \n",
    "#val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=32, shuffle=False)                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[  0.0000,   0.0000,   0.0000,   0.0000],\n        [ 69.0000,  78.0000,  70.0000,  70.5000],\n        [  0.2100,   0.2100,   0.2100,   0.2100],\n        [  4.0000,   4.0000,   4.0000,   4.0000],\n        [  6.0000,   6.0000,   6.0000,   6.0000],\n        [ 15.0000,  15.0000,  15.0000,  15.0000],\n        [  5.0000,   5.0000,   5.0000,   5.0000],\n        [105.0000, 105.0000, 105.0000, 129.0000],\n        [ 69.0000,  63.0000,  61.0000,  68.5000],\n        [170.0000, 170.0000, 170.0000, 170.0000],\n        [ 87.3333,  88.6667,  83.3333,  85.1666],\n        [ 93.0000, 100.0000, 100.0000,  99.0000],\n        [ 11.0000,  11.0000,  11.0000,  13.5000],\n        [124.0000, 110.0000, 110.0000, 114.5000],\n        [ 37.4444,  37.4444,  37.4444,  37.4444],\n        [ 86.5000,  86.5000,  86.5000,  86.5000],\n        [  5.0000,   5.0000,   5.0000,   5.0000]]) tensor(20.6392, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataset:\n",
    "    print(data[0], data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EpisiodeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EpisiodeCNN, self).__init__()\n",
    "        #input shape 1 * 17 * 4\n",
    "        #output shape 17 * 17 * 4\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=17, kernel_size=3, padding=1, stride = 1)\n",
    "        #input shape 17 * 17 * 4\n",
    "        #output shape 68 * 17 * 4\n",
    "        self.conv2 = nn.Conv2d(in_channels=17, out_channels=34, kernel_size=3, padding=1, stride = 1)\n",
    "        #input shape 34 * 17 * 4\n",
    "        #output shape 34 * 8 * 2\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        #input shape 34 * 8 * 2\n",
    "        #output shape 68 * 8 * 2\n",
    "        self.conv3 = nn.Conv2d(in_channels=34, out_channels=68, kernel_size=3, padding=1, stride = 1)\n",
    "        self.fc1 = nn.Linear(68*8*2, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #input is of shape (batch_size=32, 3, 224, 224) if you did the dataloader right\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = x.view(-1, 68 * 8 * 2)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EpisiodeCNN()\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_y_true = torch.DoubleTensor()\n",
    "    all_y_pred = torch.DoubleTensor()\n",
    "    for x, y in val_loader:\n",
    "        y_hat = model(x)\n",
    "        all_y_true = torch.cat((all_y_true, y.to('cpu')), dim=0)\n",
    "        all_y_pred = torch.cat((all_y_pred,  y_hat.to('cpu')), dim=0)\n",
    "    mse= mean_squared_error(all_y_true.detach().numpy(), all_y_pred.detach().numpy())\n",
    "    print(f\"mse: {mse:.3f}\")\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ch: 233 \tTraining Loss: 2.506476\n",
      "Epoch: 233 \tTraining Loss: 2.931866\n",
      "Epoch: 233 \tTraining Loss: 3.670050\n",
      "Epoch: 233 \tTraining Loss: 3.031339\n",
      "Epoch: 233 \tTraining Loss: 2.946826\n",
      "Epoch: 233 \tTraining Loss: 2.633123\n",
      "Epoch: 233 \tTraining Loss: 3.870245\n",
      "Epoch: 233 \tTraining Loss: 2.176230\n",
      "Epoch: 233 \tTraining Loss: 2.372935\n",
      "Epoch: 233 \tTraining Loss: 3.051009\n",
      "Epoch: 233 \tTraining Loss: 2.437410\n",
      "Epoch: 234 \tTraining Loss: 3.681311\n",
      "Epoch: 234 \tTraining Loss: 2.433125\n",
      "Epoch: 234 \tTraining Loss: 2.348649\n",
      "Epoch: 234 \tTraining Loss: 2.241944\n",
      "Epoch: 234 \tTraining Loss: 2.647043\n",
      "Epoch: 234 \tTraining Loss: 2.295091\n",
      "Epoch: 234 \tTraining Loss: 2.803737\n",
      "Epoch: 234 \tTraining Loss: 1.786677\n",
      "Epoch: 234 \tTraining Loss: 2.473335\n",
      "Epoch: 234 \tTraining Loss: 3.207976\n",
      "Epoch: 234 \tTraining Loss: 2.396748\n",
      "Epoch: 234 \tTraining Loss: 2.052543\n",
      "Epoch: 234 \tTraining Loss: 3.926467\n",
      "Epoch: 234 \tTraining Loss: 1.897662\n",
      "Epoch: 234 \tTraining Loss: 2.935410\n",
      "Epoch: 234 \tTraining Loss: 2.067398\n",
      "Epoch: 234 \tTraining Loss: 3.745263\n",
      "Epoch: 234 \tTraining Loss: 4.212444\n",
      "Epoch: 234 \tTraining Loss: 4.940391\n",
      "Epoch: 234 \tTraining Loss: 2.056476\n",
      "Epoch: 234 \tTraining Loss: 3.153506\n",
      "Epoch: 234 \tTraining Loss: 1.516477\n",
      "Epoch: 234 \tTraining Loss: 2.378631\n",
      "Epoch: 234 \tTraining Loss: 3.879267\n",
      "Epoch: 234 \tTraining Loss: 2.671457\n",
      "Epoch: 234 \tTraining Loss: 1.932061\n",
      "Epoch: 234 \tTraining Loss: 1.772466\n",
      "Epoch: 234 \tTraining Loss: 2.166936\n",
      "Epoch: 234 \tTraining Loss: 3.448091\n",
      "Epoch: 234 \tTraining Loss: 4.102215\n",
      "Epoch: 234 \tTraining Loss: 2.513054\n",
      "Epoch: 234 \tTraining Loss: 3.823055\n",
      "Epoch: 235 \tTraining Loss: 2.023831\n",
      "Epoch: 235 \tTraining Loss: 4.111556\n",
      "Epoch: 235 \tTraining Loss: 2.780861\n",
      "Epoch: 235 \tTraining Loss: 2.425937\n",
      "Epoch: 235 \tTraining Loss: 2.627434\n",
      "Epoch: 235 \tTraining Loss: 3.089713\n",
      "Epoch: 235 \tTraining Loss: 2.079224\n",
      "Epoch: 235 \tTraining Loss: 2.665952\n",
      "Epoch: 235 \tTraining Loss: 2.006807\n",
      "Epoch: 235 \tTraining Loss: 2.303889\n",
      "Epoch: 235 \tTraining Loss: 3.218540\n",
      "Epoch: 235 \tTraining Loss: 5.062800\n",
      "Epoch: 235 \tTraining Loss: 2.606397\n",
      "Epoch: 235 \tTraining Loss: 2.641144\n",
      "Epoch: 235 \tTraining Loss: 2.385115\n",
      "Epoch: 235 \tTraining Loss: 2.137624\n",
      "Epoch: 235 \tTraining Loss: 2.763935\n",
      "Epoch: 235 \tTraining Loss: 2.733148\n",
      "Epoch: 235 \tTraining Loss: 2.899352\n",
      "Epoch: 235 \tTraining Loss: 2.766543\n",
      "Epoch: 235 \tTraining Loss: 3.529316\n",
      "Epoch: 235 \tTraining Loss: 4.174399\n",
      "Epoch: 235 \tTraining Loss: 4.119155\n",
      "Epoch: 235 \tTraining Loss: 3.232253\n",
      "Epoch: 235 \tTraining Loss: 2.702671\n",
      "Epoch: 235 \tTraining Loss: 4.341320\n",
      "Epoch: 235 \tTraining Loss: 3.332584\n",
      "Epoch: 235 \tTraining Loss: 3.985017\n",
      "Epoch: 235 \tTraining Loss: 2.498691\n",
      "Epoch: 235 \tTraining Loss: 3.478204\n",
      "Epoch: 235 \tTraining Loss: 3.114786\n",
      "Epoch: 235 \tTraining Loss: 2.066975\n",
      "Epoch: 236 \tTraining Loss: 2.938153\n",
      "Epoch: 236 \tTraining Loss: 1.938925\n",
      "Epoch: 236 \tTraining Loss: 3.042075\n",
      "Epoch: 236 \tTraining Loss: 2.335086\n",
      "Epoch: 236 \tTraining Loss: 2.187604\n",
      "Epoch: 236 \tTraining Loss: 3.143850\n",
      "Epoch: 236 \tTraining Loss: 3.445593\n",
      "Epoch: 236 \tTraining Loss: 2.077024\n",
      "Epoch: 236 \tTraining Loss: 2.146385\n",
      "Epoch: 236 \tTraining Loss: 1.663617\n",
      "Epoch: 236 \tTraining Loss: 2.228323\n",
      "Epoch: 236 \tTraining Loss: 6.830564\n",
      "Epoch: 236 \tTraining Loss: 4.466781\n",
      "Epoch: 236 \tTraining Loss: 6.309742\n",
      "Epoch: 236 \tTraining Loss: 4.892858\n",
      "Epoch: 236 \tTraining Loss: 2.829383\n",
      "Epoch: 236 \tTraining Loss: 4.154410\n",
      "Epoch: 236 \tTraining Loss: 3.846398\n",
      "Epoch: 236 \tTraining Loss: 5.144889\n",
      "Epoch: 236 \tTraining Loss: 3.087510\n",
      "Epoch: 236 \tTraining Loss: 4.189992\n",
      "Epoch: 236 \tTraining Loss: 4.237685\n",
      "Epoch: 236 \tTraining Loss: 5.454382\n",
      "Epoch: 236 \tTraining Loss: 4.836857\n",
      "Epoch: 236 \tTraining Loss: 4.974754\n",
      "Epoch: 236 \tTraining Loss: 3.966321\n",
      "Epoch: 236 \tTraining Loss: 4.583110\n",
      "Epoch: 236 \tTraining Loss: 3.606849\n",
      "Epoch: 236 \tTraining Loss: 3.231196\n",
      "Epoch: 236 \tTraining Loss: 2.888673\n",
      "Epoch: 236 \tTraining Loss: 3.804148\n",
      "Epoch: 236 \tTraining Loss: 3.553600\n",
      "Epoch: 237 \tTraining Loss: 3.101450\n",
      "Epoch: 237 \tTraining Loss: 2.742655\n",
      "Epoch: 237 \tTraining Loss: 3.785841\n",
      "Epoch: 237 \tTraining Loss: 4.243248\n",
      "Epoch: 237 \tTraining Loss: 3.307153\n",
      "Epoch: 237 \tTraining Loss: 2.709956\n",
      "Epoch: 237 \tTraining Loss: 2.797791\n",
      "Epoch: 237 \tTraining Loss: 3.670903\n",
      "Epoch: 237 \tTraining Loss: 3.947785\n",
      "Epoch: 237 \tTraining Loss: 3.591998\n",
      "Epoch: 237 \tTraining Loss: 2.848183\n",
      "Epoch: 237 \tTraining Loss: 2.854611\n",
      "Epoch: 237 \tTraining Loss: 4.477421\n",
      "Epoch: 237 \tTraining Loss: 3.181946\n",
      "Epoch: 237 \tTraining Loss: 3.302519\n",
      "Epoch: 237 \tTraining Loss: 3.328267\n",
      "Epoch: 237 \tTraining Loss: 3.408363\n",
      "Epoch: 237 \tTraining Loss: 3.406505\n",
      "Epoch: 237 \tTraining Loss: 2.361701\n",
      "Epoch: 237 \tTraining Loss: 3.321965\n",
      "Epoch: 237 \tTraining Loss: 3.314196\n",
      "Epoch: 237 \tTraining Loss: 2.251951\n",
      "Epoch: 237 \tTraining Loss: 5.382391\n",
      "Epoch: 237 \tTraining Loss: 2.768590\n",
      "Epoch: 237 \tTraining Loss: 2.043805\n",
      "Epoch: 237 \tTraining Loss: 4.351320\n",
      "Epoch: 237 \tTraining Loss: 3.204599\n",
      "Epoch: 237 \tTraining Loss: 3.396151\n",
      "Epoch: 237 \tTraining Loss: 2.844481\n",
      "Epoch: 237 \tTraining Loss: 2.324705\n",
      "Epoch: 237 \tTraining Loss: 3.240249\n",
      "Epoch: 237 \tTraining Loss: 2.745334\n",
      "Epoch: 238 \tTraining Loss: 4.004736\n",
      "Epoch: 238 \tTraining Loss: 2.901059\n",
      "Epoch: 238 \tTraining Loss: 4.093000\n",
      "Epoch: 238 \tTraining Loss: 4.383953\n",
      "Epoch: 238 \tTraining Loss: 2.793254\n",
      "Epoch: 238 \tTraining Loss: 2.717601\n",
      "Epoch: 238 \tTraining Loss: 3.509353\n",
      "Epoch: 238 \tTraining Loss: 3.533976\n",
      "Epoch: 238 \tTraining Loss: 2.822064\n",
      "Epoch: 238 \tTraining Loss: 5.767709\n",
      "Epoch: 238 \tTraining Loss: 2.934611\n",
      "Epoch: 238 \tTraining Loss: 2.236015\n",
      "Epoch: 238 \tTraining Loss: 1.692683\n",
      "Epoch: 238 \tTraining Loss: 2.098871\n",
      "Epoch: 238 \tTraining Loss: 3.447003\n",
      "Epoch: 238 \tTraining Loss: 2.524109\n",
      "Epoch: 238 \tTraining Loss: 3.531903\n",
      "Epoch: 238 \tTraining Loss: 2.732280\n",
      "Epoch: 238 \tTraining Loss: 2.141306\n",
      "Epoch: 238 \tTraining Loss: 2.099887\n",
      "Epoch: 238 \tTraining Loss: 3.292576\n",
      "Epoch: 238 \tTraining Loss: 3.189562\n",
      "Epoch: 238 \tTraining Loss: 3.987868\n",
      "Epoch: 238 \tTraining Loss: 3.180769\n",
      "Epoch: 238 \tTraining Loss: 2.949493\n",
      "Epoch: 238 \tTraining Loss: 1.937555\n",
      "Epoch: 238 \tTraining Loss: 1.717818\n",
      "Epoch: 238 \tTraining Loss: 3.504374\n",
      "Epoch: 238 \tTraining Loss: 4.763238\n",
      "Epoch: 238 \tTraining Loss: 3.524726\n",
      "Epoch: 238 \tTraining Loss: 2.957241\n",
      "Epoch: 238 \tTraining Loss: 4.975176\n",
      "Epoch: 239 \tTraining Loss: 3.072657\n",
      "Epoch: 239 \tTraining Loss: 3.503238\n",
      "Epoch: 239 \tTraining Loss: 3.881119\n",
      "Epoch: 239 \tTraining Loss: 3.779118\n",
      "Epoch: 239 \tTraining Loss: 2.668332\n",
      "Epoch: 239 \tTraining Loss: 4.494865\n",
      "Epoch: 239 \tTraining Loss: 4.465426\n",
      "Epoch: 239 \tTraining Loss: 3.866874\n",
      "Epoch: 239 \tTraining Loss: 3.377312\n",
      "Epoch: 239 \tTraining Loss: 2.721061\n",
      "Epoch: 239 \tTraining Loss: 2.240390\n",
      "Epoch: 239 \tTraining Loss: 3.378401\n",
      "Epoch: 239 \tTraining Loss: 3.587208\n",
      "Epoch: 239 \tTraining Loss: 2.573396\n",
      "Epoch: 239 \tTraining Loss: 3.168956\n",
      "Epoch: 239 \tTraining Loss: 2.618662\n",
      "Epoch: 239 \tTraining Loss: 3.741979\n",
      "Epoch: 239 \tTraining Loss: 2.488131\n",
      "Epoch: 239 \tTraining Loss: 3.561822\n",
      "Epoch: 239 \tTraining Loss: 3.135985\n",
      "Epoch: 239 \tTraining Loss: 2.158404\n",
      "Epoch: 239 \tTraining Loss: 2.672097\n",
      "Epoch: 239 \tTraining Loss: 4.800563\n",
      "Epoch: 239 \tTraining Loss: 4.027279\n",
      "Epoch: 239 \tTraining Loss: 3.165448\n",
      "Epoch: 239 \tTraining Loss: 4.405644\n",
      "Epoch: 239 \tTraining Loss: 2.319982\n",
      "Epoch: 239 \tTraining Loss: 3.029782\n",
      "Epoch: 239 \tTraining Loss: 2.416854\n",
      "Epoch: 239 \tTraining Loss: 3.106401\n",
      "Epoch: 239 \tTraining Loss: 2.837775\n",
      "Epoch: 239 \tTraining Loss: 3.498355\n",
      "Epoch: 240 \tTraining Loss: 2.050829\n",
      "Epoch: 240 \tTraining Loss: 3.726645\n",
      "Epoch: 240 \tTraining Loss: 2.428330\n",
      "Epoch: 240 \tTraining Loss: 4.307897\n",
      "Epoch: 240 \tTraining Loss: 2.453432\n",
      "Epoch: 240 \tTraining Loss: 3.007702\n",
      "Epoch: 240 \tTraining Loss: 2.539804\n",
      "Epoch: 240 \tTraining Loss: 2.575418\n",
      "Epoch: 240 \tTraining Loss: 3.233582\n",
      "Epoch: 240 \tTraining Loss: 3.453864\n",
      "Epoch: 240 \tTraining Loss: 2.152889\n",
      "Epoch: 240 \tTraining Loss: 2.389272\n",
      "Epoch: 240 \tTraining Loss: 3.481321\n",
      "Epoch: 240 \tTraining Loss: 2.820599\n",
      "Epoch: 240 \tTraining Loss: 3.119570\n",
      "Epoch: 240 \tTraining Loss: 3.256750\n",
      "Epoch: 240 \tTraining Loss: 3.173534\n",
      "Epoch: 240 \tTraining Loss: 2.217690\n",
      "Epoch: 240 \tTraining Loss: 4.392168\n",
      "Epoch: 240 \tTraining Loss: 3.057524\n",
      "Epoch: 240 \tTraining Loss: 3.199502\n",
      "Epoch: 240 \tTraining Loss: 2.888935\n",
      "Epoch: 240 \tTraining Loss: 3.075493\n",
      "Epoch: 240 \tTraining Loss: 2.475178\n",
      "Epoch: 240 \tTraining Loss: 2.865046\n",
      "Epoch: 240 \tTraining Loss: 3.277544\n",
      "Epoch: 240 \tTraining Loss: 3.738268\n",
      "Epoch: 240 \tTraining Loss: 2.459394\n",
      "Epoch: 240 \tTraining Loss: 2.496168\n",
      "Epoch: 240 \tTraining Loss: 3.214787\n",
      "Epoch: 240 \tTraining Loss: 2.719010\n",
      "Epoch: 240 \tTraining Loss: 2.030174\n",
      "Epoch: 241 \tTraining Loss: 2.811175\n",
      "Epoch: 241 \tTraining Loss: 3.591800\n",
      "Epoch: 241 \tTraining Loss: 2.487115\n",
      "Epoch: 241 \tTraining Loss: 3.009115\n",
      "Epoch: 241 \tTraining Loss: 2.716054\n",
      "Epoch: 241 \tTraining Loss: 3.200554\n",
      "Epoch: 241 \tTraining Loss: 2.332019\n",
      "Epoch: 241 \tTraining Loss: 3.906314\n",
      "Epoch: 241 \tTraining Loss: 3.746360\n",
      "Epoch: 241 \tTraining Loss: 3.914620\n",
      "Epoch: 241 \tTraining Loss: 2.077420\n",
      "Epoch: 241 \tTraining Loss: 3.162898\n",
      "Epoch: 241 \tTraining Loss: 4.052968\n",
      "Epoch: 241 \tTraining Loss: 2.803743\n",
      "Epoch: 241 \tTraining Loss: 3.365575\n",
      "Epoch: 241 \tTraining Loss: 2.970973\n",
      "Epoch: 241 \tTraining Loss: 2.699590\n",
      "Epoch: 241 \tTraining Loss: 2.923077\n",
      "Epoch: 241 \tTraining Loss: 2.068512\n",
      "Epoch: 241 \tTraining Loss: 4.027714\n",
      "Epoch: 241 \tTraining Loss: 2.622985\n",
      "Epoch: 241 \tTraining Loss: 1.734331\n",
      "Epoch: 241 \tTraining Loss: 1.496272\n",
      "Epoch: 241 \tTraining Loss: 3.421521\n",
      "Epoch: 241 \tTraining Loss: 3.661732\n",
      "Epoch: 241 \tTraining Loss: 1.838464\n",
      "Epoch: 241 \tTraining Loss: 3.282176\n",
      "Epoch: 241 \tTraining Loss: 2.025331\n",
      "Epoch: 241 \tTraining Loss: 3.139187\n",
      "Epoch: 241 \tTraining Loss: 3.085047\n",
      "Epoch: 241 \tTraining Loss: 2.331808\n",
      "Epoch: 241 \tTraining Loss: 1.948561\n",
      "Epoch: 242 \tTraining Loss: 1.944814\n",
      "Epoch: 242 \tTraining Loss: 3.079745\n",
      "Epoch: 242 \tTraining Loss: 2.406904\n",
      "Epoch: 242 \tTraining Loss: 2.780702\n",
      "Epoch: 242 \tTraining Loss: 2.258622\n",
      "Epoch: 242 \tTraining Loss: 1.842958\n",
      "Epoch: 242 \tTraining Loss: 2.519400\n",
      "Epoch: 242 \tTraining Loss: 3.472382\n",
      "Epoch: 242 \tTraining Loss: 3.759524\n",
      "Epoch: 242 \tTraining Loss: 3.206580\n",
      "Epoch: 242 \tTraining Loss: 2.889260\n",
      "Epoch: 242 \tTraining Loss: 2.622400\n",
      "Epoch: 242 \tTraining Loss: 2.226941\n",
      "Epoch: 242 \tTraining Loss: 2.618113\n",
      "Epoch: 242 \tTraining Loss: 2.347865\n",
      "Epoch: 242 \tTraining Loss: 3.477104\n",
      "Epoch: 242 \tTraining Loss: 2.229560\n",
      "Epoch: 242 \tTraining Loss: 1.912878\n",
      "Epoch: 242 \tTraining Loss: 3.187821\n",
      "Epoch: 242 \tTraining Loss: 3.095503\n",
      "Epoch: 242 \tTraining Loss: 2.261908\n",
      "Epoch: 242 \tTraining Loss: 2.079645\n",
      "Epoch: 242 \tTraining Loss: 2.066095\n",
      "Epoch: 242 \tTraining Loss: 2.596893\n",
      "Epoch: 242 \tTraining Loss: 2.084498\n",
      "Epoch: 242 \tTraining Loss: 4.013072\n",
      "Epoch: 242 \tTraining Loss: 2.889414\n",
      "Epoch: 242 \tTraining Loss: 3.313840\n",
      "Epoch: 242 \tTraining Loss: 2.617766\n",
      "Epoch: 242 \tTraining Loss: 2.027762\n",
      "Epoch: 242 \tTraining Loss: 3.026969\n",
      "Epoch: 242 \tTraining Loss: 2.426312\n",
      "Epoch: 243 \tTraining Loss: 2.864956\n",
      "Epoch: 243 \tTraining Loss: 2.195642\n",
      "Epoch: 243 \tTraining Loss: 1.980739\n",
      "Epoch: 243 \tTraining Loss: 2.800450\n",
      "Epoch: 243 \tTraining Loss: 4.915311\n",
      "Epoch: 243 \tTraining Loss: 2.840496\n",
      "Epoch: 243 \tTraining Loss: 2.491111\n",
      "Epoch: 243 \tTraining Loss: 2.928108\n",
      "Epoch: 243 \tTraining Loss: 1.594488\n",
      "Epoch: 243 \tTraining Loss: 3.414002\n",
      "Epoch: 243 \tTraining Loss: 2.432690\n",
      "Epoch: 243 \tTraining Loss: 3.021494\n",
      "Epoch: 243 \tTraining Loss: 2.575774\n",
      "Epoch: 243 \tTraining Loss: 2.550165\n",
      "Epoch: 243 \tTraining Loss: 2.444691\n",
      "Epoch: 243 \tTraining Loss: 2.326262\n",
      "Epoch: 243 \tTraining Loss: 3.132117\n",
      "Epoch: 243 \tTraining Loss: 2.674628\n",
      "Epoch: 243 \tTraining Loss: 3.462603\n",
      "Epoch: 243 \tTraining Loss: 4.549076\n",
      "Epoch: 243 \tTraining Loss: 2.005122\n",
      "Epoch: 243 \tTraining Loss: 3.413778\n",
      "Epoch: 243 \tTraining Loss: 2.871479\n",
      "Epoch: 243 \tTraining Loss: 3.699448\n",
      "Epoch: 243 \tTraining Loss: 2.000909\n",
      "Epoch: 243 \tTraining Loss: 2.673399\n",
      "Epoch: 243 \tTraining Loss: 1.863241\n",
      "Epoch: 243 \tTraining Loss: 2.303642\n",
      "Epoch: 243 \tTraining Loss: 2.038500\n",
      "Epoch: 243 \tTraining Loss: 1.995309\n",
      "Epoch: 243 \tTraining Loss: 4.501815\n",
      "Epoch: 243 \tTraining Loss: 3.253684\n",
      "Epoch: 244 \tTraining Loss: 3.662601\n",
      "Epoch: 244 \tTraining Loss: 2.612892\n",
      "Epoch: 244 \tTraining Loss: 3.225358\n",
      "Epoch: 244 \tTraining Loss: 3.394703\n",
      "Epoch: 244 \tTraining Loss: 2.958329\n",
      "Epoch: 244 \tTraining Loss: 1.853490\n",
      "Epoch: 244 \tTraining Loss: 2.582975\n",
      "Epoch: 244 \tTraining Loss: 2.248692\n",
      "Epoch: 244 \tTraining Loss: 2.698312\n",
      "Epoch: 244 \tTraining Loss: 2.779341\n",
      "Epoch: 244 \tTraining Loss: 2.718201\n",
      "Epoch: 244 \tTraining Loss: 1.677396\n",
      "Epoch: 244 \tTraining Loss: 2.149836\n",
      "Epoch: 244 \tTraining Loss: 2.864711\n",
      "Epoch: 244 \tTraining Loss: 2.271430\n",
      "Epoch: 244 \tTraining Loss: 2.733309\n",
      "Epoch: 244 \tTraining Loss: 2.527994\n",
      "Epoch: 244 \tTraining Loss: 1.968171\n",
      "Epoch: 244 \tTraining Loss: 3.909657\n",
      "Epoch: 244 \tTraining Loss: 2.728144\n",
      "Epoch: 244 \tTraining Loss: 2.318480\n",
      "Epoch: 244 \tTraining Loss: 1.723085\n",
      "Epoch: 244 \tTraining Loss: 2.368080\n",
      "Epoch: 244 \tTraining Loss: 3.651299\n",
      "Epoch: 244 \tTraining Loss: 1.911364\n",
      "Epoch: 244 \tTraining Loss: 1.579812\n",
      "Epoch: 244 \tTraining Loss: 2.507878\n",
      "Epoch: 244 \tTraining Loss: 2.020788\n",
      "Epoch: 244 \tTraining Loss: 1.988239\n",
      "Epoch: 244 \tTraining Loss: 2.352185\n",
      "Epoch: 244 \tTraining Loss: 2.611085\n",
      "Epoch: 244 \tTraining Loss: 1.827067\n",
      "Epoch: 245 \tTraining Loss: 2.464737\n",
      "Epoch: 245 \tTraining Loss: 2.670668\n",
      "Epoch: 245 \tTraining Loss: 2.750780\n",
      "Epoch: 245 \tTraining Loss: 2.305997\n",
      "Epoch: 245 \tTraining Loss: 2.348438\n",
      "Epoch: 245 \tTraining Loss: 2.554844\n",
      "Epoch: 245 \tTraining Loss: 1.908865\n",
      "Epoch: 245 \tTraining Loss: 1.762805\n",
      "Epoch: 245 \tTraining Loss: 2.053398\n",
      "Epoch: 245 \tTraining Loss: 3.187561\n",
      "Epoch: 245 \tTraining Loss: 2.396142\n",
      "Epoch: 245 \tTraining Loss: 2.581369\n",
      "Epoch: 245 \tTraining Loss: 2.428396\n",
      "Epoch: 245 \tTraining Loss: 3.386008\n",
      "Epoch: 245 \tTraining Loss: 2.196975\n",
      "Epoch: 245 \tTraining Loss: 3.724397\n",
      "Epoch: 245 \tTraining Loss: 2.785615\n",
      "Epoch: 245 \tTraining Loss: 2.813496\n",
      "Epoch: 245 \tTraining Loss: 4.198330\n",
      "Epoch: 245 \tTraining Loss: 1.807757\n",
      "Epoch: 245 \tTraining Loss: 1.963127\n",
      "Epoch: 245 \tTraining Loss: 1.739800\n",
      "Epoch: 245 \tTraining Loss: 1.988383\n",
      "Epoch: 245 \tTraining Loss: 2.451345\n",
      "Epoch: 245 \tTraining Loss: 2.584958\n",
      "Epoch: 245 \tTraining Loss: 2.539398\n",
      "Epoch: 245 \tTraining Loss: 2.459902\n",
      "Epoch: 245 \tTraining Loss: 2.828531\n",
      "Epoch: 245 \tTraining Loss: 2.809832\n",
      "Epoch: 245 \tTraining Loss: 3.144432\n",
      "Epoch: 245 \tTraining Loss: 1.787491\n",
      "Epoch: 245 \tTraining Loss: 2.368065\n",
      "Epoch: 246 \tTraining Loss: 2.813573\n",
      "Epoch: 246 \tTraining Loss: 2.423853\n",
      "Epoch: 246 \tTraining Loss: 2.006713\n",
      "Epoch: 246 \tTraining Loss: 2.007677\n",
      "Epoch: 246 \tTraining Loss: 2.427043\n",
      "Epoch: 246 \tTraining Loss: 2.661554\n",
      "Epoch: 246 \tTraining Loss: 2.271040\n",
      "Epoch: 246 \tTraining Loss: 1.881781\n",
      "Epoch: 246 \tTraining Loss: 2.046029\n",
      "Epoch: 246 \tTraining Loss: 1.885463\n",
      "Epoch: 246 \tTraining Loss: 3.629682\n",
      "Epoch: 246 \tTraining Loss: 4.030998\n",
      "Epoch: 246 \tTraining Loss: 3.335247\n",
      "Epoch: 246 \tTraining Loss: 2.406031\n",
      "Epoch: 246 \tTraining Loss: 3.653336\n",
      "Epoch: 246 \tTraining Loss: 2.341969\n",
      "Epoch: 246 \tTraining Loss: 2.742559\n",
      "Epoch: 246 \tTraining Loss: 1.730195\n",
      "Epoch: 246 \tTraining Loss: 4.235573\n",
      "Epoch: 246 \tTraining Loss: 1.803549\n",
      "Epoch: 246 \tTraining Loss: 1.546623\n",
      "Epoch: 246 \tTraining Loss: 1.800240\n",
      "Epoch: 246 \tTraining Loss: 2.447138\n",
      "Epoch: 246 \tTraining Loss: 2.717464\n",
      "Epoch: 246 \tTraining Loss: 2.299734\n",
      "Epoch: 246 \tTraining Loss: 1.837746\n",
      "Epoch: 246 \tTraining Loss: 2.108660\n",
      "Epoch: 246 \tTraining Loss: 3.532464\n",
      "Epoch: 246 \tTraining Loss: 2.593243\n",
      "Epoch: 246 \tTraining Loss: 3.399264\n",
      "Epoch: 246 \tTraining Loss: 2.909972\n",
      "Epoch: 246 \tTraining Loss: 2.519372\n",
      "Epoch: 247 \tTraining Loss: 3.222035\n",
      "Epoch: 247 \tTraining Loss: 2.234991\n",
      "Epoch: 247 \tTraining Loss: 2.312285\n",
      "Epoch: 247 \tTraining Loss: 3.176772\n",
      "Epoch: 247 \tTraining Loss: 4.400401\n",
      "Epoch: 247 \tTraining Loss: 2.849581\n",
      "Epoch: 247 \tTraining Loss: 3.687581\n",
      "Epoch: 247 \tTraining Loss: 2725.697522\n",
      "Epoch: 247 \tTraining Loss: 89.073456\n",
      "Epoch: 247 \tTraining Loss: 12.240466\n",
      "Epoch: 247 \tTraining Loss: 12.946724\n",
      "Epoch: 247 \tTraining Loss: 11.226425\n",
      "Epoch: 247 \tTraining Loss: 11.613647\n",
      "Epoch: 247 \tTraining Loss: 8.318343\n",
      "Epoch: 247 \tTraining Loss: 10.378883\n",
      "Epoch: 247 \tTraining Loss: 7.995431\n",
      "Epoch: 247 \tTraining Loss: 7.546056\n",
      "Epoch: 247 \tTraining Loss: 8.031525\n",
      "Epoch: 247 \tTraining Loss: 8.307985\n",
      "Epoch: 247 \tTraining Loss: 6.830557\n",
      "Epoch: 247 \tTraining Loss: 10.971851\n",
      "Epoch: 247 \tTraining Loss: 9.849760\n",
      "Epoch: 247 \tTraining Loss: 10.864399\n",
      "Epoch: 247 \tTraining Loss: 7.797915\n",
      "Epoch: 247 \tTraining Loss: 7.010767\n",
      "Epoch: 247 \tTraining Loss: 9.396194\n",
      "Epoch: 247 \tTraining Loss: 9.101599\n",
      "Epoch: 247 \tTraining Loss: 8.542485\n",
      "Epoch: 247 \tTraining Loss: 9.755604\n",
      "Epoch: 247 \tTraining Loss: 7.538311\n",
      "Epoch: 247 \tTraining Loss: 7.633711\n",
      "Epoch: 247 \tTraining Loss: 5.609460\n",
      "Epoch: 248 \tTraining Loss: 7.452905\n",
      "Epoch: 248 \tTraining Loss: 8.748504\n",
      "Epoch: 248 \tTraining Loss: 7.554093\n",
      "Epoch: 248 \tTraining Loss: 6.830762\n",
      "Epoch: 248 \tTraining Loss: 11.182853\n",
      "Epoch: 248 \tTraining Loss: 10.130835\n",
      "Epoch: 248 \tTraining Loss: 7.583477\n",
      "Epoch: 248 \tTraining Loss: 8.098204\n",
      "Epoch: 248 \tTraining Loss: 7.352092\n",
      "Epoch: 248 \tTraining Loss: 7.547324\n",
      "Epoch: 248 \tTraining Loss: 7.262040\n",
      "Epoch: 248 \tTraining Loss: 7.746541\n",
      "Epoch: 248 \tTraining Loss: 7.937976\n",
      "Epoch: 248 \tTraining Loss: 9.547996\n",
      "Epoch: 248 \tTraining Loss: 8.658855\n",
      "Epoch: 248 \tTraining Loss: 10.357666\n",
      "Epoch: 248 \tTraining Loss: 10.915077\n",
      "Epoch: 248 \tTraining Loss: 7.958795\n",
      "Epoch: 248 \tTraining Loss: 8.675501\n",
      "Epoch: 248 \tTraining Loss: 8.408906\n",
      "Epoch: 248 \tTraining Loss: 6.803977\n",
      "Epoch: 248 \tTraining Loss: 7.407268\n",
      "Epoch: 248 \tTraining Loss: 6.890775\n",
      "Epoch: 248 \tTraining Loss: 8.530401\n",
      "Epoch: 248 \tTraining Loss: 5.936340\n",
      "Epoch: 248 \tTraining Loss: 8.589343\n",
      "Epoch: 248 \tTraining Loss: 9.758460\n",
      "Epoch: 248 \tTraining Loss: 8.084067\n",
      "Epoch: 248 \tTraining Loss: 7.608789\n",
      "Epoch: 248 \tTraining Loss: 8.198213\n",
      "Epoch: 248 \tTraining Loss: 7.425183\n",
      "Epoch: 248 \tTraining Loss: 7.647334\n",
      "Epoch: 249 \tTraining Loss: 8.520622\n",
      "Epoch: 249 \tTraining Loss: 7.168652\n",
      "Epoch: 249 \tTraining Loss: 5.974724\n",
      "Epoch: 249 \tTraining Loss: 7.927401\n",
      "Epoch: 249 \tTraining Loss: 7.107811\n",
      "Epoch: 249 \tTraining Loss: 10.191901\n",
      "Epoch: 249 \tTraining Loss: 9.168007\n",
      "Epoch: 249 \tTraining Loss: 7.612191\n",
      "Epoch: 249 \tTraining Loss: 10.073268\n",
      "Epoch: 249 \tTraining Loss: 8.787673\n",
      "Epoch: 249 \tTraining Loss: 9.504829\n",
      "Epoch: 249 \tTraining Loss: 8.984507\n",
      "Epoch: 249 \tTraining Loss: 7.260222\n",
      "Epoch: 249 \tTraining Loss: 9.433459\n",
      "Epoch: 249 \tTraining Loss: 10.633747\n",
      "Epoch: 249 \tTraining Loss: 7.185848\n",
      "Epoch: 249 \tTraining Loss: 7.935519\n",
      "Epoch: 249 \tTraining Loss: 8.602179\n",
      "Epoch: 249 \tTraining Loss: 7.473210\n",
      "Epoch: 249 \tTraining Loss: 7.072888\n",
      "Epoch: 249 \tTraining Loss: 7.122758\n",
      "Epoch: 249 \tTraining Loss: 9.578480\n",
      "Epoch: 249 \tTraining Loss: 9.445396\n",
      "Epoch: 249 \tTraining Loss: 7.234759\n",
      "Epoch: 249 \tTraining Loss: 8.847575\n",
      "Epoch: 249 \tTraining Loss: 10.891903\n",
      "Epoch: 249 \tTraining Loss: 8.448647\n",
      "Epoch: 249 \tTraining Loss: 6.219130\n",
      "Epoch: 249 \tTraining Loss: 7.030183\n",
      "Epoch: 249 \tTraining Loss: 7.864685\n",
      "Epoch: 249 \tTraining Loss: 8.828504\n",
      "Epoch: 249 \tTraining Loss: 11.514642\n",
      "Epoch: 250 \tTraining Loss: 6.993083\n",
      "Epoch: 250 \tTraining Loss: 8.734693\n",
      "Epoch: 250 \tTraining Loss: 11.866682\n",
      "Epoch: 250 \tTraining Loss: 10.366562\n",
      "Epoch: 250 \tTraining Loss: 8.768275\n",
      "Epoch: 250 \tTraining Loss: 12.450027\n",
      "Epoch: 250 \tTraining Loss: 8.159058\n",
      "Epoch: 250 \tTraining Loss: 7.051141\n",
      "Epoch: 250 \tTraining Loss: 8.929499\n",
      "Epoch: 250 \tTraining Loss: 6.642288\n",
      "Epoch: 250 \tTraining Loss: 9.412349\n",
      "Epoch: 250 \tTraining Loss: 7.609578\n",
      "Epoch: 250 \tTraining Loss: 7.759499\n",
      "Epoch: 250 \tTraining Loss: 7.689518\n",
      "Epoch: 250 \tTraining Loss: 8.105890\n",
      "Epoch: 250 \tTraining Loss: 8.054095\n",
      "Epoch: 250 \tTraining Loss: 6.405851\n",
      "Epoch: 250 \tTraining Loss: 8.448041\n",
      "Epoch: 250 \tTraining Loss: 8.750310\n",
      "Epoch: 250 \tTraining Loss: 8.581215\n",
      "Epoch: 250 \tTraining Loss: 8.032236\n",
      "Epoch: 250 \tTraining Loss: 8.772046\n",
      "Epoch: 250 \tTraining Loss: 7.673285\n",
      "Epoch: 250 \tTraining Loss: 7.337169\n",
      "Epoch: 250 \tTraining Loss: 6.146290\n",
      "Epoch: 250 \tTraining Loss: 8.149022\n",
      "Epoch: 250 \tTraining Loss: 7.360265\n",
      "Epoch: 250 \tTraining Loss: 5.054810\n",
      "Epoch: 250 \tTraining Loss: 6.310107\n",
      "Epoch: 250 \tTraining Loss: 6.752989\n",
      "Epoch: 250 \tTraining Loss: 6.915818\n",
      "Epoch: 250 \tTraining Loss: 6.256070\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            y_hat = y_hat.view(y_hat.shape[0]).double()\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        #eval_model(model, val_loader)\n",
    "\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 250\n",
    "train(model, train_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), DATA_PATH+\"/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mse: 91337.575\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "91337.57466707213"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "model_loaded = EpisiodeCNN()\n",
    "model_loaded.load_state_dict(torch.load(DATA_PATH+\"/model.pt\"))\n",
    "eval_model(model_loaded, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}