{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "fb43f3484ec9eb2eb818f2c4b60702eb23b3e73ba2f9349f357001e8ed925189"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 29\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"/Users/prashanti.nilayam/Desktop/temp/\"\n",
    "prev_value_map = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value_map = {\"Capillary refill rate\": 0.0,\n",
    "    \"Diastolic blood pressure\": 59.0,\n",
    "    \"Fraction inspired oxygen\": 0.21,\n",
    "    \"Glascow coma scale eye opening\": 4,\n",
    "    \"Glascow coma scale motor response\": 6,\n",
    "    \"Glascow coma scale total\": 15,\n",
    "    \"Glascow coma scale verbal response\": 5,\n",
    "    \"Glucose\": 128.0,\n",
    "    \"Heart Rate\": 86,\n",
    "    \"Height\": 170.0,\n",
    "    \"Mean blood pressure\": 77.0,\n",
    "    \"Oxygen saturation\": 98.0,\n",
    "    \"Respiratory rate\": 19,\n",
    "    \"Systolic blood pressure\": 118.0,\n",
    "    \"Temperature\": 36.6,\n",
    "    \"Weight\": 81.0,\n",
    "    \"pH\": 7.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_map = {\n",
    "    \"Glascow coma scale eye opening\":{\n",
    "        \"1 No Response\" : 1,\n",
    "        \"No Response\" : 1,\n",
    "        \"2 To pain\" : 2,\n",
    "        \"To Pain\" : 2,\n",
    "        \"3 To speech\" : 3,\n",
    "        \"To Speech\" : 3,\n",
    "        \"4 Spontaneously\" : 4,\n",
    "        \"Spontaneously\" : 4,\n",
    "        \"None\" : 5\n",
    "    },\n",
    "    \"Glascow coma scale motor response\":{\n",
    "        \"1 No Response\": 1,\n",
    "        \"2 Abnorm extensn\" : 2,\n",
    "        \"Abnormal extension\": 2,\n",
    "        \"3 Abnorm flexion\": 3,\n",
    "        \"Abnormal Flexion\": 3,\n",
    "        \"4 Flex-withdraws\" : 4,\n",
    "        \"Flex-withdraws\": 4,\n",
    "        \"5 Localizes Pain\": 5,\n",
    "        \"Localizes Pain\": 5,\n",
    "        \"6 Obeys Commands\": 6,\n",
    "        \"Obeys Commands\": 6,\n",
    "        \"No response\" : 7,\n",
    "    },\n",
    "    \"Glascow coma scale verbal response\":{\n",
    "        \"1 No Response\" :1,\n",
    "        \"No Response\":1,\n",
    "        \"2 Incomp sounds\": 2,\n",
    "        \"Incomprehensible sounds\":2,\n",
    "        \"3 Inapprop words\":3,\n",
    "        \"Inappropriate Words\":3,\n",
    "        \"4 Confused\":4,\n",
    "        \"Confused\":4,\n",
    "        \"5 Oriented\":5,\n",
    "        \"Oriented\":5,\n",
    "        \"No Response-ETT\":6,\n",
    "        \"1.0 ET/Trach\":7\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(elem):\n",
    "    return np.concatenate([np.array(i) for i in elem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(episode_df):\n",
    "    episode_df[\"Glascow coma scale eye opening\"] = episode_df[\"Glascow coma scale eye opening\"].apply(lambda x: replacement_map[\"Glascow coma scale eye opening\"][x] if x in replacement_map[\"Glascow coma scale eye opening\"] else x)\n",
    "    episode_df[\"Glascow coma scale motor response\"] = episode_df[\"Glascow coma scale motor response\"].apply(lambda x: replacement_map[\"Glascow coma scale motor response\"][x] if x in replacement_map[\"Glascow coma scale motor response\"] else x)\n",
    "    episode_df[\"Glascow coma scale verbal response\"] = episode_df[\"Glascow coma scale verbal response\"].apply(lambda x: replacement_map[\"Glascow coma scale verbal response\"][x] if x in replacement_map[\"Glascow coma scale verbal response\"] else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_capillary_refill_rate(person_id, value, colname):\n",
    "    if value is not None and not np.isnan(value):\n",
    "        prev_value_map[person_id][colname] = value\n",
    "        return value\n",
    "    if person_id in prev_value_map and colname in prev_value_map[person_id] and prev_value_map[person_id][colname] is not None:\n",
    "        prev = prev_value_map[person_id][colname]\n",
    "    else:\n",
    "        prev = default_value_map[colname]\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(pateint_id, episode_df):\n",
    "     prev_value_map[pateint_id] = {}\n",
    "     episode_df[\"Capillary refill rate\"] = episode_df[\"Capillary refill rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Capillary refill rate\"))\n",
    "     episode_df[\"Diastolic blood pressure\"] = episode_df[\"Diastolic blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Diastolic blood pressure\"))\n",
    "     episode_df[\"Fraction inspired oxygen\"] = episode_df[\"Fraction inspired oxygen\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Fraction inspired oxygen\"))\n",
    "     episode_df[\"Glascow coma scale eye opening\"] = episode_df[\"Glascow coma scale eye opening\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale eye opening\"))\n",
    "     episode_df[\"Glascow coma scale motor response\"] = episode_df[\"Glascow coma scale motor response\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale motor response\"))\n",
    "     episode_df[\"Glascow coma scale total\"] = episode_df[\"Glascow coma scale total\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glascow coma scale total\"))\n",
    "     episode_df[\"Glascow coma scale verbal response\"] = episode_df[\"Glascow coma scale verbal response\"].apply(lambda x: process_capillary_refill_rate(pateint_id,  x, \"Glascow coma scale verbal response\"))\n",
    "     episode_df[\"Glucose\"] = episode_df[\"Glucose\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Glucose\"))\n",
    "     episode_df[\"Heart Rate\"] = episode_df[\"Heart Rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Heart Rate\"))\n",
    "     episode_df[\"Mean blood pressure\"] = episode_df[\"Mean blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Mean blood pressure\"))\n",
    "     episode_df[\"Height\"] = episode_df[\"Height\"].apply(lambda x: process_capillary_refill_rate(pateint_id,x, \"Height\"))\n",
    "     episode_df[\"Oxygen saturation\"] = episode_df[\"Oxygen saturation\"].apply(lambda x: process_capillary_refill_rate(pateint_id,  x, \"Oxygen saturation\"))\n",
    "     episode_df[\"Respiratory rate\"] = episode_df[\"Respiratory rate\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Respiratory rate\"))\n",
    "     episode_df[\"Systolic blood pressure\"] = episode_df[\"Systolic blood pressure\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Systolic blood pressure\"))\n",
    "     episode_df[\"Temperature\"] = episode_df[\"Temperature\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Temperature\"))\n",
    "     episode_df[\"Weight\"] = episode_df[\"Weight\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"Weight\"))\n",
    "     episode_df[\"pH\"] = episode_df[\"pH\"].apply(lambda x: process_capillary_refill_rate(pateint_id, x, \"pH\"))\n",
    "     del prev_value_map[pateint_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_indices(data_len):\n",
    "    i = 0\n",
    "    indices = []\n",
    "    while i <= data_len-4:\n",
    "        indices.append([i, i+1, i+2, i+3])\n",
    "        i +=1\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "get_window_indices(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    x_path = DATA_PATH +'/'+path+'/'\n",
    "    X = torch.empty(0,17,4)\n",
    "    Y = torch.empty(0,)\n",
    "    y_df = pd.read_csv(DATA_PATH + path +'_listfile.csv') \n",
    "    data_files = os.listdir(x_path)\n",
    "    print(data_files)\n",
    "    for data_file in data_files:\n",
    "        print(data_file)\n",
    "        if data_file.endswith(\".csv\"):\n",
    "            episode_df = pd.read_csv(x_path + data_file)\n",
    "            cleanup(episode_df)\n",
    "            fill_missing_values(data_file, episode_df)\n",
    "            episode_df[\"H_IDX\"] = episode_df.Hours.apply(np.floor).astype('int32')\n",
    "            episode_df = episode_df.groupby(by = \"H_IDX\").mean()\n",
    "            episode_df = episode_df[episode_df.Hours>=5].reset_index(drop = True)\n",
    "            temp_y = y_df[y_df.stay == data_file].sort_values(by = \"period_length\").reset_index(drop = True)\n",
    "            temp_y = temp_y[[\"period_length\", \"y_true\"]].set_index(\"period_length\")\n",
    "            episode_df = episode_df.join(temp_y, how = \"inner\").drop('Hours', axis=1).reset_index(drop = True)\n",
    "            if(len(episode_df) >0):\n",
    "                indices = get_window_indices(len(episode_df))\n",
    "                windows = []\n",
    "                y_values = []\n",
    "                for idx in indices:\n",
    "                    window = episode_df.loc[idx]\n",
    "                    y_values.append(window.loc[idx[-1]].y_true)\n",
    "                    windows.append(window.drop(\"y_true\", axis=1).transpose().values.astype(np.float32))\n",
    "                t_windows = torch.tensor(windows)\n",
    "                t_y_values = torch.tensor(y_values)\n",
    "                X = torch.cat((X, t_windows), 0)\n",
    "                Y = torch.cat((Y, t_y_values), 0)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['.DS_Store', '10021_episode1_timeseries.csv', '10010_episode1_timeseries.csv', '10003_episode1_timeseries.csv', '10014_episode1_timeseries.csv', '10007_episode1_timeseries.csv', '1000_episode1_timeseries.csv', '10017_episode1_timeseries.csv', '10022_episode1_timeseries.csv', '10013_episode1_timeseries.csv']\n",
      ".DS_Store\n",
      "10021_episode1_timeseries.csv\n",
      "10010_episode1_timeseries.csv\n",
      "10003_episode1_timeseries.csv\n",
      "10014_episode1_timeseries.csv\n",
      "10007_episode1_timeseries.csv\n",
      "1000_episode1_timeseries.csv\n",
      "10017_episode1_timeseries.csv\n",
      "10022_episode1_timeseries.csv\n",
      "10013_episode1_timeseries.csv\n",
      "['10006_episode1_timeseries.csv', '.DS_Store', '10004_episode2_timeseries.csv', '10004_episode1_timeseries.csv']\n",
      "10006_episode1_timeseries.csv\n",
      ".DS_Store\n",
      "10004_episode2_timeseries.csv\n",
      "10004_episode1_timeseries.csv\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EpisodeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, obs, los):\n",
    "        self.x = obs\n",
    "        self.y = los\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "        \n",
    "X_train, Y_train = preprocess('train')\n",
    "train_dataset = EpisodeDataset(X_train, Y_train)\n",
    "X_val, Y_val = preprocess('val')\n",
    "val_dataset = EpisodeDataset(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True)                              \n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=32, shuffle=False)                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [5.0000e+01, 5.7000e+01, 5.7000e+01, 5.1000e+01],\n        [2.1000e-01, 2.1000e-01, 2.1000e-01, 2.1000e-01],\n        [4.0000e+00, 4.0000e+00, 4.0000e+00, 4.0000e+00],\n        [6.0000e+00, 6.0000e+00, 6.0000e+00, 6.0000e+00],\n        [1.5000e+01, 1.5000e+01, 1.5000e+01, 1.5000e+01],\n        [5.0000e+00, 5.0000e+00, 5.0000e+00, 5.0000e+00],\n        [9.3000e+01, 2.3200e+02, 2.3200e+02, 2.3200e+02],\n        [7.6000e+01, 7.4000e+01, 7.0000e+01, 7.1000e+01],\n        [1.7000e+02, 1.7000e+02, 1.7000e+02, 1.7000e+02],\n        [7.2333e+01, 8.0667e+01, 8.0667e+01, 7.4333e+01],\n        [1.0000e+02, 1.0000e+02, 1.0000e+02, 9.8000e+01],\n        [2.2000e+01, 1.5000e+01, 2.2000e+01, 2.0000e+01],\n        [1.1700e+02, 1.2800e+02, 1.2800e+02, 1.2100e+02],\n        [3.7556e+01, 3.6444e+01, 3.6444e+01, 3.7389e+01],\n        [8.1000e+01, 8.1000e+01, 8.1000e+01, 8.1000e+01],\n        [7.4000e+00, 7.4000e+00, 7.4000e+00, 7.4000e+00]]) tensor(31.1800, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for data in val_dataset:\n",
    "    print(data[0], data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EpisiodeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EpisiodeCNN, self).__init__()\n",
    "        #input shape 1 * 17 * 4\n",
    "        #output shape 17 * 17 * 4\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=17, kernel_size=3, padding=1, stride = 1)\n",
    "        #input shape 17 * 17 * 4\n",
    "        #output shape 68 * 17 * 4\n",
    "        self.conv2 = nn.Conv2d(in_channels=17, out_channels=34, kernel_size=3, padding=1, stride = 1)\n",
    "        #input shape 34 * 17 * 4\n",
    "        #output shape 34 * 8 * 2\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        #input shape 34 * 8 * 2\n",
    "        #output shape 68 * 8 * 2\n",
    "        self.conv3 = nn.Conv2d(in_channels=34, out_channels=68, kernel_size=3, padding=1, stride = 1)\n",
    "        self.fc1 = nn.Linear(68*8*2, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #input is of shape (batch_size=32, 3, 224, 224) if you did the dataloader right\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = x.view(-1, 68 * 8 * 2)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EpisiodeCNN()\n",
    "learning_rate = 0.00001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_y_true = torch.DoubleTensor()\n",
    "    all_y_pred = torch.DoubleTensor()\n",
    "    for x, y in val_loader:\n",
    "        y_hat = model(x)\n",
    "        all_y_true = torch.cat((all_y_true, y.to('cpu')), dim=0)\n",
    "        all_y_pred = torch.cat((all_y_pred,  y_hat.to('cpu')), dim=0)\n",
    "    mse= mean_squared_error(all_y_true.detach().numpy(), all_y_pred.detach().numpy())\n",
    "    print(f\"mse: {mse:.3f}\")\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "h: 9 \tTraining Loss: 3921.843105\n",
      "Epoch: 9 \tTraining Loss: 6160.921161\n",
      "Epoch: 9 \tTraining Loss: 6840.539089\n",
      "Epoch: 9 \tTraining Loss: 4948.254384\n",
      "Epoch: 9 \tTraining Loss: 5902.978822\n",
      "Epoch: 9 \tTraining Loss: 4705.889201\n",
      "Epoch: 9 \tTraining Loss: 5391.649305\n",
      "Epoch: 9 \tTraining Loss: 5862.548983\n",
      "mse: 6886.409\n",
      "Epoch: 10 \tTraining Loss: 5495.734153\n",
      "Epoch: 10 \tTraining Loss: 4494.424250\n",
      "Epoch: 10 \tTraining Loss: 6149.102043\n",
      "Epoch: 10 \tTraining Loss: 5965.194009\n",
      "Epoch: 10 \tTraining Loss: 3729.190124\n",
      "Epoch: 10 \tTraining Loss: 4676.429008\n",
      "Epoch: 10 \tTraining Loss: 3691.105245\n",
      "Epoch: 10 \tTraining Loss: 5978.315370\n",
      "Epoch: 10 \tTraining Loss: 5438.181704\n",
      "Epoch: 10 \tTraining Loss: 5107.474373\n",
      "Epoch: 10 \tTraining Loss: 5915.862023\n",
      "Epoch: 10 \tTraining Loss: 4313.433090\n",
      "Epoch: 10 \tTraining Loss: 5741.168999\n",
      "Epoch: 10 \tTraining Loss: 4515.954368\n",
      "Epoch: 10 \tTraining Loss: 3973.846340\n",
      "Epoch: 10 \tTraining Loss: 5583.563349\n",
      "Epoch: 10 \tTraining Loss: 4037.188831\n",
      "Epoch: 10 \tTraining Loss: 5799.311178\n",
      "Epoch: 10 \tTraining Loss: 6574.428707\n",
      "Epoch: 10 \tTraining Loss: 6169.143672\n",
      "Epoch: 10 \tTraining Loss: 4555.142663\n",
      "Epoch: 10 \tTraining Loss: 8097.806797\n",
      "Epoch: 10 \tTraining Loss: 5102.223394\n",
      "Epoch: 10 \tTraining Loss: 5939.418679\n",
      "Epoch: 10 \tTraining Loss: 4704.439673\n",
      "Epoch: 10 \tTraining Loss: 4316.333649\n",
      "Epoch: 10 \tTraining Loss: 5114.782934\n",
      "Epoch: 10 \tTraining Loss: 5751.602455\n",
      "Epoch: 10 \tTraining Loss: 4158.505028\n",
      "Epoch: 10 \tTraining Loss: 4711.819304\n",
      "Epoch: 10 \tTraining Loss: 4681.563263\n",
      "Epoch: 10 \tTraining Loss: 3175.333989\n",
      "mse: 5085.507\n",
      "Epoch: 11 \tTraining Loss: 4684.798940\n",
      "Epoch: 11 \tTraining Loss: 6413.929704\n",
      "Epoch: 11 \tTraining Loss: 2909.551478\n",
      "Epoch: 11 \tTraining Loss: 4945.761021\n",
      "Epoch: 11 \tTraining Loss: 5047.664735\n",
      "Epoch: 11 \tTraining Loss: 5668.492530\n",
      "Epoch: 11 \tTraining Loss: 3464.870569\n",
      "Epoch: 11 \tTraining Loss: 5344.278000\n",
      "Epoch: 11 \tTraining Loss: 5015.583058\n",
      "Epoch: 11 \tTraining Loss: 3914.842610\n",
      "Epoch: 11 \tTraining Loss: 4503.350727\n",
      "Epoch: 11 \tTraining Loss: 3215.526252\n",
      "Epoch: 11 \tTraining Loss: 5656.362463\n",
      "Epoch: 11 \tTraining Loss: 3292.641819\n",
      "Epoch: 11 \tTraining Loss: 5494.964685\n",
      "Epoch: 11 \tTraining Loss: 4009.006057\n",
      "Epoch: 11 \tTraining Loss: 3773.221954\n",
      "Epoch: 11 \tTraining Loss: 3160.304706\n",
      "Epoch: 11 \tTraining Loss: 3843.650094\n",
      "Epoch: 11 \tTraining Loss: 3949.939025\n",
      "Epoch: 11 \tTraining Loss: 4093.167258\n",
      "Epoch: 11 \tTraining Loss: 3300.281446\n",
      "Epoch: 11 \tTraining Loss: 5004.466630\n",
      "Epoch: 11 \tTraining Loss: 4385.659701\n",
      "Epoch: 11 \tTraining Loss: 5929.594999\n",
      "Epoch: 11 \tTraining Loss: 3063.549473\n",
      "Epoch: 11 \tTraining Loss: 6636.059093\n",
      "Epoch: 11 \tTraining Loss: 4423.501850\n",
      "Epoch: 11 \tTraining Loss: 3105.890733\n",
      "Epoch: 11 \tTraining Loss: 3374.049186\n",
      "Epoch: 11 \tTraining Loss: 3087.010596\n",
      "Epoch: 11 \tTraining Loss: 2440.106821\n",
      "mse: 9044.583\n",
      "Epoch: 12 \tTraining Loss: 2141.086894\n",
      "Epoch: 12 \tTraining Loss: 3718.050727\n",
      "Epoch: 12 \tTraining Loss: 2333.980455\n",
      "Epoch: 12 \tTraining Loss: 3663.859207\n",
      "Epoch: 12 \tTraining Loss: 3246.197131\n",
      "Epoch: 12 \tTraining Loss: 5127.981826\n",
      "Epoch: 12 \tTraining Loss: 4368.985568\n",
      "Epoch: 12 \tTraining Loss: 3579.877401\n",
      "Epoch: 12 \tTraining Loss: 2279.169692\n",
      "Epoch: 12 \tTraining Loss: 3481.712861\n",
      "Epoch: 12 \tTraining Loss: 3624.685996\n",
      "Epoch: 12 \tTraining Loss: 2057.412985\n",
      "Epoch: 12 \tTraining Loss: 2869.513113\n",
      "Epoch: 12 \tTraining Loss: 3110.304513\n",
      "Epoch: 12 \tTraining Loss: 4363.306407\n",
      "Epoch: 12 \tTraining Loss: 4729.144392\n",
      "Epoch: 12 \tTraining Loss: 2966.991238\n",
      "Epoch: 12 \tTraining Loss: 3090.525228\n",
      "Epoch: 12 \tTraining Loss: 3999.704851\n",
      "Epoch: 12 \tTraining Loss: 4083.228189\n",
      "Epoch: 12 \tTraining Loss: 4031.959309\n",
      "Epoch: 12 \tTraining Loss: 3758.686290\n",
      "Epoch: 12 \tTraining Loss: 3979.391927\n",
      "Epoch: 12 \tTraining Loss: 2944.020343\n",
      "Epoch: 12 \tTraining Loss: 3443.494195\n",
      "Epoch: 12 \tTraining Loss: 4202.940586\n",
      "Epoch: 12 \tTraining Loss: 4657.355622\n",
      "Epoch: 12 \tTraining Loss: 3082.722638\n",
      "Epoch: 12 \tTraining Loss: 2988.031715\n",
      "Epoch: 12 \tTraining Loss: 3059.851320\n",
      "Epoch: 12 \tTraining Loss: 2539.115441\n",
      "Epoch: 12 \tTraining Loss: 2646.703859\n",
      "mse: 23601.945\n",
      "Epoch: 13 \tTraining Loss: 2666.516031\n",
      "Epoch: 13 \tTraining Loss: 2739.715508\n",
      "Epoch: 13 \tTraining Loss: 2911.260075\n",
      "Epoch: 13 \tTraining Loss: 4226.602352\n",
      "Epoch: 13 \tTraining Loss: 3646.274413\n",
      "Epoch: 13 \tTraining Loss: 2541.852454\n",
      "Epoch: 13 \tTraining Loss: 3788.628258\n",
      "Epoch: 13 \tTraining Loss: 3326.853189\n",
      "Epoch: 13 \tTraining Loss: 2107.522536\n",
      "Epoch: 13 \tTraining Loss: 2958.102382\n",
      "Epoch: 13 \tTraining Loss: 1943.576166\n",
      "Epoch: 13 \tTraining Loss: 3038.339457\n",
      "Epoch: 13 \tTraining Loss: 2669.527799\n",
      "Epoch: 13 \tTraining Loss: 2404.154738\n",
      "Epoch: 13 \tTraining Loss: 3130.290277\n",
      "Epoch: 13 \tTraining Loss: 2313.761174\n",
      "Epoch: 13 \tTraining Loss: 2980.916492\n",
      "Epoch: 13 \tTraining Loss: 3454.688247\n",
      "Epoch: 13 \tTraining Loss: 1892.348721\n",
      "Epoch: 13 \tTraining Loss: 2469.197040\n",
      "Epoch: 13 \tTraining Loss: 2613.275819\n",
      "Epoch: 13 \tTraining Loss: 2498.889658\n",
      "Epoch: 13 \tTraining Loss: 3014.846460\n",
      "Epoch: 13 \tTraining Loss: 2770.989080\n",
      "Epoch: 13 \tTraining Loss: 2365.279186\n",
      "Epoch: 13 \tTraining Loss: 2461.579585\n",
      "Epoch: 13 \tTraining Loss: 2745.733412\n",
      "Epoch: 13 \tTraining Loss: 2551.467236\n",
      "Epoch: 13 \tTraining Loss: 3176.048291\n",
      "Epoch: 13 \tTraining Loss: 2836.353495\n",
      "Epoch: 13 \tTraining Loss: 2851.055511\n",
      "Epoch: 13 \tTraining Loss: 2212.536706\n",
      "mse: 49560.046\n",
      "Epoch: 14 \tTraining Loss: 2815.567352\n",
      "Epoch: 14 \tTraining Loss: 2164.303675\n",
      "Epoch: 14 \tTraining Loss: 1668.356245\n",
      "Epoch: 14 \tTraining Loss: 2820.866410\n",
      "Epoch: 14 \tTraining Loss: 2509.255310\n",
      "Epoch: 14 \tTraining Loss: 2471.258799\n",
      "Epoch: 14 \tTraining Loss: 2383.540216\n",
      "Epoch: 14 \tTraining Loss: 2320.861037\n",
      "Epoch: 14 \tTraining Loss: 2884.454883\n",
      "Epoch: 14 \tTraining Loss: 2768.456085\n",
      "Epoch: 14 \tTraining Loss: 3259.460159\n",
      "Epoch: 14 \tTraining Loss: 3224.600608\n",
      "Epoch: 14 \tTraining Loss: 2866.356187\n",
      "Epoch: 14 \tTraining Loss: 2713.503977\n",
      "Epoch: 14 \tTraining Loss: 2566.512589\n",
      "Epoch: 14 \tTraining Loss: 2719.049860\n",
      "Epoch: 14 \tTraining Loss: 2394.338909\n",
      "Epoch: 14 \tTraining Loss: 2051.500444\n",
      "Epoch: 14 \tTraining Loss: 3003.472931\n",
      "Epoch: 14 \tTraining Loss: 2831.820122\n",
      "Epoch: 14 \tTraining Loss: 2247.595255\n",
      "Epoch: 14 \tTraining Loss: 1621.297446\n",
      "Epoch: 14 \tTraining Loss: 2256.872552\n",
      "Epoch: 14 \tTraining Loss: 1764.463256\n",
      "Epoch: 14 \tTraining Loss: 2687.021510\n",
      "Epoch: 14 \tTraining Loss: 2179.868579\n",
      "Epoch: 14 \tTraining Loss: 1709.127117\n",
      "Epoch: 14 \tTraining Loss: 2360.321400\n",
      "Epoch: 14 \tTraining Loss: 2614.945609\n",
      "Epoch: 14 \tTraining Loss: 2213.348271\n",
      "Epoch: 14 \tTraining Loss: 2291.715403\n",
      "Epoch: 14 \tTraining Loss: 2732.631840\n",
      "mse: 75514.490\n",
      "Epoch: 15 \tTraining Loss: 2523.812946\n",
      "Epoch: 15 \tTraining Loss: 1889.388435\n",
      "Epoch: 15 \tTraining Loss: 2037.978986\n",
      "Epoch: 15 \tTraining Loss: 3007.748004\n",
      "Epoch: 15 \tTraining Loss: 2571.612414\n",
      "Epoch: 15 \tTraining Loss: 2788.539360\n",
      "Epoch: 15 \tTraining Loss: 1936.052399\n",
      "Epoch: 15 \tTraining Loss: 2008.751615\n",
      "Epoch: 15 \tTraining Loss: 2837.435395\n",
      "Epoch: 15 \tTraining Loss: 2418.207344\n",
      "Epoch: 15 \tTraining Loss: 2783.243254\n",
      "Epoch: 15 \tTraining Loss: 2057.620579\n",
      "Epoch: 15 \tTraining Loss: 2175.985882\n",
      "Epoch: 15 \tTraining Loss: 2003.689193\n",
      "Epoch: 15 \tTraining Loss: 2355.136664\n",
      "Epoch: 15 \tTraining Loss: 1578.867133\n",
      "Epoch: 15 \tTraining Loss: 2751.930817\n",
      "Epoch: 15 \tTraining Loss: 2664.660896\n",
      "Epoch: 15 \tTraining Loss: 2248.410224\n",
      "Epoch: 15 \tTraining Loss: 1779.907896\n",
      "Epoch: 15 \tTraining Loss: 2873.376270\n",
      "Epoch: 15 \tTraining Loss: 1727.682579\n",
      "Epoch: 15 \tTraining Loss: 2317.974593\n",
      "Epoch: 15 \tTraining Loss: 2789.948550\n",
      "Epoch: 15 \tTraining Loss: 2615.146248\n",
      "Epoch: 15 \tTraining Loss: 1903.044004\n",
      "Epoch: 15 \tTraining Loss: 3374.981493\n",
      "Epoch: 15 \tTraining Loss: 2364.296844\n",
      "Epoch: 15 \tTraining Loss: 2515.546322\n",
      "Epoch: 15 \tTraining Loss: 2773.305116\n",
      "Epoch: 15 \tTraining Loss: 2067.316057\n",
      "Epoch: 15 \tTraining Loss: 2691.247998\n",
      "mse: 88641.193\n",
      "Epoch: 16 \tTraining Loss: 2322.519164\n",
      "Epoch: 16 \tTraining Loss: 2532.379145\n",
      "Epoch: 16 \tTraining Loss: 1752.205018\n",
      "Epoch: 16 \tTraining Loss: 1923.091768\n",
      "Epoch: 16 \tTraining Loss: 2931.725088\n",
      "Epoch: 16 \tTraining Loss: 2649.244899\n",
      "Epoch: 16 \tTraining Loss: 2128.511155\n",
      "Epoch: 16 \tTraining Loss: 3208.501746\n",
      "Epoch: 16 \tTraining Loss: 2197.129378\n",
      "Epoch: 16 \tTraining Loss: 2307.461681\n",
      "Epoch: 16 \tTraining Loss: 2918.421428\n",
      "Epoch: 16 \tTraining Loss: 1961.950798\n",
      "Epoch: 16 \tTraining Loss: 2456.165133\n",
      "Epoch: 16 \tTraining Loss: 2501.494796\n",
      "Epoch: 16 \tTraining Loss: 2854.921418\n",
      "Epoch: 16 \tTraining Loss: 2310.127100\n",
      "Epoch: 16 \tTraining Loss: 1918.304397\n",
      "Epoch: 16 \tTraining Loss: 2395.739634\n",
      "Epoch: 16 \tTraining Loss: 2399.314781\n",
      "Epoch: 16 \tTraining Loss: 2417.583538\n",
      "Epoch: 16 \tTraining Loss: 2232.089026\n",
      "Epoch: 16 \tTraining Loss: 2581.655829\n",
      "Epoch: 16 \tTraining Loss: 3248.310403\n",
      "Epoch: 16 \tTraining Loss: 2773.775210\n",
      "Epoch: 16 \tTraining Loss: 2327.239235\n",
      "Epoch: 16 \tTraining Loss: 1697.631682\n",
      "Epoch: 16 \tTraining Loss: 2273.086315\n",
      "Epoch: 16 \tTraining Loss: 2206.796207\n",
      "Epoch: 16 \tTraining Loss: 2008.150574\n",
      "Epoch: 16 \tTraining Loss: 2438.976164\n",
      "Epoch: 16 \tTraining Loss: 1622.802939\n",
      "Epoch: 16 \tTraining Loss: 2554.746690\n",
      "mse: 92027.266\n",
      "Epoch: 17 \tTraining Loss: 2032.054733\n",
      "Epoch: 17 \tTraining Loss: 2574.786714\n",
      "Epoch: 17 \tTraining Loss: 2479.232370\n",
      "Epoch: 17 \tTraining Loss: 2195.454547\n",
      "Epoch: 17 \tTraining Loss: 2418.140585\n",
      "Epoch: 17 \tTraining Loss: 2127.094937\n",
      "Epoch: 17 \tTraining Loss: 1950.359964\n",
      "Epoch: 17 \tTraining Loss: 2697.710761\n",
      "Epoch: 17 \tTraining Loss: 2379.322183\n",
      "Epoch: 17 \tTraining Loss: 2257.572105\n",
      "Epoch: 17 \tTraining Loss: 2660.609774\n",
      "Epoch: 17 \tTraining Loss: 2446.881767\n",
      "Epoch: 17 \tTraining Loss: 2700.285862\n",
      "Epoch: 17 \tTraining Loss: 2473.190002\n",
      "Epoch: 17 \tTraining Loss: 2382.034435\n",
      "Epoch: 17 \tTraining Loss: 2324.339885\n",
      "Epoch: 17 \tTraining Loss: 2247.940478\n",
      "Epoch: 17 \tTraining Loss: 2653.700430\n",
      "Epoch: 17 \tTraining Loss: 2590.341026\n",
      "Epoch: 17 \tTraining Loss: 2512.022755\n",
      "Epoch: 17 \tTraining Loss: 1765.228096\n",
      "Epoch: 17 \tTraining Loss: 2607.368309\n",
      "Epoch: 17 \tTraining Loss: 3493.123724\n",
      "Epoch: 17 \tTraining Loss: 1787.380926\n",
      "Epoch: 17 \tTraining Loss: 2258.586701\n",
      "Epoch: 17 \tTraining Loss: 2549.992057\n",
      "Epoch: 17 \tTraining Loss: 1780.652834\n",
      "Epoch: 17 \tTraining Loss: 2391.202492\n",
      "Epoch: 17 \tTraining Loss: 2486.947106\n",
      "Epoch: 17 \tTraining Loss: 3081.996591\n",
      "Epoch: 17 \tTraining Loss: 1624.704320\n",
      "Epoch: 17 \tTraining Loss: 1846.366568\n",
      "mse: 89057.764\n",
      "Epoch: 18 \tTraining Loss: 3009.120541\n",
      "Epoch: 18 \tTraining Loss: 1993.656342\n",
      "Epoch: 18 \tTraining Loss: 2475.548148\n",
      "Epoch: 18 \tTraining Loss: 2562.260261\n",
      "Epoch: 18 \tTraining Loss: 2402.967562\n",
      "Epoch: 18 \tTraining Loss: 1752.251332\n",
      "Epoch: 18 \tTraining Loss: 2218.762741\n",
      "Epoch: 18 \tTraining Loss: 2802.618589\n",
      "Epoch: 18 \tTraining Loss: 2744.856374\n",
      "Epoch: 18 \tTraining Loss: 2589.260300\n",
      "Epoch: 18 \tTraining Loss: 2159.473528\n",
      "Epoch: 18 \tTraining Loss: 2868.506200\n",
      "Epoch: 18 \tTraining Loss: 1939.478328\n",
      "Epoch: 18 \tTraining Loss: 2150.656207\n",
      "Epoch: 18 \tTraining Loss: 2332.515027\n",
      "Epoch: 18 \tTraining Loss: 2220.471444\n",
      "Epoch: 18 \tTraining Loss: 2655.114109\n",
      "Epoch: 18 \tTraining Loss: 2013.101789\n",
      "Epoch: 18 \tTraining Loss: 2920.890908\n",
      "Epoch: 18 \tTraining Loss: 2471.589438\n",
      "Epoch: 18 \tTraining Loss: 2416.618552\n",
      "Epoch: 18 \tTraining Loss: 2317.458820\n",
      "Epoch: 18 \tTraining Loss: 2116.718585\n",
      "Epoch: 18 \tTraining Loss: 2269.982498\n",
      "Epoch: 18 \tTraining Loss: 1942.102561\n",
      "Epoch: 18 \tTraining Loss: 2424.960837\n",
      "Epoch: 18 \tTraining Loss: 2247.994011\n",
      "Epoch: 18 \tTraining Loss: 2038.626283\n",
      "Epoch: 18 \tTraining Loss: 2114.313910\n",
      "Epoch: 18 \tTraining Loss: 2580.604082\n",
      "Epoch: 18 \tTraining Loss: 2232.591782\n",
      "Epoch: 18 \tTraining Loss: 3223.856828\n",
      "mse: 93690.534\n",
      "Epoch: 19 \tTraining Loss: 2138.904609\n",
      "Epoch: 19 \tTraining Loss: 2789.130208\n",
      "Epoch: 19 \tTraining Loss: 2376.480925\n",
      "Epoch: 19 \tTraining Loss: 2684.138678\n",
      "Epoch: 19 \tTraining Loss: 1816.644836\n",
      "Epoch: 19 \tTraining Loss: 2508.259226\n",
      "Epoch: 19 \tTraining Loss: 2313.522629\n",
      "Epoch: 19 \tTraining Loss: 2927.929723\n",
      "Epoch: 19 \tTraining Loss: 2502.120987\n",
      "Epoch: 19 \tTraining Loss: 2479.844395\n",
      "Epoch: 19 \tTraining Loss: 2340.101490\n",
      "Epoch: 19 \tTraining Loss: 2443.378405\n",
      "Epoch: 19 \tTraining Loss: 2207.991484\n",
      "Epoch: 19 \tTraining Loss: 2239.483572\n",
      "Epoch: 19 \tTraining Loss: 2459.886469\n",
      "Epoch: 19 \tTraining Loss: 2402.453742\n",
      "Epoch: 19 \tTraining Loss: 2042.534946\n",
      "Epoch: 19 \tTraining Loss: 2970.127253\n",
      "Epoch: 19 \tTraining Loss: 2932.516192\n",
      "Epoch: 19 \tTraining Loss: 2131.935297\n",
      "Epoch: 19 \tTraining Loss: 1775.522044\n",
      "Epoch: 19 \tTraining Loss: 2866.935335\n",
      "Epoch: 19 \tTraining Loss: 2639.585652\n",
      "Epoch: 19 \tTraining Loss: 2253.948161\n",
      "Epoch: 19 \tTraining Loss: 1615.743315\n",
      "Epoch: 19 \tTraining Loss: 2731.226886\n",
      "Epoch: 19 \tTraining Loss: 2058.436021\n",
      "Epoch: 19 \tTraining Loss: 2421.276877\n",
      "Epoch: 19 \tTraining Loss: 2158.459125\n",
      "Epoch: 19 \tTraining Loss: 2706.610709\n",
      "Epoch: 19 \tTraining Loss: 1762.759031\n",
      "Epoch: 19 \tTraining Loss: 1942.592221\n",
      "mse: 91427.392\n",
      "Epoch: 20 \tTraining Loss: 2138.043773\n",
      "Epoch: 20 \tTraining Loss: 2403.895931\n",
      "Epoch: 20 \tTraining Loss: 2930.354843\n",
      "Epoch: 20 \tTraining Loss: 2239.634491\n",
      "Epoch: 20 \tTraining Loss: 2319.984817\n",
      "Epoch: 20 \tTraining Loss: 2575.566542\n",
      "Epoch: 20 \tTraining Loss: 2392.077256\n",
      "Epoch: 20 \tTraining Loss: 2134.226264\n",
      "Epoch: 20 \tTraining Loss: 2489.931932\n",
      "Epoch: 20 \tTraining Loss: 2225.757930\n",
      "Epoch: 20 \tTraining Loss: 2296.466060\n",
      "Epoch: 20 \tTraining Loss: 2224.958157\n",
      "Epoch: 20 \tTraining Loss: 2454.796062\n",
      "Epoch: 20 \tTraining Loss: 2094.548511\n",
      "Epoch: 20 \tTraining Loss: 1929.383679\n",
      "Epoch: 20 \tTraining Loss: 2543.132760\n",
      "Epoch: 20 \tTraining Loss: 2722.883550\n",
      "Epoch: 20 \tTraining Loss: 2358.457261\n",
      "Epoch: 20 \tTraining Loss: 2173.348946\n",
      "Epoch: 20 \tTraining Loss: 2334.506597\n",
      "Epoch: 20 \tTraining Loss: 2604.260954\n",
      "Epoch: 20 \tTraining Loss: 3061.180131\n",
      "Epoch: 20 \tTraining Loss: 2745.293470\n",
      "Epoch: 20 \tTraining Loss: 1792.109724\n",
      "Epoch: 20 \tTraining Loss: 2205.024669\n",
      "Epoch: 20 \tTraining Loss: 1920.455476\n",
      "Epoch: 20 \tTraining Loss: 2236.225833\n",
      "Epoch: 20 \tTraining Loss: 2561.472292\n",
      "Epoch: 20 \tTraining Loss: 2732.474966\n",
      "Epoch: 20 \tTraining Loss: 2598.922979\n",
      "Epoch: 20 \tTraining Loss: 1966.708510\n",
      "Epoch: 20 \tTraining Loss: 2414.960972\n",
      "mse: 92128.708\n",
      "Epoch: 21 \tTraining Loss: 2751.444783\n",
      "Epoch: 21 \tTraining Loss: 2168.648538\n",
      "Epoch: 21 \tTraining Loss: 2729.135194\n",
      "Epoch: 21 \tTraining Loss: 2308.889517\n",
      "Epoch: 21 \tTraining Loss: 2359.676246\n",
      "Epoch: 21 \tTraining Loss: 2361.241878\n",
      "Epoch: 21 \tTraining Loss: 1960.298271\n",
      "Epoch: 21 \tTraining Loss: 2061.900018\n",
      "Epoch: 21 \tTraining Loss: 1939.732633\n",
      "Epoch: 21 \tTraining Loss: 3115.882769\n",
      "Epoch: 21 \tTraining Loss: 2251.228697\n",
      "Epoch: 21 \tTraining Loss: 2494.213195\n",
      "Epoch: 21 \tTraining Loss: 2140.995520\n",
      "Epoch: 21 \tTraining Loss: 1811.851399\n",
      "Epoch: 21 \tTraining Loss: 2703.882734\n",
      "Epoch: 21 \tTraining Loss: 2407.062033\n",
      "Epoch: 21 \tTraining Loss: 2347.685824\n",
      "Epoch: 21 \tTraining Loss: 2065.016983\n",
      "Epoch: 21 \tTraining Loss: 2161.529268\n",
      "Epoch: 21 \tTraining Loss: 2611.780750\n",
      "Epoch: 21 \tTraining Loss: 2143.212673\n",
      "Epoch: 21 \tTraining Loss: 2703.109308\n",
      "Epoch: 21 \tTraining Loss: 2118.869023\n",
      "Epoch: 21 \tTraining Loss: 2487.882737\n",
      "Epoch: 21 \tTraining Loss: 2167.017618\n",
      "Epoch: 21 \tTraining Loss: 2416.902892\n",
      "Epoch: 21 \tTraining Loss: 2528.952585\n",
      "Epoch: 21 \tTraining Loss: 2639.213993\n",
      "Epoch: 21 \tTraining Loss: 2303.270355\n",
      "Epoch: 21 \tTraining Loss: 2018.209426\n",
      "Epoch: 21 \tTraining Loss: 3076.856208\n",
      "Epoch: 21 \tTraining Loss: 2401.857383\n",
      "mse: 90612.770\n",
      "Epoch: 22 \tTraining Loss: 2372.442047\n",
      "Epoch: 22 \tTraining Loss: 2697.787018\n",
      "Epoch: 22 \tTraining Loss: 1976.143893\n",
      "Epoch: 22 \tTraining Loss: 1985.402924\n",
      "Epoch: 22 \tTraining Loss: 2063.665574\n",
      "Epoch: 22 \tTraining Loss: 2571.119533\n",
      "Epoch: 22 \tTraining Loss: 2723.833509\n",
      "Epoch: 22 \tTraining Loss: 1998.782053\n",
      "Epoch: 22 \tTraining Loss: 2642.040468\n",
      "Epoch: 22 \tTraining Loss: 2522.613433\n",
      "Epoch: 22 \tTraining Loss: 2630.427362\n",
      "Epoch: 22 \tTraining Loss: 2464.878464\n",
      "Epoch: 22 \tTraining Loss: 2419.231165\n",
      "Epoch: 22 \tTraining Loss: 2357.740670\n",
      "Epoch: 22 \tTraining Loss: 2727.138087\n",
      "Epoch: 22 \tTraining Loss: 2522.447625\n",
      "Epoch: 22 \tTraining Loss: 1527.301142\n",
      "Epoch: 22 \tTraining Loss: 1831.846988\n",
      "Epoch: 22 \tTraining Loss: 2466.079658\n",
      "Epoch: 22 \tTraining Loss: 2579.702006\n",
      "Epoch: 22 \tTraining Loss: 2632.818083\n",
      "Epoch: 22 \tTraining Loss: 2313.459806\n",
      "Epoch: 22 \tTraining Loss: 2117.850853\n",
      "Epoch: 22 \tTraining Loss: 2612.361798\n",
      "Epoch: 22 \tTraining Loss: 1936.445834\n",
      "Epoch: 22 \tTraining Loss: 2744.233781\n",
      "Epoch: 22 \tTraining Loss: 2513.813442\n",
      "Epoch: 22 \tTraining Loss: 1814.761783\n",
      "Epoch: 22 \tTraining Loss: 2621.722603\n",
      "Epoch: 22 \tTraining Loss: 2921.679882\n",
      "Epoch: 22 \tTraining Loss: 2405.762112\n",
      "Epoch: 22 \tTraining Loss: 1818.707061\n",
      "mse: 92064.115\n",
      "Epoch: 23 \tTraining Loss: 1806.165373\n",
      "Epoch: 23 \tTraining Loss: 2091.369231\n",
      "Epoch: 23 \tTraining Loss: 2711.220350\n",
      "Epoch: 23 \tTraining Loss: 2163.375248\n",
      "Epoch: 23 \tTraining Loss: 2576.472316\n",
      "Epoch: 23 \tTraining Loss: 2114.029955\n",
      "Epoch: 23 \tTraining Loss: 2299.196658\n",
      "Epoch: 23 \tTraining Loss: 2961.857693\n",
      "Epoch: 23 \tTraining Loss: 2069.399451\n",
      "Epoch: 23 \tTraining Loss: 3074.783675\n",
      "Epoch: 23 \tTraining Loss: 1516.098773\n",
      "Epoch: 23 \tTraining Loss: 2292.465488\n",
      "Epoch: 23 \tTraining Loss: 2022.660183\n",
      "Epoch: 23 \tTraining Loss: 2796.758450\n",
      "Epoch: 23 \tTraining Loss: 2929.499072\n",
      "Epoch: 23 \tTraining Loss: 1805.884240\n",
      "Epoch: 23 \tTraining Loss: 2661.476065\n",
      "Epoch: 23 \tTraining Loss: 1958.600669\n",
      "Epoch: 23 \tTraining Loss: 2485.788657\n",
      "Epoch: 23 \tTraining Loss: 2638.099254\n",
      "Epoch: 23 \tTraining Loss: 2331.672045\n",
      "Epoch: 23 \tTraining Loss: 2512.432761\n",
      "Epoch: 23 \tTraining Loss: 2739.694632\n",
      "Epoch: 23 \tTraining Loss: 2067.343180\n",
      "Epoch: 23 \tTraining Loss: 2439.348830\n",
      "Epoch: 23 \tTraining Loss: 2715.278775\n",
      "Epoch: 23 \tTraining Loss: 2384.663456\n",
      "Epoch: 23 \tTraining Loss: 1977.519246\n",
      "Epoch: 23 \tTraining Loss: 2162.617474\n",
      "Epoch: 23 \tTraining Loss: 2237.017536\n",
      "Epoch: 23 \tTraining Loss: 2497.131966\n",
      "Epoch: 23 \tTraining Loss: 2870.498363\n",
      "mse: 90469.487\n",
      "Epoch: 24 \tTraining Loss: 2146.587169\n",
      "Epoch: 24 \tTraining Loss: 3035.255319\n",
      "Epoch: 24 \tTraining Loss: 3032.898725\n",
      "Epoch: 24 \tTraining Loss: 2119.592359\n",
      "Epoch: 24 \tTraining Loss: 2233.122049\n",
      "Epoch: 24 \tTraining Loss: 2675.195993\n",
      "Epoch: 24 \tTraining Loss: 2506.961823\n",
      "Epoch: 24 \tTraining Loss: 1989.164243\n",
      "Epoch: 24 \tTraining Loss: 2764.525696\n",
      "Epoch: 24 \tTraining Loss: 2465.010772\n",
      "Epoch: 24 \tTraining Loss: 2224.473412\n",
      "Epoch: 24 \tTraining Loss: 1939.479666\n",
      "Epoch: 24 \tTraining Loss: 2064.182599\n",
      "Epoch: 24 \tTraining Loss: 2819.664089\n",
      "Epoch: 24 \tTraining Loss: 2451.431982\n",
      "Epoch: 24 \tTraining Loss: 2443.251616\n",
      "Epoch: 24 \tTraining Loss: 2267.462500\n",
      "Epoch: 24 \tTraining Loss: 2102.016123\n",
      "Epoch: 24 \tTraining Loss: 2277.323893\n",
      "Epoch: 24 \tTraining Loss: 2521.217476\n",
      "Epoch: 24 \tTraining Loss: 2407.722620\n",
      "Epoch: 24 \tTraining Loss: 2707.594151\n",
      "Epoch: 24 \tTraining Loss: 2711.298420\n",
      "Epoch: 24 \tTraining Loss: 2438.649239\n",
      "Epoch: 24 \tTraining Loss: 1981.405563\n",
      "Epoch: 24 \tTraining Loss: 2048.560684\n",
      "Epoch: 24 \tTraining Loss: 2691.526966\n",
      "Epoch: 24 \tTraining Loss: 1834.952040\n",
      "Epoch: 24 \tTraining Loss: 1995.836842\n",
      "Epoch: 24 \tTraining Loss: 2205.437163\n",
      "Epoch: 24 \tTraining Loss: 2211.046431\n",
      "Epoch: 24 \tTraining Loss: 2367.336470\n",
      "mse: 90456.571\n",
      "Epoch: 25 \tTraining Loss: 1957.187678\n",
      "Epoch: 25 \tTraining Loss: 2635.244321\n",
      "Epoch: 25 \tTraining Loss: 2885.108829\n",
      "Epoch: 25 \tTraining Loss: 2173.925462\n",
      "Epoch: 25 \tTraining Loss: 2569.675109\n",
      "Epoch: 25 \tTraining Loss: 2630.686132\n",
      "Epoch: 25 \tTraining Loss: 2835.511983\n",
      "Epoch: 25 \tTraining Loss: 2921.285209\n",
      "Epoch: 25 \tTraining Loss: 2015.370367\n",
      "Epoch: 25 \tTraining Loss: 2713.696833\n",
      "Epoch: 25 \tTraining Loss: 2201.148141\n",
      "Epoch: 25 \tTraining Loss: 2017.325805\n",
      "Epoch: 25 \tTraining Loss: 2669.318805\n",
      "Epoch: 25 \tTraining Loss: 2296.536301\n",
      "Epoch: 25 \tTraining Loss: 1978.619345\n",
      "Epoch: 25 \tTraining Loss: 2079.565712\n",
      "Epoch: 25 \tTraining Loss: 2437.410517\n",
      "Epoch: 25 \tTraining Loss: 2557.501388\n",
      "Epoch: 25 \tTraining Loss: 2409.559181\n",
      "Epoch: 25 \tTraining Loss: 2167.746693\n",
      "Epoch: 25 \tTraining Loss: 2289.731065\n",
      "Epoch: 25 \tTraining Loss: 2035.515174\n",
      "Epoch: 25 \tTraining Loss: 2092.696151\n",
      "Epoch: 25 \tTraining Loss: 2142.833552\n",
      "Epoch: 25 \tTraining Loss: 2307.021095\n",
      "Epoch: 25 \tTraining Loss: 2327.835116\n",
      "Epoch: 25 \tTraining Loss: 2389.134669\n",
      "Epoch: 25 \tTraining Loss: 1855.712244\n",
      "Epoch: 25 \tTraining Loss: 2723.065174\n",
      "Epoch: 25 \tTraining Loss: 2490.703522\n",
      "Epoch: 25 \tTraining Loss: 2501.917698\n",
      "Epoch: 25 \tTraining Loss: 2380.089449\n",
      "mse: 91337.575\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            y_hat = y_hat.view(y_hat.shape[0]).double()\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        eval_model(model, val_loader)\n",
    "\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 25\n",
    "train(model, train_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), DATA_PATH+\"/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'momodel_loadeddel' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-584703e8213c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpisiodeCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmomodel_loadeddel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_loaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'momodel_loadeddel' is not defined"
     ]
    }
   ],
   "source": [
    "model_loaded = EpisiodeCNN()\n",
    "model_loaded.load_state_dict(torch.load(DATA_PATH+\"/model.pt\"))\n",
    "eval_model(model_loaded, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}